{
  "best_global_step": 250,
  "best_metric": 1.0983375310897827,
  "best_model_checkpoint": "./qwen3_lora_ttalkkac_20250805_114955/checkpoint-200",
  "epoch": 3.0,
  "eval_steps": 50,
  "global_step": 273,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011049723756906077,
      "grad_norm": 0.6914224028587341,
      "learning_rate": 0.0,
      "loss": 1.835,
      "step": 1
    },
    {
      "epoch": 0.022099447513812154,
      "grad_norm": 0.763301432132721,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.8459,
      "step": 2
    },
    {
      "epoch": 0.03314917127071823,
      "grad_norm": 0.699762761592865,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.8915,
      "step": 3
    },
    {
      "epoch": 0.04419889502762431,
      "grad_norm": 0.654112696647644,
      "learning_rate": 1.2e-05,
      "loss": 1.7364,
      "step": 4
    },
    {
      "epoch": 0.055248618784530384,
      "grad_norm": 0.6744799613952637,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.8465,
      "step": 5
    },
    {
      "epoch": 0.06629834254143646,
      "grad_norm": 0.650366485118866,
      "learning_rate": 2e-05,
      "loss": 1.67,
      "step": 6
    },
    {
      "epoch": 0.07734806629834254,
      "grad_norm": 0.6451612114906311,
      "learning_rate": 2.4e-05,
      "loss": 1.8818,
      "step": 7
    },
    {
      "epoch": 0.08839779005524862,
      "grad_norm": 0.6034003496170044,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.8163,
      "step": 8
    },
    {
      "epoch": 0.09944751381215469,
      "grad_norm": 0.5569033622741699,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.7506,
      "step": 9
    },
    {
      "epoch": 0.11049723756906077,
      "grad_norm": 0.5074149370193481,
      "learning_rate": 3.6e-05,
      "loss": 1.7312,
      "step": 10
    },
    {
      "epoch": 0.12154696132596685,
      "grad_norm": 0.5263546109199524,
      "learning_rate": 4e-05,
      "loss": 1.839,
      "step": 11
    },
    {
      "epoch": 0.13259668508287292,
      "grad_norm": 0.4584280252456665,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.6667,
      "step": 12
    },
    {
      "epoch": 0.143646408839779,
      "grad_norm": 0.433086097240448,
      "learning_rate": 4.8e-05,
      "loss": 1.7502,
      "step": 13
    },
    {
      "epoch": 0.15469613259668508,
      "grad_norm": 0.39814573526382446,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.7014,
      "step": 14
    },
    {
      "epoch": 0.16574585635359115,
      "grad_norm": 0.3201138377189636,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.6073,
      "step": 15
    },
    {
      "epoch": 0.17679558011049723,
      "grad_norm": 0.5872890949249268,
      "learning_rate": 6e-05,
      "loss": 1.6124,
      "step": 16
    },
    {
      "epoch": 0.1878453038674033,
      "grad_norm": 0.3425402343273163,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.7433,
      "step": 17
    },
    {
      "epoch": 0.19889502762430938,
      "grad_norm": 0.2316761165857315,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.5081,
      "step": 18
    },
    {
      "epoch": 0.20994475138121546,
      "grad_norm": 0.2124030441045761,
      "learning_rate": 7.2e-05,
      "loss": 1.5738,
      "step": 19
    },
    {
      "epoch": 0.22099447513812154,
      "grad_norm": 0.20682917535305023,
      "learning_rate": 7.6e-05,
      "loss": 1.6729,
      "step": 20
    },
    {
      "epoch": 0.23204419889502761,
      "grad_norm": 0.19711501896381378,
      "learning_rate": 8e-05,
      "loss": 1.5442,
      "step": 21
    },
    {
      "epoch": 0.2430939226519337,
      "grad_norm": 0.19528836011886597,
      "learning_rate": 8.4e-05,
      "loss": 1.5861,
      "step": 22
    },
    {
      "epoch": 0.2541436464088398,
      "grad_norm": 0.30180594325065613,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.6418,
      "step": 23
    },
    {
      "epoch": 0.26519337016574585,
      "grad_norm": 0.23624034225940704,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.6839,
      "step": 24
    },
    {
      "epoch": 0.27624309392265195,
      "grad_norm": 0.25812825560569763,
      "learning_rate": 9.6e-05,
      "loss": 1.5422,
      "step": 25
    },
    {
      "epoch": 0.287292817679558,
      "grad_norm": 0.27966073155403137,
      "learning_rate": 0.0001,
      "loss": 1.5908,
      "step": 26
    },
    {
      "epoch": 0.2983425414364641,
      "grad_norm": 0.22783435881137848,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.459,
      "step": 27
    },
    {
      "epoch": 0.30939226519337015,
      "grad_norm": 0.3476146161556244,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.4536,
      "step": 28
    },
    {
      "epoch": 0.32044198895027626,
      "grad_norm": 0.24458962678909302,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.4986,
      "step": 29
    },
    {
      "epoch": 0.3314917127071823,
      "grad_norm": 0.4072430729866028,
      "learning_rate": 0.000116,
      "loss": 1.3649,
      "step": 30
    },
    {
      "epoch": 0.3425414364640884,
      "grad_norm": 0.19545404613018036,
      "learning_rate": 0.00012,
      "loss": 1.4443,
      "step": 31
    },
    {
      "epoch": 0.35359116022099446,
      "grad_norm": 0.19080454111099243,
      "learning_rate": 0.000124,
      "loss": 1.4458,
      "step": 32
    },
    {
      "epoch": 0.36464088397790057,
      "grad_norm": 0.35080114006996155,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.4044,
      "step": 33
    },
    {
      "epoch": 0.3756906077348066,
      "grad_norm": 0.22526270151138306,
      "learning_rate": 0.000132,
      "loss": 1.4552,
      "step": 34
    },
    {
      "epoch": 0.3867403314917127,
      "grad_norm": 0.20294120907783508,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.3809,
      "step": 35
    },
    {
      "epoch": 0.39779005524861877,
      "grad_norm": 0.1927383989095688,
      "learning_rate": 0.00014,
      "loss": 1.3384,
      "step": 36
    },
    {
      "epoch": 0.4088397790055249,
      "grad_norm": 0.18885070085525513,
      "learning_rate": 0.000144,
      "loss": 1.3287,
      "step": 37
    },
    {
      "epoch": 0.4198895027624309,
      "grad_norm": 0.2105056494474411,
      "learning_rate": 0.000148,
      "loss": 1.3611,
      "step": 38
    },
    {
      "epoch": 0.430939226519337,
      "grad_norm": 0.1934136152267456,
      "learning_rate": 0.000152,
      "loss": 1.3542,
      "step": 39
    },
    {
      "epoch": 0.4419889502762431,
      "grad_norm": 0.1846965104341507,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.3081,
      "step": 40
    },
    {
      "epoch": 0.4530386740331492,
      "grad_norm": 0.1741563379764557,
      "learning_rate": 0.00016,
      "loss": 1.239,
      "step": 41
    },
    {
      "epoch": 0.46408839779005523,
      "grad_norm": 0.19168110191822052,
      "learning_rate": 0.000164,
      "loss": 1.3348,
      "step": 42
    },
    {
      "epoch": 0.47513812154696133,
      "grad_norm": 0.20200836658477783,
      "learning_rate": 0.000168,
      "loss": 1.2447,
      "step": 43
    },
    {
      "epoch": 0.4861878453038674,
      "grad_norm": 0.15895646810531616,
      "learning_rate": 0.000172,
      "loss": 1.1509,
      "step": 44
    },
    {
      "epoch": 0.4972375690607735,
      "grad_norm": 0.17689508199691772,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.3871,
      "step": 45
    },
    {
      "epoch": 0.5082872928176796,
      "grad_norm": 0.18178878724575043,
      "learning_rate": 0.00018,
      "loss": 1.282,
      "step": 46
    },
    {
      "epoch": 0.5193370165745856,
      "grad_norm": 0.12618565559387207,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.2235,
      "step": 47
    },
    {
      "epoch": 0.5303867403314917,
      "grad_norm": 0.13420601189136505,
      "learning_rate": 0.000188,
      "loss": 1.2527,
      "step": 48
    },
    {
      "epoch": 0.5414364640883977,
      "grad_norm": 0.10892190784215927,
      "learning_rate": 0.000192,
      "loss": 1.1506,
      "step": 49
    },
    {
      "epoch": 0.5524861878453039,
      "grad_norm": 0.12400861084461212,
      "learning_rate": 0.000196,
      "loss": 1.2269,
      "step": 50
    },
    {
      "epoch": 0.5524861878453039,
      "eval_loss": 1.2095272541046143,
      "eval_runtime": 220.836,
      "eval_samples_per_second": 1.639,
      "eval_steps_per_second": 1.639,
      "step": 50
    },
    {
      "epoch": 0.56353591160221,
      "grad_norm": 0.10805337876081467,
      "learning_rate": 0.0002,
      "loss": 1.2472,
      "step": 51
    },
    {
      "epoch": 0.574585635359116,
      "grad_norm": 0.16074572503566742,
      "learning_rate": 0.0001991031390134529,
      "loss": 1.2473,
      "step": 52
    },
    {
      "epoch": 0.585635359116022,
      "grad_norm": 0.09303444623947144,
      "learning_rate": 0.00019820627802690584,
      "loss": 1.1587,
      "step": 53
    },
    {
      "epoch": 0.5966850828729282,
      "grad_norm": 0.10061949491500854,
      "learning_rate": 0.00019730941704035874,
      "loss": 1.2417,
      "step": 54
    },
    {
      "epoch": 0.6077348066298343,
      "grad_norm": 0.09468260407447815,
      "learning_rate": 0.00019641255605381167,
      "loss": 1.1905,
      "step": 55
    },
    {
      "epoch": 0.6187845303867403,
      "grad_norm": 0.1364942342042923,
      "learning_rate": 0.0001955156950672646,
      "loss": 1.1677,
      "step": 56
    },
    {
      "epoch": 0.6298342541436464,
      "grad_norm": 0.12514422833919525,
      "learning_rate": 0.0001946188340807175,
      "loss": 1.2923,
      "step": 57
    },
    {
      "epoch": 0.6408839779005525,
      "grad_norm": 0.1082070842385292,
      "learning_rate": 0.00019372197309417043,
      "loss": 1.2405,
      "step": 58
    },
    {
      "epoch": 0.6519337016574586,
      "grad_norm": 0.09862517565488815,
      "learning_rate": 0.00019282511210762333,
      "loss": 1.1923,
      "step": 59
    },
    {
      "epoch": 0.6629834254143646,
      "grad_norm": 0.12741585075855255,
      "learning_rate": 0.00019192825112107625,
      "loss": 1.2628,
      "step": 60
    },
    {
      "epoch": 0.6740331491712708,
      "grad_norm": 0.09493010491132736,
      "learning_rate": 0.00019103139013452916,
      "loss": 1.2609,
      "step": 61
    },
    {
      "epoch": 0.6850828729281768,
      "grad_norm": 0.10805431753396988,
      "learning_rate": 0.00019013452914798206,
      "loss": 1.2184,
      "step": 62
    },
    {
      "epoch": 0.6961325966850829,
      "grad_norm": 0.10536783188581467,
      "learning_rate": 0.00018923766816143498,
      "loss": 1.2865,
      "step": 63
    },
    {
      "epoch": 0.7071823204419889,
      "grad_norm": 0.09068594127893448,
      "learning_rate": 0.0001883408071748879,
      "loss": 1.1465,
      "step": 64
    },
    {
      "epoch": 0.7182320441988951,
      "grad_norm": 0.09306588768959045,
      "learning_rate": 0.00018744394618834081,
      "loss": 1.2735,
      "step": 65
    },
    {
      "epoch": 0.7292817679558011,
      "grad_norm": 0.09507708996534348,
      "learning_rate": 0.00018654708520179374,
      "loss": 1.152,
      "step": 66
    },
    {
      "epoch": 0.7403314917127072,
      "grad_norm": 0.10063356161117554,
      "learning_rate": 0.00018565022421524664,
      "loss": 1.0832,
      "step": 67
    },
    {
      "epoch": 0.7513812154696132,
      "grad_norm": 0.12207921594381332,
      "learning_rate": 0.00018475336322869957,
      "loss": 1.14,
      "step": 68
    },
    {
      "epoch": 0.7624309392265194,
      "grad_norm": 0.10090208053588867,
      "learning_rate": 0.00018385650224215247,
      "loss": 1.217,
      "step": 69
    },
    {
      "epoch": 0.7734806629834254,
      "grad_norm": 0.10271742939949036,
      "learning_rate": 0.00018295964125560537,
      "loss": 1.1773,
      "step": 70
    },
    {
      "epoch": 0.7845303867403315,
      "grad_norm": 0.09849319607019424,
      "learning_rate": 0.0001820627802690583,
      "loss": 1.1778,
      "step": 71
    },
    {
      "epoch": 0.7955801104972375,
      "grad_norm": 0.19458265602588654,
      "learning_rate": 0.0001811659192825112,
      "loss": 1.2691,
      "step": 72
    },
    {
      "epoch": 0.8066298342541437,
      "grad_norm": 0.09495647996664047,
      "learning_rate": 0.00018026905829596416,
      "loss": 1.2108,
      "step": 73
    },
    {
      "epoch": 0.8176795580110497,
      "grad_norm": 0.11161425709724426,
      "learning_rate": 0.00017937219730941706,
      "loss": 1.1542,
      "step": 74
    },
    {
      "epoch": 0.8287292817679558,
      "grad_norm": 0.11193816363811493,
      "learning_rate": 0.00017847533632286996,
      "loss": 1.1124,
      "step": 75
    },
    {
      "epoch": 0.8397790055248618,
      "grad_norm": 0.218347430229187,
      "learning_rate": 0.0001775784753363229,
      "loss": 1.0971,
      "step": 76
    },
    {
      "epoch": 0.850828729281768,
      "grad_norm": 0.0994362086057663,
      "learning_rate": 0.0001766816143497758,
      "loss": 1.1637,
      "step": 77
    },
    {
      "epoch": 0.861878453038674,
      "grad_norm": 0.10226061940193176,
      "learning_rate": 0.00017578475336322872,
      "loss": 1.127,
      "step": 78
    },
    {
      "epoch": 0.8729281767955801,
      "grad_norm": 0.09925787895917892,
      "learning_rate": 0.00017488789237668162,
      "loss": 1.1263,
      "step": 79
    },
    {
      "epoch": 0.8839779005524862,
      "grad_norm": 0.10713555663824081,
      "learning_rate": 0.00017399103139013452,
      "loss": 1.2032,
      "step": 80
    },
    {
      "epoch": 0.8950276243093923,
      "grad_norm": 0.11105916649103165,
      "learning_rate": 0.00017309417040358745,
      "loss": 1.2053,
      "step": 81
    },
    {
      "epoch": 0.9060773480662984,
      "grad_norm": 0.11805322766304016,
      "learning_rate": 0.00017219730941704037,
      "loss": 1.1952,
      "step": 82
    },
    {
      "epoch": 0.9171270718232044,
      "grad_norm": 0.11607382446527481,
      "learning_rate": 0.00017130044843049328,
      "loss": 1.2599,
      "step": 83
    },
    {
      "epoch": 0.9281767955801105,
      "grad_norm": 0.1105731651186943,
      "learning_rate": 0.0001704035874439462,
      "loss": 1.1633,
      "step": 84
    },
    {
      "epoch": 0.9392265193370166,
      "grad_norm": 0.10626610368490219,
      "learning_rate": 0.0001695067264573991,
      "loss": 1.1802,
      "step": 85
    },
    {
      "epoch": 0.9502762430939227,
      "grad_norm": 0.11586163938045502,
      "learning_rate": 0.00016860986547085203,
      "loss": 1.2171,
      "step": 86
    },
    {
      "epoch": 0.9613259668508287,
      "grad_norm": 0.10481986403465271,
      "learning_rate": 0.00016771300448430493,
      "loss": 1.1825,
      "step": 87
    },
    {
      "epoch": 0.9723756906077348,
      "grad_norm": 0.1162494644522667,
      "learning_rate": 0.00016681614349775784,
      "loss": 1.2069,
      "step": 88
    },
    {
      "epoch": 0.9834254143646409,
      "grad_norm": 0.12950854003429413,
      "learning_rate": 0.00016591928251121076,
      "loss": 1.2209,
      "step": 89
    },
    {
      "epoch": 0.994475138121547,
      "grad_norm": 0.12824054062366486,
      "learning_rate": 0.0001650224215246637,
      "loss": 1.2193,
      "step": 90
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.16001911461353302,
      "learning_rate": 0.00016412556053811662,
      "loss": 1.2245,
      "step": 91
    },
    {
      "epoch": 1.011049723756906,
      "grad_norm": 0.11162827163934708,
      "learning_rate": 0.00016322869955156952,
      "loss": 1.193,
      "step": 92
    },
    {
      "epoch": 1.022099447513812,
      "grad_norm": 0.11312992125749588,
      "learning_rate": 0.00016233183856502242,
      "loss": 1.2141,
      "step": 93
    },
    {
      "epoch": 1.0331491712707181,
      "grad_norm": 0.11246398091316223,
      "learning_rate": 0.00016143497757847535,
      "loss": 1.1242,
      "step": 94
    },
    {
      "epoch": 1.0441988950276242,
      "grad_norm": 0.11086495220661163,
      "learning_rate": 0.00016053811659192825,
      "loss": 1.1425,
      "step": 95
    },
    {
      "epoch": 1.0552486187845305,
      "grad_norm": 0.10961254686117172,
      "learning_rate": 0.00015964125560538118,
      "loss": 1.1866,
      "step": 96
    },
    {
      "epoch": 1.0662983425414365,
      "grad_norm": 0.10675738751888275,
      "learning_rate": 0.00015874439461883408,
      "loss": 1.1062,
      "step": 97
    },
    {
      "epoch": 1.0773480662983426,
      "grad_norm": 0.12869057059288025,
      "learning_rate": 0.00015784753363228698,
      "loss": 1.2123,
      "step": 98
    },
    {
      "epoch": 1.0883977900552486,
      "grad_norm": 0.13190655410289764,
      "learning_rate": 0.00015695067264573994,
      "loss": 1.2346,
      "step": 99
    },
    {
      "epoch": 1.0994475138121547,
      "grad_norm": 0.1161801889538765,
      "learning_rate": 0.00015605381165919284,
      "loss": 1.1201,
      "step": 100
    },
    {
      "epoch": 1.0994475138121547,
      "eval_loss": 1.1419545412063599,
      "eval_runtime": 220.9858,
      "eval_samples_per_second": 1.638,
      "eval_steps_per_second": 1.638,
      "step": 100
    },
    {
      "epoch": 1.1104972375690607,
      "grad_norm": 0.1139589250087738,
      "learning_rate": 0.00015515695067264574,
      "loss": 1.1656,
      "step": 101
    },
    {
      "epoch": 1.1215469613259668,
      "grad_norm": 0.11483489722013474,
      "learning_rate": 0.00015426008968609867,
      "loss": 1.1659,
      "step": 102
    },
    {
      "epoch": 1.132596685082873,
      "grad_norm": 0.11825653165578842,
      "learning_rate": 0.00015336322869955157,
      "loss": 1.1817,
      "step": 103
    },
    {
      "epoch": 1.143646408839779,
      "grad_norm": 0.12471823394298553,
      "learning_rate": 0.0001524663677130045,
      "loss": 1.2069,
      "step": 104
    },
    {
      "epoch": 1.1546961325966851,
      "grad_norm": 0.12162560224533081,
      "learning_rate": 0.0001515695067264574,
      "loss": 1.2069,
      "step": 105
    },
    {
      "epoch": 1.1657458563535912,
      "grad_norm": 0.12127780169248581,
      "learning_rate": 0.00015067264573991032,
      "loss": 1.2245,
      "step": 106
    },
    {
      "epoch": 1.1767955801104972,
      "grad_norm": 0.11410847306251526,
      "learning_rate": 0.00014977578475336325,
      "loss": 1.1657,
      "step": 107
    },
    {
      "epoch": 1.1878453038674033,
      "grad_norm": 0.12433631718158722,
      "learning_rate": 0.00014887892376681615,
      "loss": 1.2408,
      "step": 108
    },
    {
      "epoch": 1.1988950276243093,
      "grad_norm": 0.14033325016498566,
      "learning_rate": 0.00014798206278026908,
      "loss": 1.1214,
      "step": 109
    },
    {
      "epoch": 1.2099447513812154,
      "grad_norm": 0.12373771518468857,
      "learning_rate": 0.00014708520179372198,
      "loss": 1.2034,
      "step": 110
    },
    {
      "epoch": 1.2209944751381214,
      "grad_norm": 0.12013916671276093,
      "learning_rate": 0.00014618834080717488,
      "loss": 1.1227,
      "step": 111
    },
    {
      "epoch": 1.2320441988950277,
      "grad_norm": 0.13893333077430725,
      "learning_rate": 0.0001452914798206278,
      "loss": 1.2257,
      "step": 112
    },
    {
      "epoch": 1.2430939226519337,
      "grad_norm": 0.4413214325904846,
      "learning_rate": 0.0001443946188340807,
      "loss": 1.1865,
      "step": 113
    },
    {
      "epoch": 1.2541436464088398,
      "grad_norm": 0.26031777262687683,
      "learning_rate": 0.00014349775784753364,
      "loss": 1.1088,
      "step": 114
    },
    {
      "epoch": 1.2651933701657458,
      "grad_norm": 0.12398220598697662,
      "learning_rate": 0.00014260089686098654,
      "loss": 1.1108,
      "step": 115
    },
    {
      "epoch": 1.276243093922652,
      "grad_norm": 0.11643989384174347,
      "learning_rate": 0.00014170403587443947,
      "loss": 1.1702,
      "step": 116
    },
    {
      "epoch": 1.287292817679558,
      "grad_norm": 0.1341121941804886,
      "learning_rate": 0.0001408071748878924,
      "loss": 1.206,
      "step": 117
    },
    {
      "epoch": 1.298342541436464,
      "grad_norm": 0.12597429752349854,
      "learning_rate": 0.0001399103139013453,
      "loss": 1.1551,
      "step": 118
    },
    {
      "epoch": 1.3093922651933703,
      "grad_norm": 0.15488499402999878,
      "learning_rate": 0.00013901345291479823,
      "loss": 1.059,
      "step": 119
    },
    {
      "epoch": 1.3204419889502763,
      "grad_norm": 0.13145287334918976,
      "learning_rate": 0.00013811659192825113,
      "loss": 1.1273,
      "step": 120
    },
    {
      "epoch": 1.3314917127071824,
      "grad_norm": 0.1309371143579483,
      "learning_rate": 0.00013721973094170403,
      "loss": 1.1406,
      "step": 121
    },
    {
      "epoch": 1.3425414364640884,
      "grad_norm": 0.18078631162643433,
      "learning_rate": 0.00013632286995515696,
      "loss": 1.1255,
      "step": 122
    },
    {
      "epoch": 1.3535911602209945,
      "grad_norm": 0.12170786410570145,
      "learning_rate": 0.00013542600896860986,
      "loss": 1.1345,
      "step": 123
    },
    {
      "epoch": 1.3646408839779005,
      "grad_norm": 0.1406368762254715,
      "learning_rate": 0.0001345291479820628,
      "loss": 1.2187,
      "step": 124
    },
    {
      "epoch": 1.3756906077348066,
      "grad_norm": 0.12874585390090942,
      "learning_rate": 0.00013363228699551572,
      "loss": 1.1271,
      "step": 125
    },
    {
      "epoch": 1.3867403314917128,
      "grad_norm": 0.14321300387382507,
      "learning_rate": 0.00013273542600896862,
      "loss": 1.1037,
      "step": 126
    },
    {
      "epoch": 1.3977900552486187,
      "grad_norm": 0.12756681442260742,
      "learning_rate": 0.00013183856502242154,
      "loss": 1.1051,
      "step": 127
    },
    {
      "epoch": 1.408839779005525,
      "grad_norm": 0.13687138259410858,
      "learning_rate": 0.00013094170403587445,
      "loss": 1.1838,
      "step": 128
    },
    {
      "epoch": 1.419889502762431,
      "grad_norm": 0.20723775029182434,
      "learning_rate": 0.00013004484304932735,
      "loss": 1.1167,
      "step": 129
    },
    {
      "epoch": 1.430939226519337,
      "grad_norm": 0.15164515376091003,
      "learning_rate": 0.00012914798206278027,
      "loss": 1.1002,
      "step": 130
    },
    {
      "epoch": 1.441988950276243,
      "grad_norm": 0.13521021604537964,
      "learning_rate": 0.00012825112107623318,
      "loss": 1.1352,
      "step": 131
    },
    {
      "epoch": 1.4530386740331491,
      "grad_norm": 0.12698358297348022,
      "learning_rate": 0.0001273542600896861,
      "loss": 1.0706,
      "step": 132
    },
    {
      "epoch": 1.4640883977900552,
      "grad_norm": 0.13760152459144592,
      "learning_rate": 0.00012645739910313903,
      "loss": 1.0693,
      "step": 133
    },
    {
      "epoch": 1.4751381215469612,
      "grad_norm": 0.13078564405441284,
      "learning_rate": 0.00012556053811659193,
      "loss": 1.1384,
      "step": 134
    },
    {
      "epoch": 1.4861878453038675,
      "grad_norm": 0.13116896152496338,
      "learning_rate": 0.00012466367713004486,
      "loss": 1.0964,
      "step": 135
    },
    {
      "epoch": 1.4972375690607735,
      "grad_norm": 0.13266412913799286,
      "learning_rate": 0.00012376681614349776,
      "loss": 1.1278,
      "step": 136
    },
    {
      "epoch": 1.5082872928176796,
      "grad_norm": 0.1245986819267273,
      "learning_rate": 0.0001228699551569507,
      "loss": 1.1419,
      "step": 137
    },
    {
      "epoch": 1.5193370165745856,
      "grad_norm": 0.1315087378025055,
      "learning_rate": 0.00012197309417040359,
      "loss": 1.195,
      "step": 138
    },
    {
      "epoch": 1.5303867403314917,
      "grad_norm": 0.15290208160877228,
      "learning_rate": 0.0001210762331838565,
      "loss": 1.1898,
      "step": 139
    },
    {
      "epoch": 1.5414364640883977,
      "grad_norm": 0.16130852699279785,
      "learning_rate": 0.00012017937219730942,
      "loss": 1.2221,
      "step": 140
    },
    {
      "epoch": 1.5524861878453038,
      "grad_norm": 0.12925276160240173,
      "learning_rate": 0.00011928251121076232,
      "loss": 1.1683,
      "step": 141
    },
    {
      "epoch": 1.56353591160221,
      "grad_norm": 0.13666342198848724,
      "learning_rate": 0.00011838565022421526,
      "loss": 1.1552,
      "step": 142
    },
    {
      "epoch": 1.5745856353591159,
      "grad_norm": 0.18330234289169312,
      "learning_rate": 0.00011748878923766818,
      "loss": 1.1263,
      "step": 143
    },
    {
      "epoch": 1.5856353591160222,
      "grad_norm": 0.13622553646564484,
      "learning_rate": 0.00011659192825112109,
      "loss": 1.1331,
      "step": 144
    },
    {
      "epoch": 1.5966850828729282,
      "grad_norm": 0.4852944314479828,
      "learning_rate": 0.00011569506726457399,
      "loss": 1.1505,
      "step": 145
    },
    {
      "epoch": 1.6077348066298343,
      "grad_norm": 0.15388807654380798,
      "learning_rate": 0.00011479820627802691,
      "loss": 1.1702,
      "step": 146
    },
    {
      "epoch": 1.6187845303867403,
      "grad_norm": 0.1284780204296112,
      "learning_rate": 0.00011390134529147982,
      "loss": 1.1404,
      "step": 147
    },
    {
      "epoch": 1.6298342541436464,
      "grad_norm": 0.14049428701400757,
      "learning_rate": 0.00011300448430493274,
      "loss": 1.1007,
      "step": 148
    },
    {
      "epoch": 1.6408839779005526,
      "grad_norm": 0.1467740833759308,
      "learning_rate": 0.00011210762331838565,
      "loss": 1.1891,
      "step": 149
    },
    {
      "epoch": 1.6519337016574585,
      "grad_norm": 0.19248372316360474,
      "learning_rate": 0.00011121076233183858,
      "loss": 1.0864,
      "step": 150
    },
    {
      "epoch": 1.6519337016574585,
      "eval_loss": 1.1177692413330078,
      "eval_runtime": 221.0643,
      "eval_samples_per_second": 1.638,
      "eval_steps_per_second": 1.638,
      "step": 150
    },
    {
      "epoch": 1.6629834254143647,
      "grad_norm": 0.13391561806201935,
      "learning_rate": 0.0001103139013452915,
      "loss": 1.1687,
      "step": 151
    },
    {
      "epoch": 1.6740331491712708,
      "grad_norm": 0.14702454209327698,
      "learning_rate": 0.00010941704035874441,
      "loss": 1.1705,
      "step": 152
    },
    {
      "epoch": 1.6850828729281768,
      "grad_norm": 0.1348869502544403,
      "learning_rate": 0.00010852017937219732,
      "loss": 1.1117,
      "step": 153
    },
    {
      "epoch": 1.6961325966850829,
      "grad_norm": 0.13086700439453125,
      "learning_rate": 0.00010762331838565022,
      "loss": 1.121,
      "step": 154
    },
    {
      "epoch": 1.707182320441989,
      "grad_norm": 0.1343594789505005,
      "learning_rate": 0.00010672645739910314,
      "loss": 1.1331,
      "step": 155
    },
    {
      "epoch": 1.7182320441988952,
      "grad_norm": 0.1446271389722824,
      "learning_rate": 0.00010582959641255605,
      "loss": 1.0937,
      "step": 156
    },
    {
      "epoch": 1.729281767955801,
      "grad_norm": 0.14803504943847656,
      "learning_rate": 0.00010493273542600897,
      "loss": 1.1079,
      "step": 157
    },
    {
      "epoch": 1.7403314917127073,
      "grad_norm": 0.11417852342128754,
      "learning_rate": 0.00010403587443946188,
      "loss": 1.0363,
      "step": 158
    },
    {
      "epoch": 1.7513812154696131,
      "grad_norm": 0.1393822878599167,
      "learning_rate": 0.00010313901345291481,
      "loss": 1.0726,
      "step": 159
    },
    {
      "epoch": 1.7624309392265194,
      "grad_norm": 0.15742680430412292,
      "learning_rate": 0.00010224215246636773,
      "loss": 1.1321,
      "step": 160
    },
    {
      "epoch": 1.7734806629834254,
      "grad_norm": 0.1626468151807785,
      "learning_rate": 0.00010134529147982064,
      "loss": 1.0822,
      "step": 161
    },
    {
      "epoch": 1.7845303867403315,
      "grad_norm": 0.13253317773342133,
      "learning_rate": 0.00010044843049327355,
      "loss": 1.1476,
      "step": 162
    },
    {
      "epoch": 1.7955801104972375,
      "grad_norm": 0.15184971690177917,
      "learning_rate": 9.955156950672646e-05,
      "loss": 1.1462,
      "step": 163
    },
    {
      "epoch": 1.8066298342541436,
      "grad_norm": 0.13928218185901642,
      "learning_rate": 9.865470852017937e-05,
      "loss": 1.1596,
      "step": 164
    },
    {
      "epoch": 1.8176795580110499,
      "grad_norm": 0.13209719955921173,
      "learning_rate": 9.77578475336323e-05,
      "loss": 1.1014,
      "step": 165
    },
    {
      "epoch": 1.8287292817679557,
      "grad_norm": 0.15224555134773254,
      "learning_rate": 9.686098654708521e-05,
      "loss": 1.1392,
      "step": 166
    },
    {
      "epoch": 1.839779005524862,
      "grad_norm": 0.16650570929050446,
      "learning_rate": 9.596412556053813e-05,
      "loss": 1.0828,
      "step": 167
    },
    {
      "epoch": 1.850828729281768,
      "grad_norm": 0.13355407118797302,
      "learning_rate": 9.506726457399103e-05,
      "loss": 1.1061,
      "step": 168
    },
    {
      "epoch": 1.861878453038674,
      "grad_norm": 0.13767004013061523,
      "learning_rate": 9.417040358744396e-05,
      "loss": 1.1216,
      "step": 169
    },
    {
      "epoch": 1.87292817679558,
      "grad_norm": 0.14333240687847137,
      "learning_rate": 9.327354260089687e-05,
      "loss": 1.1796,
      "step": 170
    },
    {
      "epoch": 1.8839779005524862,
      "grad_norm": 0.15721948444843292,
      "learning_rate": 9.237668161434979e-05,
      "loss": 1.1579,
      "step": 171
    },
    {
      "epoch": 1.8950276243093924,
      "grad_norm": 0.12894432246685028,
      "learning_rate": 9.147982062780269e-05,
      "loss": 1.0626,
      "step": 172
    },
    {
      "epoch": 1.9060773480662982,
      "grad_norm": 0.14462435245513916,
      "learning_rate": 9.05829596412556e-05,
      "loss": 1.0489,
      "step": 173
    },
    {
      "epoch": 1.9171270718232045,
      "grad_norm": 0.13348817825317383,
      "learning_rate": 8.968609865470853e-05,
      "loss": 1.0993,
      "step": 174
    },
    {
      "epoch": 1.9281767955801103,
      "grad_norm": 0.16293863952159882,
      "learning_rate": 8.878923766816144e-05,
      "loss": 1.1645,
      "step": 175
    },
    {
      "epoch": 1.9392265193370166,
      "grad_norm": 2.2671921253204346,
      "learning_rate": 8.789237668161436e-05,
      "loss": 1.1352,
      "step": 176
    },
    {
      "epoch": 1.9502762430939227,
      "grad_norm": 0.13991500437259674,
      "learning_rate": 8.699551569506726e-05,
      "loss": 1.0953,
      "step": 177
    },
    {
      "epoch": 1.9613259668508287,
      "grad_norm": 0.13626305758953094,
      "learning_rate": 8.609865470852019e-05,
      "loss": 1.1103,
      "step": 178
    },
    {
      "epoch": 1.9723756906077348,
      "grad_norm": 0.13762162625789642,
      "learning_rate": 8.52017937219731e-05,
      "loss": 1.142,
      "step": 179
    },
    {
      "epoch": 1.9834254143646408,
      "grad_norm": 0.20315420627593994,
      "learning_rate": 8.430493273542602e-05,
      "loss": 1.1415,
      "step": 180
    },
    {
      "epoch": 1.994475138121547,
      "grad_norm": 0.1550753116607666,
      "learning_rate": 8.340807174887892e-05,
      "loss": 1.0822,
      "step": 181
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.19012823700904846,
      "learning_rate": 8.251121076233185e-05,
      "loss": 1.1203,
      "step": 182
    },
    {
      "epoch": 2.0110497237569063,
      "grad_norm": 0.2528732419013977,
      "learning_rate": 8.161434977578476e-05,
      "loss": 1.0484,
      "step": 183
    },
    {
      "epoch": 2.022099447513812,
      "grad_norm": 0.1476088911294937,
      "learning_rate": 8.071748878923767e-05,
      "loss": 1.1432,
      "step": 184
    },
    {
      "epoch": 2.0331491712707184,
      "grad_norm": 0.14782659709453583,
      "learning_rate": 7.982062780269059e-05,
      "loss": 1.0852,
      "step": 185
    },
    {
      "epoch": 2.044198895027624,
      "grad_norm": 0.1396738737821579,
      "learning_rate": 7.892376681614349e-05,
      "loss": 1.0721,
      "step": 186
    },
    {
      "epoch": 2.0552486187845305,
      "grad_norm": 0.14749227464199066,
      "learning_rate": 7.802690582959642e-05,
      "loss": 1.0828,
      "step": 187
    },
    {
      "epoch": 2.0662983425414363,
      "grad_norm": 0.1425081193447113,
      "learning_rate": 7.713004484304933e-05,
      "loss": 1.1614,
      "step": 188
    },
    {
      "epoch": 2.0773480662983426,
      "grad_norm": 0.1441386342048645,
      "learning_rate": 7.623318385650225e-05,
      "loss": 1.1029,
      "step": 189
    },
    {
      "epoch": 2.0883977900552484,
      "grad_norm": 0.15295100212097168,
      "learning_rate": 7.533632286995516e-05,
      "loss": 1.165,
      "step": 190
    },
    {
      "epoch": 2.0994475138121547,
      "grad_norm": 0.1398109495639801,
      "learning_rate": 7.443946188340808e-05,
      "loss": 1.1104,
      "step": 191
    },
    {
      "epoch": 2.110497237569061,
      "grad_norm": 0.16900423169136047,
      "learning_rate": 7.354260089686099e-05,
      "loss": 1.1844,
      "step": 192
    },
    {
      "epoch": 2.1215469613259668,
      "grad_norm": 0.14667105674743652,
      "learning_rate": 7.26457399103139e-05,
      "loss": 1.1402,
      "step": 193
    },
    {
      "epoch": 2.132596685082873,
      "grad_norm": 0.1336687207221985,
      "learning_rate": 7.174887892376682e-05,
      "loss": 1.1132,
      "step": 194
    },
    {
      "epoch": 2.143646408839779,
      "grad_norm": 0.1447455883026123,
      "learning_rate": 7.085201793721974e-05,
      "loss": 1.132,
      "step": 195
    },
    {
      "epoch": 2.154696132596685,
      "grad_norm": 0.15304546058177948,
      "learning_rate": 6.995515695067265e-05,
      "loss": 1.0852,
      "step": 196
    },
    {
      "epoch": 2.165745856353591,
      "grad_norm": 0.14529670774936676,
      "learning_rate": 6.905829596412556e-05,
      "loss": 1.1222,
      "step": 197
    },
    {
      "epoch": 2.1767955801104972,
      "grad_norm": 0.15422552824020386,
      "learning_rate": 6.816143497757848e-05,
      "loss": 1.1003,
      "step": 198
    },
    {
      "epoch": 2.1878453038674035,
      "grad_norm": 0.1395694613456726,
      "learning_rate": 6.72645739910314e-05,
      "loss": 1.1408,
      "step": 199
    },
    {
      "epoch": 2.1988950276243093,
      "grad_norm": 0.13757504522800446,
      "learning_rate": 6.636771300448431e-05,
      "loss": 1.1121,
      "step": 200
    },
    {
      "epoch": 2.1988950276243093,
      "eval_loss": 1.1049835681915283,
      "eval_runtime": 220.4613,
      "eval_samples_per_second": 1.642,
      "eval_steps_per_second": 1.642,
      "step": 200
    },
    {
      "epoch": 2.2099447513812156,
      "grad_norm": 0.1498759686946869,
      "learning_rate": 6.547085201793722e-05,
      "loss": 1.1088,
      "step": 201
    },
    {
      "epoch": 2.2209944751381214,
      "grad_norm": 0.1425621509552002,
      "learning_rate": 6.457399103139014e-05,
      "loss": 1.0752,
      "step": 202
    },
    {
      "epoch": 2.2320441988950277,
      "grad_norm": 0.1451946645975113,
      "learning_rate": 6.367713004484305e-05,
      "loss": 1.1595,
      "step": 203
    },
    {
      "epoch": 2.2430939226519335,
      "grad_norm": 0.14309263229370117,
      "learning_rate": 6.278026905829597e-05,
      "loss": 1.1057,
      "step": 204
    },
    {
      "epoch": 2.25414364640884,
      "grad_norm": 0.1461573839187622,
      "learning_rate": 6.188340807174888e-05,
      "loss": 1.0666,
      "step": 205
    },
    {
      "epoch": 2.265193370165746,
      "grad_norm": 0.1607990860939026,
      "learning_rate": 6.0986547085201795e-05,
      "loss": 1.1547,
      "step": 206
    },
    {
      "epoch": 2.276243093922652,
      "grad_norm": 0.1511756181716919,
      "learning_rate": 6.008968609865471e-05,
      "loss": 1.1193,
      "step": 207
    },
    {
      "epoch": 2.287292817679558,
      "grad_norm": 0.13503848016262054,
      "learning_rate": 5.919282511210763e-05,
      "loss": 1.0432,
      "step": 208
    },
    {
      "epoch": 2.298342541436464,
      "grad_norm": 0.15477235615253448,
      "learning_rate": 5.8295964125560546e-05,
      "loss": 1.1192,
      "step": 209
    },
    {
      "epoch": 2.3093922651933703,
      "grad_norm": 0.13977229595184326,
      "learning_rate": 5.7399103139013454e-05,
      "loss": 1.0682,
      "step": 210
    },
    {
      "epoch": 2.320441988950276,
      "grad_norm": 0.1413058191537857,
      "learning_rate": 5.650224215246637e-05,
      "loss": 1.1686,
      "step": 211
    },
    {
      "epoch": 2.3314917127071824,
      "grad_norm": 0.13595381379127502,
      "learning_rate": 5.560538116591929e-05,
      "loss": 1.0683,
      "step": 212
    },
    {
      "epoch": 2.3425414364640886,
      "grad_norm": 0.15803082287311554,
      "learning_rate": 5.4708520179372204e-05,
      "loss": 1.1378,
      "step": 213
    },
    {
      "epoch": 2.3535911602209945,
      "grad_norm": 0.1420089304447174,
      "learning_rate": 5.381165919282511e-05,
      "loss": 1.0965,
      "step": 214
    },
    {
      "epoch": 2.3646408839779007,
      "grad_norm": 0.15077060461044312,
      "learning_rate": 5.291479820627803e-05,
      "loss": 1.1429,
      "step": 215
    },
    {
      "epoch": 2.3756906077348066,
      "grad_norm": 0.14063163101673126,
      "learning_rate": 5.201793721973094e-05,
      "loss": 1.0651,
      "step": 216
    },
    {
      "epoch": 2.386740331491713,
      "grad_norm": 0.14430533349514008,
      "learning_rate": 5.112107623318386e-05,
      "loss": 1.1478,
      "step": 217
    },
    {
      "epoch": 2.3977900552486187,
      "grad_norm": 0.15848800539970398,
      "learning_rate": 5.022421524663678e-05,
      "loss": 1.145,
      "step": 218
    },
    {
      "epoch": 2.408839779005525,
      "grad_norm": 0.14188893139362335,
      "learning_rate": 4.9327354260089685e-05,
      "loss": 1.0681,
      "step": 219
    },
    {
      "epoch": 2.4198895027624308,
      "grad_norm": 0.29377657175064087,
      "learning_rate": 4.8430493273542606e-05,
      "loss": 1.1261,
      "step": 220
    },
    {
      "epoch": 2.430939226519337,
      "grad_norm": 0.15488936007022858,
      "learning_rate": 4.7533632286995514e-05,
      "loss": 1.1607,
      "step": 221
    },
    {
      "epoch": 2.441988950276243,
      "grad_norm": 0.14375810325145721,
      "learning_rate": 4.6636771300448435e-05,
      "loss": 1.0887,
      "step": 222
    },
    {
      "epoch": 2.453038674033149,
      "grad_norm": 0.14663346111774445,
      "learning_rate": 4.573991031390134e-05,
      "loss": 1.1287,
      "step": 223
    },
    {
      "epoch": 2.4640883977900554,
      "grad_norm": 0.14967873692512512,
      "learning_rate": 4.4843049327354265e-05,
      "loss": 1.1699,
      "step": 224
    },
    {
      "epoch": 2.4751381215469612,
      "grad_norm": 0.13040657341480255,
      "learning_rate": 4.394618834080718e-05,
      "loss": 0.9654,
      "step": 225
    },
    {
      "epoch": 2.4861878453038675,
      "grad_norm": 0.16705797612667084,
      "learning_rate": 4.3049327354260094e-05,
      "loss": 1.1452,
      "step": 226
    },
    {
      "epoch": 2.4972375690607733,
      "grad_norm": 0.14133284986019135,
      "learning_rate": 4.215246636771301e-05,
      "loss": 1.0962,
      "step": 227
    },
    {
      "epoch": 2.5082872928176796,
      "grad_norm": 0.15830443799495697,
      "learning_rate": 4.125560538116592e-05,
      "loss": 1.1198,
      "step": 228
    },
    {
      "epoch": 2.5193370165745854,
      "grad_norm": 0.140859454870224,
      "learning_rate": 4.035874439461884e-05,
      "loss": 1.116,
      "step": 229
    },
    {
      "epoch": 2.5303867403314917,
      "grad_norm": 0.13807478547096252,
      "learning_rate": 3.9461883408071745e-05,
      "loss": 1.0892,
      "step": 230
    },
    {
      "epoch": 2.541436464088398,
      "grad_norm": 0.15147298574447632,
      "learning_rate": 3.8565022421524667e-05,
      "loss": 1.1692,
      "step": 231
    },
    {
      "epoch": 2.552486187845304,
      "grad_norm": 0.15767556428909302,
      "learning_rate": 3.766816143497758e-05,
      "loss": 1.1066,
      "step": 232
    },
    {
      "epoch": 2.56353591160221,
      "grad_norm": 0.14872698485851288,
      "learning_rate": 3.6771300448430496e-05,
      "loss": 1.1663,
      "step": 233
    },
    {
      "epoch": 2.574585635359116,
      "grad_norm": 0.15473051369190216,
      "learning_rate": 3.587443946188341e-05,
      "loss": 1.0848,
      "step": 234
    },
    {
      "epoch": 2.585635359116022,
      "grad_norm": 0.1461065709590912,
      "learning_rate": 3.4977578475336325e-05,
      "loss": 1.1491,
      "step": 235
    },
    {
      "epoch": 2.596685082872928,
      "grad_norm": 0.14413118362426758,
      "learning_rate": 3.408071748878924e-05,
      "loss": 1.1428,
      "step": 236
    },
    {
      "epoch": 2.6077348066298343,
      "grad_norm": 0.3069554269313812,
      "learning_rate": 3.3183856502242154e-05,
      "loss": 1.0421,
      "step": 237
    },
    {
      "epoch": 2.6187845303867405,
      "grad_norm": 0.13180379569530487,
      "learning_rate": 3.228699551569507e-05,
      "loss": 1.0906,
      "step": 238
    },
    {
      "epoch": 2.6298342541436464,
      "grad_norm": 0.1394793689250946,
      "learning_rate": 3.139013452914798e-05,
      "loss": 1.0832,
      "step": 239
    },
    {
      "epoch": 2.6408839779005526,
      "grad_norm": 0.1400204747915268,
      "learning_rate": 3.0493273542600898e-05,
      "loss": 1.0891,
      "step": 240
    },
    {
      "epoch": 2.6519337016574585,
      "grad_norm": 0.15431839227676392,
      "learning_rate": 2.9596412556053816e-05,
      "loss": 1.0946,
      "step": 241
    },
    {
      "epoch": 2.6629834254143647,
      "grad_norm": 0.16163624823093414,
      "learning_rate": 2.8699551569506727e-05,
      "loss": 1.1504,
      "step": 242
    },
    {
      "epoch": 2.6740331491712706,
      "grad_norm": 0.14061221480369568,
      "learning_rate": 2.7802690582959645e-05,
      "loss": 1.1364,
      "step": 243
    },
    {
      "epoch": 2.685082872928177,
      "grad_norm": 0.13800838589668274,
      "learning_rate": 2.6905829596412556e-05,
      "loss": 1.1108,
      "step": 244
    },
    {
      "epoch": 2.696132596685083,
      "grad_norm": 0.14221879839897156,
      "learning_rate": 2.600896860986547e-05,
      "loss": 1.1459,
      "step": 245
    },
    {
      "epoch": 2.707182320441989,
      "grad_norm": 0.15524445474147797,
      "learning_rate": 2.511210762331839e-05,
      "loss": 1.0507,
      "step": 246
    },
    {
      "epoch": 2.718232044198895,
      "grad_norm": 0.3070281744003296,
      "learning_rate": 2.4215246636771303e-05,
      "loss": 1.119,
      "step": 247
    },
    {
      "epoch": 2.729281767955801,
      "grad_norm": 0.16032551229000092,
      "learning_rate": 2.3318385650224218e-05,
      "loss": 1.2303,
      "step": 248
    },
    {
      "epoch": 2.7403314917127073,
      "grad_norm": 0.15503336489200592,
      "learning_rate": 2.2421524663677132e-05,
      "loss": 1.1395,
      "step": 249
    },
    {
      "epoch": 2.751381215469613,
      "grad_norm": 0.14681628346443176,
      "learning_rate": 2.1524663677130047e-05,
      "loss": 1.1043,
      "step": 250
    },
    {
      "epoch": 2.751381215469613,
      "eval_loss": 1.0983375310897827,
      "eval_runtime": 221.0247,
      "eval_samples_per_second": 1.638,
      "eval_steps_per_second": 1.638,
      "step": 250
    },
    {
      "epoch": 2.7624309392265194,
      "grad_norm": 0.1430046558380127,
      "learning_rate": 2.062780269058296e-05,
      "loss": 1.0276,
      "step": 251
    },
    {
      "epoch": 2.7734806629834257,
      "grad_norm": 0.1431874781847,
      "learning_rate": 1.9730941704035873e-05,
      "loss": 1.1074,
      "step": 252
    },
    {
      "epoch": 2.7845303867403315,
      "grad_norm": 0.14173899590969086,
      "learning_rate": 1.883408071748879e-05,
      "loss": 1.0915,
      "step": 253
    },
    {
      "epoch": 2.7955801104972373,
      "grad_norm": 0.13987913727760315,
      "learning_rate": 1.7937219730941705e-05,
      "loss": 1.0802,
      "step": 254
    },
    {
      "epoch": 2.8066298342541436,
      "grad_norm": 0.15572905540466309,
      "learning_rate": 1.704035874439462e-05,
      "loss": 1.1388,
      "step": 255
    },
    {
      "epoch": 2.81767955801105,
      "grad_norm": 0.1411125659942627,
      "learning_rate": 1.6143497757847534e-05,
      "loss": 1.1298,
      "step": 256
    },
    {
      "epoch": 2.8287292817679557,
      "grad_norm": 0.15045477449893951,
      "learning_rate": 1.5246636771300449e-05,
      "loss": 1.122,
      "step": 257
    },
    {
      "epoch": 2.839779005524862,
      "grad_norm": 0.15456359088420868,
      "learning_rate": 1.4349775784753363e-05,
      "loss": 1.0161,
      "step": 258
    },
    {
      "epoch": 2.8508287292817682,
      "grad_norm": 0.13889527320861816,
      "learning_rate": 1.3452914798206278e-05,
      "loss": 1.0739,
      "step": 259
    },
    {
      "epoch": 2.861878453038674,
      "grad_norm": 0.1445886492729187,
      "learning_rate": 1.2556053811659194e-05,
      "loss": 1.1772,
      "step": 260
    },
    {
      "epoch": 2.87292817679558,
      "grad_norm": 0.15460295975208282,
      "learning_rate": 1.1659192825112109e-05,
      "loss": 1.0473,
      "step": 261
    },
    {
      "epoch": 2.883977900552486,
      "grad_norm": 0.1387525349855423,
      "learning_rate": 1.0762331838565023e-05,
      "loss": 1.0767,
      "step": 262
    },
    {
      "epoch": 2.8950276243093924,
      "grad_norm": 0.13738854229450226,
      "learning_rate": 9.865470852017936e-06,
      "loss": 1.136,
      "step": 263
    },
    {
      "epoch": 2.9060773480662982,
      "grad_norm": 0.14503474533557892,
      "learning_rate": 8.968609865470853e-06,
      "loss": 1.0576,
      "step": 264
    },
    {
      "epoch": 2.9171270718232045,
      "grad_norm": 0.13716945052146912,
      "learning_rate": 8.071748878923767e-06,
      "loss": 1.0128,
      "step": 265
    },
    {
      "epoch": 2.9281767955801103,
      "grad_norm": 0.1443960815668106,
      "learning_rate": 7.174887892376682e-06,
      "loss": 1.1213,
      "step": 266
    },
    {
      "epoch": 2.9392265193370166,
      "grad_norm": 0.15026089549064636,
      "learning_rate": 6.278026905829597e-06,
      "loss": 1.1237,
      "step": 267
    },
    {
      "epoch": 2.9502762430939224,
      "grad_norm": 0.14045049250125885,
      "learning_rate": 5.381165919282512e-06,
      "loss": 1.1,
      "step": 268
    },
    {
      "epoch": 2.9613259668508287,
      "grad_norm": 0.17065158486366272,
      "learning_rate": 4.484304932735426e-06,
      "loss": 1.1269,
      "step": 269
    },
    {
      "epoch": 2.972375690607735,
      "grad_norm": 0.14843949675559998,
      "learning_rate": 3.587443946188341e-06,
      "loss": 1.1551,
      "step": 270
    },
    {
      "epoch": 2.983425414364641,
      "grad_norm": 0.13885876536369324,
      "learning_rate": 2.690582959641256e-06,
      "loss": 1.0404,
      "step": 271
    },
    {
      "epoch": 2.994475138121547,
      "grad_norm": 0.15004462003707886,
      "learning_rate": 1.7937219730941704e-06,
      "loss": 1.1435,
      "step": 272
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.19218948483467102,
      "learning_rate": 8.968609865470852e-07,
      "loss": 1.1679,
      "step": 273
    },
    {
      "epoch": 3.0,
      "step": 273,
      "total_flos": 4.109331389259571e+17,
      "train_loss": 1.2100492761685298,
      "train_runtime": 7666.8167,
      "train_samples_per_second": 0.567,
      "train_steps_per_second": 0.036
    }
  ],
  "logging_steps": 1,
  "max_steps": 273,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.109331389259571e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
