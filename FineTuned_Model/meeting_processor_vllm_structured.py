import json
import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import torch
import gc

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ModelConfig:
    """ëª¨ë¸ ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤"""
    MODEL_PATH: str = "Qwen/Qwen3-4B-AWQ"
    MAX_NEW_TOKENS: int = 2048
    TEMPERATURE: float = 0.2
    TOP_P: float = 0.9
    REPETITION_PENALTY: float = 1.1
    CHUNK_SIZE: int = 5000
    CHUNK_OVERLAP: int = 512
    TEST_FILE_LIMIT: int = 0  # 0ì´ë©´ ì „ì²´ íŒŒì¼ ì²˜ë¦¬, ì–‘ìˆ˜ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œ ì²˜ë¦¬
    
    # vLLM ì „ìš© ì„¤ì •
    TENSOR_PARALLEL_SIZE: int = 1  # GPU ìˆ˜
    GPU_MEMORY_UTILIZATION: float = 0.9  # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    MAX_MODEL_LEN: int = 16384  # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
    DTYPE: str = "auto"  # ë˜ëŠ” "float16", "bfloat16"
    TRUST_REMOTE_CODE: bool = True  # Qwen ëª¨ë¸ì— í•„ìš”
    ENFORCE_EAGER: bool = False  # CUDA graphs ì‚¬ìš©
    MAX_NUM_SEQS: int = 256  # ë™ì‹œ ì²˜ë¦¬ ì‹œí€€ìŠ¤ ìˆ˜
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
    BATCH_SIZE: int = 3  # í•œ ë²ˆì— ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜
    MAX_CHUNKS_PER_BATCH: int = 30  # ë°°ì¹˜ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜


@dataclass
class MeetingData:
    """íšŒì˜ ë°ì´í„° êµ¬ì¡°ì²´"""
    transcript: Optional[str] = None
    chunks: Optional[List[str]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def is_chunked(self) -> bool:
        return self.chunks is not None


@dataclass
class ProcessingStats:
    """ì²˜ë¦¬ í†µê³„ ê´€ë¦¬"""
    total: int = 0
    processed: int = 0
    success: int = 0
    failed: int = 0
    chunked: int = 0
    
    @property
    def success_rate(self) -> float:
        if self.processed == 0:
            return 0.0
        return (self.success / self.processed) * 100


class QwenVLLMMeetingProcessor:
    """vLLMì„ ì‚¬ìš©í•œ Qwen AWQ ëª¨ë¸ íšŒì˜ë¡ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: Optional[ModelConfig] = None):
        """
        ìƒì„±ì
        
        Args:
            config: ëª¨ë¸ ì„¤ì • ê°ì²´
        """
        self.config = config or ModelConfig()
        self.model = None
        self.tokenizer = None
        self.sampling_params = None
        
        self._initialize_model()
    
    def _initialize_model(self) -> None:
        """vLLM ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            logger.info(f"ğŸ”§ vLLM ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            logger.info(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {self.config.MODEL_PATH}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.MODEL_PATH,
                trust_remote_code=self.config.TRUST_REMOTE_CODE
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # vLLM ëª¨ë¸ ì´ˆê¸°í™”
            self.model = LLM(
                model=self.config.MODEL_PATH,
                quantization="awq" if "AWQ" in self.config.MODEL_PATH else None,
                tensor_parallel_size=self.config.TENSOR_PARALLEL_SIZE,
                gpu_memory_utilization=self.config.GPU_MEMORY_UTILIZATION,
                max_model_len=self.config.MAX_MODEL_LEN,
                dtype=self.config.DTYPE,
                trust_remote_code=self.config.TRUST_REMOTE_CODE,
                enforce_eager=self.config.ENFORCE_EAGER,
                max_num_seqs=self.config.MAX_NUM_SEQS,
            )
            
            # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
            self.sampling_params = SamplingParams(
                temperature=self.config.TEMPERATURE,
                top_p=self.config.TOP_P,
                repetition_penalty=self.config.REPETITION_PENALTY,
                max_tokens=self.config.MAX_NEW_TOKENS,
                skip_special_tokens=True,
            )
            
            logger.info("âœ… vLLM ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ vLLM ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def find_meeting_files(self, base_dir: str) -> List[Path]:
        """
        íšŒì˜ íŒŒì¼ ê²€ìƒ‰
        
        Args:
            base_dir: ê²€ìƒ‰í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬
            
        Returns:
            ë°œê²¬ëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        base_path = Path(base_dir)
        if not base_path.exists():
            logger.warning(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {base_dir}")
            return []
        
        target_files = list(base_path.rglob("05_final_result.json"))
        logger.info(f"ğŸ“ {len(target_files)}ê°œì˜ íšŒì˜ íŒŒì¼ ë°œê²¬")
        return target_files
    
    def chunk_text(self, text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ì—¬ ë‚˜ëˆ„ê¸° (ë¬¸ì ë‹¨ìœ„)"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end >= len(text):
                chunk = text[start:]
            else:
                chunk = text[start:end]
                
                # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ëŠê¸° ì‹œë„
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                break_point = max(last_period, last_newline)
                
                if break_point > start + chunk_size // 2:
                    chunk = text[start:break_point + 1]
                    end = break_point + 1
            
            chunks.append(chunk.strip())
            
            if end >= len(text):
                break
                
            start = end - overlap
        
        logger.info(f"ğŸ“Š {len(chunks)}ê°œ ì²­í¬ ìƒì„± (ë¬¸ì ê¸°ë°˜ 5000ì)")
        return chunks
    
    def load_meeting_data(self, file_path: Path) -> Optional[MeetingData]:
        """
        íšŒì˜ ë°ì´í„° ë¡œë“œ
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            
        Returns:
            MeetingData ê°ì²´ ë˜ëŠ” None
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # í…ìŠ¤íŠ¸ ë³€í™˜
            meeting_lines = []
            speakers = set()
            
            for item in data:
                timestamp = item.get('timestamp', 'Unknown')
                speaker = item.get('speaker', 'Unknown')
                text = item.get('text', '')
                if text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    speakers.add(speaker)
                    meeting_lines.append(f"[{timestamp}] {speaker}: {text}")
            
            if not meeting_lines:
                logger.warning(f"í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” íŒŒì¼: {file_path}")
                return None
            
            full_text = '\n'.join(meeting_lines)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "source_file": str(file_path),
                "utterance_count": len(data),
                "speakers": list(speakers),
                "original_length": len(full_text)
            }
            
            # ì²­í‚¹ ì—¬ë¶€ ê²°ì •
            if len(full_text) > self.config.CHUNK_SIZE:
                logger.info(f"ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ ({len(full_text)}ì) - ì²­í‚¹ ì²˜ë¦¬")
                chunks = self.chunk_text(full_text, self.config.CHUNK_SIZE, self.config.CHUNK_OVERLAP)
                metadata["chunking_info"] = {
                    "is_chunked": True,
                    "total_chunks": len(chunks)
                }
                return MeetingData(chunks=chunks, metadata=metadata)
            else:
                metadata["chunking_info"] = {
                    "is_chunked": False,
                    "total_chunks": 1
                }
                return MeetingData(transcript=full_text, metadata=metadata)
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
            return None
    
    def generate_notion_project_prompt(self, meeting_transcript: str) -> str:
        """ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„± í”„ë¡¬í”„íŠ¸"""
        return f"""ë‹¤ìŒ íšŒì˜ ì „ì‚¬ë³¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì— ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.

**íšŒì˜ ì „ì‚¬ë³¸:**
{meeting_transcript}

**ì‘ì„± ì§€ì¹¨:**
1. íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ê¸°íšì•ˆì„ ì‘ì„±
2. í”„ë¡œì íŠ¸ëª…ì€ íšŒì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆíˆ ëª…ëª…
3. ëª©ì ê³¼ ëª©í‘œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
4. ì‹¤í–‰ ê³„íšì€ ì‹¤í˜„ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ë¡œ êµ¬ì„±
5. ê¸°ëŒ€ íš¨ê³¼ëŠ” ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ë¥¼ í¬í•¨
6. ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±

**ì‘ë‹µ í˜•ì‹:**
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "project_name": "í”„ë¡œì íŠ¸ëª…",
    "project_purpose": "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ",
    "project_period": "ì˜ˆìƒ ìˆ˜í–‰ ê¸°ê°„",
    "project_manager": "ë‹´ë‹¹ìëª…",
    "core_objectives": ["ëª©í‘œ 1", "ëª©í‘œ 2", "ëª©í‘œ 3"],
    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´",
    "idea_description": "ì•„ì´ë””ì–´ ì„¤ëª…",
    "execution_plan": "ì‹¤í–‰ ê³„íš",
    "expected_effects": ["íš¨ê³¼ 1", "íš¨ê³¼ 2", "íš¨ê³¼ 3"]
}}"""
    
    def generate_batch_responses(self, prompts: List[str]) -> List[Optional[str]]:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ vLLM ìƒì„±
        
        Args:
            prompts: í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not prompts:
                return []
            
            logger.info(f"ğŸš€ {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
            
            # vLLM ë°°ì¹˜ ìƒì„±
            outputs = self.model.generate(
                prompts=prompts,
                sampling_params=self.sampling_params
            )
            
            # ê²°ê³¼ ì¶”ì¶œ
            results = []
            for output in outputs:
                if output.outputs:
                    results.append(output.outputs[0].text.strip())
                else:
                    results.append("{}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return ["{}" for _ in prompts]
    
    def parse_json_response(self, response: str) -> Dict[str, Any]:
        """
        JSON ì‘ë‹µ íŒŒì‹±
        
        Args:
            response: ëª¨ë¸ ì‘ë‹µ ë¬¸ìì—´
            
        Returns:
            íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "{" in response and "}" in response:
                start = response.find("{")
                end = response.rfind("}") + 1
                json_str = response[start:end]
            else:
                return {"error": "No JSON found", "raw_text": response}
            
            # JSON íŒŒì‹±
            parsed = json.loads(json_str)
            
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ["project_name", "project_purpose"]
            for field in required_fields:
                if field not in parsed:
                    parsed[field] = "ë¯¸ì •"
            
            return parsed
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"error": "JSON parsing failed", "raw_text": response}
        except Exception as e:
            logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return {"error": str(e), "raw_text": response}
    
    def process_batch(self, 
                     batch_data: List[Tuple[str, Path, MeetingData]],
                     output_dir: Path) -> Tuple[int, int]:
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ íŒŒì¼ ì²˜ë¦¬
        
        Args:
            batch_data: (í´ë”ëª…, íŒŒì¼ê²½ë¡œ, íšŒì˜ë°ì´í„°) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            (ì„±ê³µ ìˆ˜, ì‹¤íŒ¨ ìˆ˜) íŠœí”Œ
        """
        success_count = 0
        fail_count = 0
        
        # ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        all_prompts = []
        metadata_list = []
        
        for folder_name, file_path, meeting_data in batch_data:
            if meeting_data.is_chunked:
                # ì²­í‚¹ëœ ë°ì´í„°
                for chunk_idx, chunk_text in enumerate(meeting_data.chunks):
                    prompt = self.generate_notion_project_prompt(chunk_text)
                    all_prompts.append(prompt)
                    metadata_list.append({
                        "folder_name": folder_name,
                        "file_path": file_path,
                        "chunk_idx": chunk_idx,
                        "total_chunks": len(meeting_data.chunks),
                        "meeting_metadata": meeting_data.metadata
                    })
            else:
                # ë‹¨ì¼ í…ìŠ¤íŠ¸
                prompt = self.generate_notion_project_prompt(meeting_data.transcript)
                all_prompts.append(prompt)
                metadata_list.append({
                    "folder_name": folder_name,
                    "file_path": file_path,
                    "chunk_idx": 0,
                    "total_chunks": 1,
                    "meeting_metadata": meeting_data.metadata
                })
        
        # ë°°ì¹˜ ìƒì„±
        responses = self.generate_batch_responses(all_prompts)
        
        # ê²°ê³¼ ì €ì¥
        for response, meta in zip(responses, metadata_list):
            try:
                # ë””ë ‰í† ë¦¬ ê²°ì •
                if meta["total_chunks"] == 1:
                    save_dir = output_dir / meta["folder_name"]
                    save_id = meta["folder_name"]
                else:
                    save_dir = output_dir / f"{meta['folder_name']}_chunk_{meta['chunk_idx']+1}"
                    save_id = f"{meta['folder_name']}_chunk_{meta['chunk_idx']+1}"
                
                save_dir.mkdir(parents=True, exist_ok=True)
                
                # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                result_data = {
                    "id": save_id,
                    "source_dir": meta["folder_name"],
                    "notion_output": self.parse_json_response(response),
                    "metadata": {
                        **meta["meeting_metadata"],
                        "is_chunk": meta["total_chunks"] > 1,
                        "chunk_index": meta["chunk_idx"] + 1 if meta["total_chunks"] > 1 else None,
                        "total_chunks": meta["total_chunks"],
                        "processing_date": datetime.now().isoformat(),
                        "model_used": self.config.MODEL_PATH
                    }
                }
                
                # JSON ì €ì¥
                with open(save_dir / "result.json", 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2)
                
                success_count += 1
                
                if meta["total_chunks"] > 1:
                    logger.info(f"âœ… ì²­í¬ {meta['chunk_idx']+1}/{meta['total_chunks']} ì €ì¥ ì™„ë£Œ: {save_id}")
                else:
                    logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {save_id}")
                    
            except Exception as e:
                logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨ ({save_id}): {e}")
                fail_count += 1
        
        return success_count, fail_count
    
    def process_all_files(self, 
                         meeting_files: List[Path],
                         output_dir: Path) -> ProcessingStats:
        """
        ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
        
        Args:
            meeting_files: ì²˜ë¦¬í•  íŒŒì¼ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            ì²˜ë¦¬ í†µê³„
        """
        stats = ProcessingStats(total=len(meeting_files))
        
        # ë°°ì¹˜ ì¤€ë¹„
        current_batch = []
        current_chunk_count = 0
        batch_num = 1
        
        for file_path in meeting_files:
            parent_folder = file_path.parent.name
            
            try:
                # ë°ì´í„° ë¡œë“œ
                meeting_data = self.load_meeting_data(file_path)
                if not meeting_data:
                    stats.failed += 1
                    stats.processed += 1
                    logger.warning(f"âš ï¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {file_path}")
                    continue
                
                # ì²­í‚¹ í†µê³„
                if meeting_data.is_chunked:
                    stats.chunked += 1
                    chunk_count = len(meeting_data.chunks)
                else:
                    chunk_count = 1
                
                # ë°°ì¹˜ í¬ê¸° í™•ì¸
                if (current_chunk_count + chunk_count > self.config.MAX_CHUNKS_PER_BATCH 
                    and current_batch):
                    # í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
                    logger.info(f"\nğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({current_chunk_count}ê°œ ì²­í¬)")
                    success, fail = self.process_batch(current_batch, output_dir)
                    stats.success += success
                    stats.failed += fail
                    stats.processed += success + fail
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # ìƒˆ ë°°ì¹˜ ì‹œì‘
                    batch_num += 1
                    current_batch = [(parent_folder, file_path, meeting_data)]
                    current_chunk_count = chunk_count
                else:
                    # í˜„ì¬ ë°°ì¹˜ì— ì¶”ê°€
                    current_batch.append((parent_folder, file_path, meeting_data))
                    current_chunk_count += chunk_count
                    
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path}): {e}")
                stats.failed += 1
                stats.processed += 1
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            logger.info(f"\nğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({current_chunk_count}ê°œ ì²­í¬)")
            success, fail = self.process_batch(current_batch, output_dir)
            stats.success += success
            stats.failed += fail
            stats.processed += success + fail
        
        return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("ğŸš€ vLLMì„ ì‚¬ìš©í•œ Qwen AWQ ëª¨ë¸ íšŒì˜ë¡ ì²˜ë¦¬ ì‹œì‘!")
    logger.info("=" * 60)
    
    # ì„¤ì • ë¡œë“œ
    config = ModelConfig()
    
    # ì…ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    input_dir = "../Raw_Data_val"
    output_dir = Path("4B_awq_model_results_structured")
    output_dir.mkdir(exist_ok=True)
    
    logger.info(f"ğŸ“‚ ì…ë ¥: {input_dir}")
    logger.info(f"ğŸ“‚ ì¶œë ¥: {output_dir}")
    logger.info(f"âš™ï¸ ë°°ì¹˜ í¬ê¸°: {config.BATCH_SIZE}, ìµœëŒ€ ì²­í¬/ë°°ì¹˜: {config.MAX_CHUNKS_PER_BATCH}")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    try:
        processor = QwenVLLMMeetingProcessor(config)
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # íšŒì˜ íŒŒì¼ ê²€ìƒ‰
    meeting_files = processor.find_meeting_files(input_dir)
    
    if not meeting_files:
        logger.error(f"{input_dir} í´ë”ì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì²˜ë¦¬
    if config.TEST_FILE_LIMIT > 0:
        meeting_files = meeting_files[:config.TEST_FILE_LIMIT]
        logger.info(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {len(meeting_files)}ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬")
    else:
        logger.info(f"ğŸ“‹ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ: {len(meeting_files)}ê°œ íŒŒì¼")
    
    # ì²˜ë¦¬ ì‹œì‘
    start_time = datetime.now()
    stats = processor.process_all_files(meeting_files, output_dir)
    elapsed_time = datetime.now() - start_time
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 60)
    logger.info("âœ… ì²˜ë¦¬ ì™„ë£Œ í†µê³„:")
    logger.info(f"  ğŸ“ ì „ì²´ íŒŒì¼: {stats.total}ê°œ")
    logger.info(f"  âœ”ï¸ ì²˜ë¦¬ ì™„ë£Œ: {stats.processed}ê°œ")
    logger.info(f"  âœ… ì„±ê³µ: {stats.success}ê°œ")
    logger.info(f"  âŒ ì‹¤íŒ¨: {stats.failed}ê°œ")
    logger.info(f"  ğŸ“Š ì²­í‚¹ ì²˜ë¦¬: {stats.chunked}ê°œ")
    logger.info(f"  ğŸ“ˆ ì„±ê³µë¥ : {stats.success_rate:.1f}%")
    logger.info(f"  â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time}")
    logger.info(f"\nğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼ëŠ” {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()