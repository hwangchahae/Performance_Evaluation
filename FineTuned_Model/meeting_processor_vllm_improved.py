# Improved version with better chunking and batch processing
import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime
from typing import List, Dict, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ëª¨ë¸ ì„ íƒ
model_path = "Qwen/Qwen3-4B-AWQ"
logger.info(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# ì „ì—­ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €
llm = None
tokenizer = None
sampling_params = None

def initialize_model():
    """ëª¨ë¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)"""
    global llm, tokenizer, sampling_params
    
    if llm is None:
        logger.info(f"ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # VLLM ì—”ì§„ ì´ˆê¸°í™” - ì†ë„ ìµœì í™”
        llm = LLM(
            model=model_path,
            tensor_parallel_size=1,
            max_model_len=8192,  # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê°ì†Œë¡œ ì†ë„ í–¥ìƒ
            gpu_memory_utilization=0.85,
            trust_remote_code=True,
            enforce_eager=False,  # CUDA graphs ì‚¬ìš©
            max_num_seqs=512,  # ë” ë§ì€ ë™ì‹œ ì²˜ë¦¬
            enable_prefix_caching=True,
            max_num_batched_tokens=8192,
            dtype="half",  # float16ìœ¼ë¡œ ì†ë„ í–¥ìƒ
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° - ì†ë„ ìµœì í™”
        sampling_params = SamplingParams(
            temperature=0.1,  # ë” ë‚®ì€ temperatureë¡œ ë¹ ë¥¸ ìˆ˜ë ´
            max_tokens=1024,  # í† í° ìˆ˜ ê°ì†Œ
            skip_special_tokens=True,
            top_p=0.9,
            repetition_penalty=1.05,
        )
        logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

def generate_notion_project_prompt(meeting_transcript: str) -> str:
    """ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„± í”„ë¡¬í”„íŠ¸"""
    return f"""ë‹¤ìŒ íšŒì˜ ì „ì‚¬ë³¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì— ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.

**íšŒì˜ ì „ì‚¬ë³¸:**
{meeting_transcript}

**ì‘ì„± ì§€ì¹¨:**
1. íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ê¸°íšì•ˆì„ ì‘ì„±
2. í”„ë¡œì íŠ¸ëª…ì€ íšŒì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆíˆ ëª…ëª…
3. ëª©ì ê³¼ ëª©í‘œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
4. ì‹¤í–‰ ê³„íšì€ ì‹¤í˜„ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ë¡œ êµ¬ì„±
5. ê¸°ëŒ€ íš¨ê³¼ëŠ” ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ë¥¼ í¬í•¨
6. ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±

**ì‘ë‹µ í˜•ì‹:**
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "project_name": "í”„ë¡œì íŠ¸ëª…",
    "project_purpose": "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ",
    "project_period": "ì˜ˆìƒ ìˆ˜í–‰ ê¸°ê°„",
    "project_manager": "ë‹´ë‹¹ìëª…",
    "core_objectives": ["ëª©í‘œ 1", "ëª©í‘œ 2", "ëª©í‘œ 3"],
    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´",
    "idea_description": "ì•„ì´ë””ì–´ ì„¤ëª…",
    "execution_plan": "ì‹¤í–‰ ê³„íš",
    "expected_effects": ["íš¨ê³¼ 1", "íš¨ê³¼ 2", "íš¨ê³¼ 3"]
}}"""

def chunk_text(text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ì—¬ ë‚˜ëˆ„ê¸° (ë¬¸ì ë‹¨ìœ„) - qwen3_loraì™€ ë™ì¼"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunk = text[start:]
        else:
            chunk = text[start:end]
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ëŠê¸° ì‹œë„
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            break_point = max(last_period, last_newline)
            
            if break_point > start + chunk_size // 2:
                chunk = text[start:break_point + 1]
                end = break_point + 1
        
        chunks.append(chunk.strip())
        
        if end >= len(text):
            break
            
        start = end - overlap
    
    logger.info(f"ğŸ“Š {len(chunks)}ê°œ ì²­í¬ ìƒì„± (ë¬¸ì ê¸°ë°˜ 5000ì)")
    return chunks

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ""
    text = re.sub(r'\[TGT\]|\[/TGT\]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_json_file(file_path):
    """JSON íŒŒì¼ ë¡œë“œ"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
            if isinstance(data, list):
                # qwen3_loraì™€ ë™ì¼í•˜ê²Œ clean_text ì œê±°
                return [{"timestamp": item.get("timestamp", "Unknown"),
                        "speaker": item.get("speaker", "Unknown"), 
                        "text": item.get("text", "")} 
                       for item in data]  # ëª¨ë“  item í¬í•¨ (text ì—†ì–´ë„)
            return []
    except Exception as e:
        logger.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
        return []

def batch_generate_responses(prompts: List[str]) -> List[str]:
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ìƒì„± - ìµœëŒ€ ì†ë„"""
    if not prompts:
        return []
    
    logger.info(f"ğŸš€ {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
    
    # ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
    outputs = llm.generate(prompts, sampling_params, use_tqdm=False)
    
    # ë¹ ë¥¸ ê²°ê³¼ ì¶”ì¶œ
    return [output.outputs[0].text.strip() if output.outputs else "{}" for output in outputs]

def parse_json_response(response: str) -> Dict:
    """JSON ì‘ë‹µ íŒŒì‹±"""
    try:
        if "```json" in response:
            start = response.find("```json") + 7
            end = response.find("```", start)
            return json.loads(response[start:end].strip())
        elif "{" in response:
            start = response.find("{")
            end = response.rfind("}") + 1
            return json.loads(response[start:end])
    except:
        pass
    return {"raw_text": response}

def process_files_batch(files_data: List[Tuple[str, str, List[str]]]) -> List[Dict]:
    """ì—¬ëŸ¬ íŒŒì¼ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
    all_prompts = []
    metadata = []
    
    # ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    for folder_name, file_path, chunks in files_data:
        for idx, chunk in enumerate(chunks):
            prompt = generate_notion_project_prompt(chunk)
            all_prompts.append(prompt)
            metadata.append({
                "folder_name": folder_name,
                "file_path": file_path,
                "chunk_idx": idx,
                "total_chunks": len(chunks),
                "is_chunked": len(chunks) > 1
            })
    
    # ë°°ì¹˜ ìƒì„±
    logger.info(f"ğŸ”„ {len(all_prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ìƒì„± ì¤‘...")
    responses = batch_generate_responses(all_prompts)
    
    # ê²°ê³¼ ì •ë¦¬
    results = []
    for response, meta in zip(responses, metadata):
        result = {
            "folder_name": meta["folder_name"],
            "chunk_idx": meta["chunk_idx"],
            "total_chunks": meta["total_chunks"],
            "response": parse_json_response(response),
            "metadata": meta
        }
        results.append(result)
    
    logger.info(f"âœ… {len(results)}ê°œ ê²°ê³¼ ìƒì„± ì™„ë£Œ")
    return results

def save_results(results: List[Dict], output_dir: str):
    """ê²°ê³¼ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    # í´ë”ë³„ë¡œ ê·¸ë£¹í™”
    grouped = {}
    for result in results:
        folder = result["folder_name"]
        if folder not in grouped:
            grouped[folder] = []
        grouped[folder].append(result)
    
    # ì €ì¥ - ëª¨ë“  íŒŒì¼ ê°œë³„ í´ë”ë¡œ ì €ì¥
    saved_count = 0
    for folder_name, folder_results in grouped.items():
        total_chunks = len(folder_results)
        for result in folder_results:
            # qwen3_loraì™€ ë™ì¼í•œ ë¡œì§: ì²­í¬ê°€ 1ê°œë©´ _chunk_X ë¶™ì´ì§€ ì•ŠìŒ
            if result["metadata"]["is_chunked"] and total_chunks > 1:
                # ì²­í‚¹ëœ íŒŒì¼ì´ê³  ì²­í¬ê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ _chunk_ ë¶™ì„
                chunk_dir = os.path.join(output_dir, f"{folder_name}_chunk_{result['chunk_idx']+1}")
                chunk_id = f"{folder_name}_chunk_{result['chunk_idx']+1}"
            else:
                # ì²­í¬ê°€ 1ê°œì´ê±°ë‚˜ ì²­í‚¹ë˜ì§€ ì•Šì€ íŒŒì¼
                chunk_dir = os.path.join(output_dir, folder_name)
                chunk_id = folder_name
            
            os.makedirs(chunk_dir, exist_ok=True)
            
            output_data = {
                "id": chunk_id,
                "source_dir": folder_name,
                "notion_output": result["response"],
                "metadata": {
                    "source_file": result["metadata"]["file_path"],
                    "is_chunk": result["metadata"]["is_chunked"],
                    "chunk_index": result["chunk_idx"] + 1 if result["metadata"]["is_chunked"] else None,
                    "total_chunks": result["total_chunks"],
                    "processing_date": datetime.now().isoformat()
                }
            }
            
            with open(os.path.join(chunk_dir, "result.json"), 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            saved_count += 1
    
    logger.info(f"âœ… {saved_count}ê°œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ ({len(grouped)}ê°œ ì›ë³¸ íŒŒì¼)")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì • - ì†ë„ ìµœì í™”
    base_directory = "../Raw_Data_val"
    output_directory = "4B_awq_model_results_improved"
    batch_size = 30  # ë” í° ë°°ì¹˜ í¬ê¸°
    max_chunks_per_batch = 300  # ë” ë§ì€ ì²­í¬ë¥¼ í•œë²ˆì— ì²˜ë¦¬
    
    logger.info(f"ğŸš€ ê°œì„ ëœ ì²˜ë¦¬ ì‹œì‘")
    logger.info(f"ğŸ“‚ ì…ë ¥: {base_directory}")
    logger.info(f"ğŸ“‚ ì¶œë ¥: {output_directory}")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    initialize_model()
    
    # íŒŒì¼ ì°¾ê¸°
    target_files = []
    for folder in os.listdir(base_directory):
        folder_path = os.path.join(base_directory, folder)
        if os.path.isdir(folder_path):
            json_file = os.path.join(folder_path, "05_final_result.json")
            if os.path.exists(json_file):
                target_files.append((folder, json_file))
    
    logger.info(f"ğŸ“ {len(target_files)}ê°œ íŒŒì¼ ë°œê²¬")
    
    # ëª¨ë“  íŒŒì¼ì„ í•œë²ˆì— ì²˜ë¦¬ ì¤€ë¹„ (ìµœëŒ€ ì†ë„)
    all_files_data = []
    total_chunks = 0
    
    for folder_name, file_path in target_files:
        utterances = load_json_file(file_path)
        if not utterances:
            logger.warning(f"âš ï¸ {folder_name} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue
            
        # í…ìŠ¤íŠ¸ ê²°í•© ë° ì²­í‚¹ (qwen3_loraì™€ ë™ì¼)
        meeting_lines = [f"[{utt.get('timestamp', 'Unknown')}] {utt.get('speaker', 'Unknown')}: {utt.get('text', '')}" 
                        for utt in utterances]
        full_text = "\n".join(meeting_lines)
        chunks = chunk_text(full_text, chunk_size=5000, overlap=512)
        
        all_files_data.append((folder_name, file_path, chunks))
        total_chunks += len(chunks)
        logger.info(f"ğŸ“„ {folder_name}: {len(chunks)}ê°œ ì²­í¬")
    
    logger.info(f"ğŸ“Š ì´ {len(all_files_data)}ê°œ íŒŒì¼, {total_chunks}ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œì‘")
    
    # ëª¨ë“  íŒŒì¼ì„ í•œë²ˆì— ë°°ì¹˜ ì²˜ë¦¬
    results = process_files_batch(all_files_data)
    save_results(results, output_directory)
    
    logger.info("ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")

if __name__ == "__main__":
    main()