# Improved version with better chunking and batch processing
import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime
from typing import List, Dict, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ëª¨ë¸ ì„ íƒ
model_path = "Qwen/Qwen3-4B-AWQ"
logger.info(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# ì „ì—­ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €
llm = None
tokenizer = None
sampling_params = None

def initialize_model():
    """ëª¨ë¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)"""
    global llm, tokenizer, sampling_params
    
    if llm is None:
        logger.info(f"ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # VLLM ì—”ì§„ ì´ˆê¸°í™”
        llm = LLM(
            model=model_path,
            quantization="awq",  # AWQ ì–‘ìí™” ì‚¬ìš©
            tensor_parallel_size=1,
            max_model_len=16384,
            gpu_memory_utilization=0.85,  # GPU ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ë³´ (44.95GB ê°€ìš©)
            trust_remote_code=True,
            enforce_eager=False,
            max_num_seqs=256,  # ë™ì‹œ ì²˜ë¦¬ ìˆ˜
            enable_prefix_caching=True,  # í”„ë¦¬í”½ìŠ¤ ìºì‹± í™œì„±í™”
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        sampling_params = SamplingParams(
            temperature=0.2,
            max_tokens=2048,
            skip_special_tokens=True,
        )
        logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

def generate_notion_project_prompt(meeting_transcript: str) -> str:
    """ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„± í”„ë¡¬í”„íŠ¸"""
    return f"""ë‹¤ìŒ íšŒì˜ ì „ì‚¬ë³¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì— ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.

**íšŒì˜ ì „ì‚¬ë³¸:**
{meeting_transcript}

**ì‘ì„± ì§€ì¹¨:**
1. íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ê¸°íšì•ˆì„ ì‘ì„±
2. í”„ë¡œì íŠ¸ëª…ì€ íšŒì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆíˆ ëª…ëª…
3. ëª©ì ê³¼ ëª©í‘œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
4. ì‹¤í–‰ ê³„íšì€ ì‹¤í˜„ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ë¡œ êµ¬ì„±
5. ê¸°ëŒ€ íš¨ê³¼ëŠ” ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ë¥¼ í¬í•¨
6. ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±

**ì‘ë‹µ í˜•ì‹:**
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "project_name": "í”„ë¡œì íŠ¸ëª…",
    "project_purpose": "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ",
    "project_period": "ì˜ˆìƒ ìˆ˜í–‰ ê¸°ê°„",
    "project_manager": "ë‹´ë‹¹ìëª…",
    "core_objectives": ["ëª©í‘œ 1", "ëª©í‘œ 2", "ëª©í‘œ 3"],
    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´",
    "idea_description": "ì•„ì´ë””ì–´ ì„¤ëª…",
    "execution_plan": "ì‹¤í–‰ ê³„íš",
    "expected_effects": ["íš¨ê³¼ 1", "íš¨ê³¼ 2", "íš¨ê³¼ 3"]
}}"""

def chunk_text(text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ì—¬ ë‚˜ëˆ„ê¸° (ë¬¸ì ë‹¨ìœ„) - qwen3_loraì™€ ë™ì¼"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunk = text[start:]
        else:
            chunk = text[start:end]
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ëŠê¸° ì‹œë„
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            break_point = max(last_period, last_newline)
            
            if break_point > start + chunk_size // 2:
                chunk = text[start:break_point + 1]
                end = break_point + 1
        
        chunks.append(chunk.strip())
        
        if end >= len(text):
            break
            
        start = end - overlap
    
    logger.info(f"ğŸ“Š {len(chunks)}ê°œ ì²­í¬ ìƒì„± (ë¬¸ì ê¸°ë°˜ 5000ì)")
    return chunks

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ""
    text = re.sub(r'\[TGT\]|\[/TGT\]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_json_file(file_path):
    """JSON íŒŒì¼ ë¡œë“œ"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
            if isinstance(data, list):
                # qwen3_loraì™€ ë™ì¼í•˜ê²Œ clean_text ì œê±°
                return [{"timestamp": item.get("timestamp", "Unknown"),
                        "speaker": item.get("speaker", "Unknown"), 
                        "text": item.get("text", "")} 
                       for item in data]  # ëª¨ë“  item í¬í•¨ (text ì—†ì–´ë„)
            return []
    except Exception as e:
        logger.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
        return []

def batch_generate_responses(prompts: List[str]) -> List[str]:
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ìƒì„±"""
    if not prompts:
        return []
    
    logger.info(f"ğŸš€ {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
    
    # ë” í° ë°°ì¹˜ í¬ê¸°ë¡œ í•œ ë²ˆì— ì²˜ë¦¬
    try:
        outputs = llm.generate(prompts, sampling_params, use_tqdm=False)  # tqdm ë¹„í™œì„±í™”ë¡œ ì†ë„ ê°œì„ 
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„
        results = []
        for i in range(0, len(prompts), 10):
            sub_batch = prompts[i:i+10]
            sub_outputs = llm.generate(sub_batch, sampling_params, use_tqdm=False)
            for output in sub_outputs:
                if output.outputs:
                    results.append(output.outputs[0].text.strip())
                else:
                    results.append("{}")
        return results
    
    results = []
    for output in outputs:
        if output.outputs:
            results.append(output.outputs[0].text.strip())
        else:
            results.append("{}")
    
    return results

def parse_json_response(response: str) -> Dict:
    """JSON ì‘ë‹µ íŒŒì‹±"""
    try:
        if "```json" in response:
            start = response.find("```json") + 7
            end = response.find("```", start)
            return json.loads(response[start:end].strip())
        elif "{" in response:
            start = response.find("{")
            end = response.rfind("}") + 1
            return json.loads(response[start:end])
    except:
        pass
    return {"raw_text": response}

def process_files_batch(files_data: List[Tuple[str, str, List[str]]]) -> List[Dict]:
    """ì—¬ëŸ¬ íŒŒì¼ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
    all_prompts = []
    metadata = []
    
    # ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    for folder_name, file_path, chunks in files_data:
        for idx, chunk in enumerate(chunks):
            prompt = generate_notion_project_prompt(chunk)
            all_prompts.append(prompt)
            metadata.append({
                "folder_name": folder_name,
                "file_path": file_path,
                "chunk_idx": idx,
                "total_chunks": len(chunks),
                "is_chunked": len(chunks) > 1
            })
    
    # ë°°ì¹˜ ìƒì„±
    logger.info(f"ğŸ”„ {len(all_prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ìƒì„± ì¤‘...")
    responses = batch_generate_responses(all_prompts)
    
    # ê²°ê³¼ ì •ë¦¬
    results = []
    for response, meta in zip(responses, metadata):
        result = {
            "folder_name": meta["folder_name"],
            "chunk_idx": meta["chunk_idx"],
            "total_chunks": meta["total_chunks"],
            "response": parse_json_response(response),
            "metadata": meta
        }
        results.append(result)
    
    logger.info(f"âœ… {len(results)}ê°œ ê²°ê³¼ ìƒì„± ì™„ë£Œ")
    return results

def save_results(results: List[Dict], output_dir: str):
    """ê²°ê³¼ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    # í´ë”ë³„ë¡œ ê·¸ë£¹í™”
    grouped = {}
    for result in results:
        folder = result["folder_name"]
        if folder not in grouped:
            grouped[folder] = []
        grouped[folder].append(result)
    
    # ì €ì¥ - ëª¨ë“  íŒŒì¼ ê°œë³„ í´ë”ë¡œ ì €ì¥
    saved_count = 0
    for folder_name, folder_results in grouped.items():
        for result in folder_results:
            # ì²­í‚¹ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ëª¨ë“  íŒŒì¼ì— ëŒ€í•´ í´ë” ìƒì„±
            if result["metadata"]["is_chunked"]:
                # ì²­í‚¹ëœ íŒŒì¼
                chunk_dir = os.path.join(output_dir, f"{folder_name}_chunk_{result['chunk_idx']+1}")
                chunk_id = f"{folder_name}_chunk_{result['chunk_idx']+1}"
            else:
                # ì²­í‚¹ë˜ì§€ ì•Šì€ íŒŒì¼ë„ í´ë” ìƒì„±
                chunk_dir = os.path.join(output_dir, folder_name)
                chunk_id = folder_name
            
            os.makedirs(chunk_dir, exist_ok=True)
            
            output_data = {
                "id": chunk_id,
                "source_dir": folder_name,
                "notion_output": result["response"],
                "metadata": {
                    "source_file": result["metadata"]["file_path"],
                    "is_chunk": result["metadata"]["is_chunked"],
                    "chunk_index": result["chunk_idx"] + 1 if result["metadata"]["is_chunked"] else None,
                    "total_chunks": result["total_chunks"],
                    "processing_date": datetime.now().isoformat()
                }
            }
            
            with open(os.path.join(chunk_dir, "result.json"), 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            saved_count += 1
    
    logger.info(f"âœ… {saved_count}ê°œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ ({len(grouped)}ê°œ ì›ë³¸ íŒŒì¼)")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì • - ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
    base_directory = "../Raw_Data_val"  # Performance_Evaluation/Raw_Data_val
    output_directory = "4B_awq_model_results_improved"  # í˜„ì¬ í´ë”ì— ìƒì„±
    batch_size = 10  # í•œ ë²ˆì— ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ (ì¦ê°€ì‹œì¼œ ì†ë„ ê°œì„ )
    max_chunks_per_batch = 100  # ë°°ì¹˜ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜ (ì¦ê°€ì‹œì¼œ ì†ë„ ê°œì„ )
    
    logger.info(f"ğŸš€ ê°œì„ ëœ ì²˜ë¦¬ ì‹œì‘")
    logger.info(f"ğŸ“‚ ì…ë ¥: {base_directory}")
    logger.info(f"ğŸ“‚ ì¶œë ¥: {output_directory}")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    initialize_model()
    
    # íŒŒì¼ ì°¾ê¸°
    target_files = []
    for folder in os.listdir(base_directory):
        folder_path = os.path.join(base_directory, folder)
        if os.path.isdir(folder_path):
            json_file = os.path.join(folder_path, "05_final_result.json")
            if os.path.exists(json_file):
                target_files.append((folder, json_file))
    
    logger.info(f"ğŸ“ {len(target_files)}ê°œ íŒŒì¼ ë°œê²¬")
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ì²­í¬ ìˆ˜ ì œí•œ ì¶”ê°€)
    current_batch = []
    current_chunk_count = 0
    batch_num = 1
    total_processed = 0
    
    for folder_name, file_path in target_files:
        utterances = load_json_file(file_path)
        if not utterances:
            logger.warning(f"âš ï¸ {folder_name} íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue
            
        # í…ìŠ¤íŠ¸ ê²°í•© ë° ì²­í‚¹ (qwen3_loraì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
        meeting_lines = []
        for utt in utterances:
            timestamp = utt.get('timestamp', 'Unknown')
            speaker = utt.get('speaker', 'Unknown')
            text = utt.get('text', '')
            meeting_lines.append(f"[{timestamp}] {speaker}: {text}")
        full_text = "\n".join(meeting_lines)
        chunks = chunk_text(full_text, chunk_size=5000, overlap=512)
        
        logger.info(f"ğŸ“„ {folder_name}: {len(chunks)}ê°œ ì²­í¬ ìƒì„± (ì›ë³¸ {len(full_text)}ì)")
        
        # ì²­í¬ ìˆ˜ í™•ì¸
        if current_chunk_count + len(chunks) > max_chunks_per_batch and current_batch:
            # í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({len(current_batch)}ê°œ íŒŒì¼, {current_chunk_count}ê°œ ì²­í¬)")
            results = process_files_batch(current_batch)
            save_results(results, output_directory)
            total_processed += len(current_batch)
            
            # ìƒˆ ë°°ì¹˜ ì‹œì‘
            batch_num += 1
            current_batch = [(folder_name, file_path, chunks)]
            current_chunk_count = len(chunks)
        else:
            # í˜„ì¬ ë°°ì¹˜ì— ì¶”ê°€
            current_batch.append((folder_name, file_path, chunks))
            current_chunk_count += len(chunks)
    
    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
    if current_batch:
        logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({len(current_batch)}ê°œ íŒŒì¼, {current_chunk_count}ê°œ ì²­í¬)")
        results = process_files_batch(current_batch)
        save_results(results, output_directory)
        total_processed += len(current_batch)
    
    logger.info(f"ğŸ“Š ì´ {total_processed}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
    
    logger.info("ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")

if __name__ == "__main__":
    main()