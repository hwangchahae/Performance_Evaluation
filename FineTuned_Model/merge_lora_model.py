"""
Standalone LoRA merge script for Qwen3-8B model
ë³‘í•© ì‹¤íŒ¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë…ë¦½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def merge_lora_model():
    """LoRA ëª¨ë¸ ë³‘í•©"""
    
    # ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).parent
    base_model_path = "Qwen/Qwen3-8B"
    lora_path = script_dir / "qwen3_lora_ttalkkac_8b"
    merged_path = script_dir / "8B_merged_qwen3_lora_model"
    
    # ë³‘í•©ëœ ëª¨ë¸ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
    if merged_path.exists():
        logger.info(f"âœ… ë³‘í•©ëœ ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬: {merged_path}")
        return str(merged_path)
    
    # LoRA ì–´ëŒ‘í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    if not lora_path.exists():
        logger.error(f"âŒ LoRA ì–´ëŒ‘í„°ê°€ ì—†ìŒ: {lora_path}")
        return None
    
    logger.info("=" * 60)
    logger.info("LoRA ë³‘í•© ì‹œì‘")
    logger.info("=" * 60)
    
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©
        logger.info(f"ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©: {base_model_path}")
        logger.info("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ CPUì—ì„œ ë¡œë“œ í›„ ë³‘í•©í•©ë‹ˆë‹¤...")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,  # bfloat16 ëŒ€ì‹  float16 ì‚¬ìš©
            device_map="cpu",  # CPUì—ì„œ ë³‘í•©
            trust_remote_code=True,
            low_cpu_mem_usage=True  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
        )
        
        logger.info(f"ğŸ“¥ LoRA ì–´ëŒ‘í„° ë¡œë”©: {lora_path}")
        model = PeftModel.from_pretrained(
            base_model, 
            str(lora_path),
            device_map="cpu"
        )
        
        logger.info("ğŸ”€ ëª¨ë¸ ë³‘í•© ì¤‘...")
        merged_model = model.merge_and_unload()
        
        logger.info(f"ğŸ’¾ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥: {merged_path}")
        merged_path.mkdir(exist_ok=True)
        merged_model.save_pretrained(
            str(merged_path),
            safe_serialization=True,  # safetensors í˜•ì‹ìœ¼ë¡œ ì €ì¥
            max_shard_size="4GB"  # í° ëª¨ë¸ì„ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„í• 
        )
        
        # í† í¬ë‚˜ì´ì €ë„ ì €ì¥
        logger.info("ğŸ’¾ í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True
        )
        tokenizer.save_pretrained(str(merged_path))
        
        logger.info("âœ… LoRA ë³‘í•© ì™„ë£Œ!")
        logger.info(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {merged_path}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del base_model
        del model
        del merged_model
        torch.cuda.empty_cache()
        
        return str(merged_path)
        
    except Exception as e:
        logger.error(f"âŒ LoRA ë³‘í•© ì‹¤íŒ¨: {e}")
        logger.error("ê°€ëŠ¥í•œ ì›ì¸:")
        logger.error("1. ë©”ëª¨ë¦¬ ë¶€ì¡± - 8B ëª¨ë¸ ë³‘í•©ì—ëŠ” ìµœì†Œ 32GB RAM í•„ìš”")
        logger.error("2. ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± - ì•½ 15GB ì—¬ìœ  ê³µê°„ í•„ìš”")
        logger.error("3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ì¸í„°ë„· ì—°ê²° í™•ì¸")
        return None

if __name__ == "__main__":
    logger.info("Qwen3-8B LoRA ë³‘í•© ìŠ¤í¬ë¦½íŠ¸")
    result = merge_lora_model()
    
    if result:
        logger.info(f"âœ… ì„±ê³µ! ë³‘í•©ëœ ëª¨ë¸ ê²½ë¡œ: {result}")
    else:
        logger.error("âŒ ë³‘í•© ì‹¤íŒ¨!")