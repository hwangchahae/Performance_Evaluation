"""
LoRA ë³‘í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë³‘í•© ì‹¤íŒ¨ ë¬¸ì œ ë””ë²„ê¹…ìš©
"""

import os
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging
import gc

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_lora_merge():
    """LoRA ë³‘í•© í…ŒìŠ¤íŠ¸"""
    
    # ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).parent
    base_model_path = "Qwen/Qwen3-8B"
    lora_path = script_dir / "qwen3_lora_ttalkkac_8b"
    
    # LoRA ì–´ëŒ‘í„° íŒŒì¼ í™•ì¸
    logger.info("=" * 60)
    logger.info("LoRA ì–´ëŒ‘í„° íŒŒì¼ í™•ì¸")
    logger.info("=" * 60)
    
    if not lora_path.exists():
        logger.error(f"âŒ LoRA í´ë” ì—†ìŒ: {lora_path}")
        return False
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    required_files = [
        "adapter_config.json",
        "adapter_model.safetensors"
    ]
    
    for file in required_files:
        file_path = lora_path / file
        if file_path.exists():
            size_mb = file_path.stat().st_size / (1024 * 1024)
            logger.info(f"âœ… {file}: {size_mb:.2f} MB")
        else:
            logger.error(f"âŒ {file} ì—†ìŒ")
            return False
    
    # adapter_config.json ë‚´ìš© í™•ì¸
    import json
    with open(lora_path / "adapter_config.json", 'r') as f:
        adapter_config = json.load(f)
    
    logger.info("\nAdapter ì„¤ì •:")
    logger.info(f"  - base_model: {adapter_config.get('base_model_name_or_path', 'N/A')}")
    logger.info(f"  - r (rank): {adapter_config.get('r', 'N/A')}")
    logger.info(f"  - lora_alpha: {adapter_config.get('lora_alpha', 'N/A')}")
    logger.info(f"  - target_modules: {adapter_config.get('target_modules', 'N/A')}")
    
    # ë² ì´ìŠ¤ ëª¨ë¸ì´ ë§ëŠ”ì§€ í™•ì¸
    expected_base = "Qwen/Qwen3-8B"
    actual_base = adapter_config.get('base_model_name_or_path', '')
    
    if expected_base not in actual_base and "Qwen3-8B" not in actual_base:
        logger.warning(f"âš ï¸ ë² ì´ìŠ¤ ëª¨ë¸ ë¶ˆì¼ì¹˜!")
        logger.warning(f"   ì˜ˆìƒ: {expected_base}")
        logger.warning(f"   ì‹¤ì œ: {actual_base}")
    
    logger.info("\n" + "=" * 60)
    logger.info("LoRA ë³‘í•© ì‹œë„")
    logger.info("=" * 60)
    
    try:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        torch.cuda.empty_cache()
        
        logger.info(f"ğŸ“¥ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©: {base_model_path}")
        logger.info("   (CPU ë¡œë”©, float16 ì‚¬ìš©)")
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ - ë©”ëª¨ë¦¬ ìµœì í™”
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="cpu",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        logger.info(f"ğŸ“¥ LoRA ì–´ëŒ‘í„° ë¡œë”©: {lora_path}")
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        model = PeftModel.from_pretrained(
            base_model, 
            str(lora_path),
            device_map="cpu"
        )
        
        logger.info("ğŸ”€ ëª¨ë¸ ë³‘í•© ì¤‘...")
        merged_model = model.merge_and_unload()
        
        logger.info("âœ… ë³‘í•© ì„±ê³µ!")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„±
        logger.info("\ní…ŒìŠ¤íŠ¸ ìƒì„±:")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        
        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ íšŒì˜ ì£¼ì œëŠ”"
        inputs = tokenizer(test_prompt, return_tensors="pt")
        
        # CPUì—ì„œ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸
        with torch.no_grad():
            outputs = merged_model.generate(
                **inputs,
                max_new_tokens=20,
                do_sample=False
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {generated_text}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del base_model
        del model  
        del merged_model
        gc.collect()
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë³‘í•© ì‹¤íŒ¨: {e}")
        logger.error(f"   ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        
        import traceback
        logger.error("ìƒì„¸ ì—ëŸ¬:")
        logger.error(traceback.format_exc())
        
        return False

if __name__ == "__main__":
    logger.info("LoRA ë³‘í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    success = test_lora_merge()
    
    if success:
        logger.info("\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! ë³‘í•© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        logger.error("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨! ë³‘í•©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        logger.info("\nê°€ëŠ¥í•œ í•´ê²° ë°©ë²•:")
        logger.info("1. LoRA ì–´ëŒ‘í„° íŒŒì¼ ì¬ë‹¤ìš´ë¡œë“œ")
        logger.info("2. ë² ì´ìŠ¤ ëª¨ë¸ê³¼ LoRA ë²„ì „ í™•ì¸")
        logger.info("3. ë©”ëª¨ë¦¬ í™•ë³´ (ìµœì†Œ 32GB RAM)")