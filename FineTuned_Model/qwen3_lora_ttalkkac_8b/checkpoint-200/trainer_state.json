{
  "best_global_step": 200,
  "best_metric": 1.038068413734436,
  "best_model_checkpoint": "./qwen3_lora_ttalkkac_20250806_013803/checkpoint-200",
  "epoch": 2.1988950276243093,
  "eval_steps": 50,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011049723756906077,
      "grad_norm": 0.4148130416870117,
      "learning_rate": 0.0,
      "loss": 1.6698,
      "step": 1
    },
    {
      "epoch": 0.022099447513812154,
      "grad_norm": 0.4203541576862335,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6558,
      "step": 2
    },
    {
      "epoch": 0.03314917127071823,
      "grad_norm": 0.40899738669395447,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.6461,
      "step": 3
    },
    {
      "epoch": 0.04419889502762431,
      "grad_norm": 0.455549418926239,
      "learning_rate": 1.2e-05,
      "loss": 1.7655,
      "step": 4
    },
    {
      "epoch": 0.055248618784530384,
      "grad_norm": 0.39366310834884644,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.6308,
      "step": 5
    },
    {
      "epoch": 0.06629834254143646,
      "grad_norm": 0.40988680720329285,
      "learning_rate": 2e-05,
      "loss": 1.6989,
      "step": 6
    },
    {
      "epoch": 0.07734806629834254,
      "grad_norm": 0.3821016848087311,
      "learning_rate": 2.4e-05,
      "loss": 1.6329,
      "step": 7
    },
    {
      "epoch": 0.08839779005524862,
      "grad_norm": 0.40496155619621277,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.7094,
      "step": 8
    },
    {
      "epoch": 0.09944751381215469,
      "grad_norm": 0.3952096402645111,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.7161,
      "step": 9
    },
    {
      "epoch": 0.11049723756906077,
      "grad_norm": 0.3381621241569519,
      "learning_rate": 3.6e-05,
      "loss": 1.6143,
      "step": 10
    },
    {
      "epoch": 0.12154696132596685,
      "grad_norm": 0.3146366775035858,
      "learning_rate": 4e-05,
      "loss": 1.6282,
      "step": 11
    },
    {
      "epoch": 0.13259668508287292,
      "grad_norm": 0.30136290192604065,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.6415,
      "step": 12
    },
    {
      "epoch": 0.143646408839779,
      "grad_norm": 0.2843429744243622,
      "learning_rate": 4.8e-05,
      "loss": 1.6054,
      "step": 13
    },
    {
      "epoch": 0.15469613259668508,
      "grad_norm": 0.26619160175323486,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.6277,
      "step": 14
    },
    {
      "epoch": 0.16574585635359115,
      "grad_norm": 0.23523738980293274,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.5489,
      "step": 15
    },
    {
      "epoch": 0.17679558011049723,
      "grad_norm": 0.23037977516651154,
      "learning_rate": 6e-05,
      "loss": 1.5917,
      "step": 16
    },
    {
      "epoch": 0.1878453038674033,
      "grad_norm": 0.20262280106544495,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.5463,
      "step": 17
    },
    {
      "epoch": 0.19889502762430938,
      "grad_norm": 0.19393104314804077,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.6148,
      "step": 18
    },
    {
      "epoch": 0.20994475138121546,
      "grad_norm": 0.17481891810894012,
      "learning_rate": 7.2e-05,
      "loss": 1.4945,
      "step": 19
    },
    {
      "epoch": 0.22099447513812154,
      "grad_norm": 0.18945299088954926,
      "learning_rate": 7.6e-05,
      "loss": 1.5027,
      "step": 20
    },
    {
      "epoch": 0.23204419889502761,
      "grad_norm": 0.15125980973243713,
      "learning_rate": 8e-05,
      "loss": 1.4745,
      "step": 21
    },
    {
      "epoch": 0.2430939226519337,
      "grad_norm": 0.140167698264122,
      "learning_rate": 8.4e-05,
      "loss": 1.4195,
      "step": 22
    },
    {
      "epoch": 0.2541436464088398,
      "grad_norm": 0.13748431205749512,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.4497,
      "step": 23
    },
    {
      "epoch": 0.26519337016574585,
      "grad_norm": 0.14576151967048645,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.4762,
      "step": 24
    },
    {
      "epoch": 0.27624309392265195,
      "grad_norm": 0.1503181755542755,
      "learning_rate": 9.6e-05,
      "loss": 1.4673,
      "step": 25
    },
    {
      "epoch": 0.287292817679558,
      "grad_norm": 0.15687035024166107,
      "learning_rate": 0.0001,
      "loss": 1.4017,
      "step": 26
    },
    {
      "epoch": 0.2983425414364641,
      "grad_norm": 0.1719256341457367,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.4623,
      "step": 27
    },
    {
      "epoch": 0.30939226519337015,
      "grad_norm": 0.1653798520565033,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.4323,
      "step": 28
    },
    {
      "epoch": 0.32044198895027626,
      "grad_norm": 0.16247913241386414,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.3519,
      "step": 29
    },
    {
      "epoch": 0.3314917127071823,
      "grad_norm": 0.1639018952846527,
      "learning_rate": 0.000116,
      "loss": 1.4569,
      "step": 30
    },
    {
      "epoch": 0.3425414364640884,
      "grad_norm": 0.1493547111749649,
      "learning_rate": 0.00012,
      "loss": 1.4469,
      "step": 31
    },
    {
      "epoch": 0.35359116022099446,
      "grad_norm": 0.1573510318994522,
      "learning_rate": 0.000124,
      "loss": 1.343,
      "step": 32
    },
    {
      "epoch": 0.36464088397790057,
      "grad_norm": 0.1349250078201294,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.3286,
      "step": 33
    },
    {
      "epoch": 0.3756906077348066,
      "grad_norm": 0.13661789894104004,
      "learning_rate": 0.000132,
      "loss": 1.2911,
      "step": 34
    },
    {
      "epoch": 0.3867403314917127,
      "grad_norm": 0.13310497999191284,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.3585,
      "step": 35
    },
    {
      "epoch": 0.39779005524861877,
      "grad_norm": 0.13823269307613373,
      "learning_rate": 0.00014,
      "loss": 1.2525,
      "step": 36
    },
    {
      "epoch": 0.4088397790055249,
      "grad_norm": 0.14860208332538605,
      "learning_rate": 0.000144,
      "loss": 1.2374,
      "step": 37
    },
    {
      "epoch": 0.4198895027624309,
      "grad_norm": 0.1525978296995163,
      "learning_rate": 0.000148,
      "loss": 1.2813,
      "step": 38
    },
    {
      "epoch": 0.430939226519337,
      "grad_norm": 0.16754749417304993,
      "learning_rate": 0.000152,
      "loss": 1.141,
      "step": 39
    },
    {
      "epoch": 0.4419889502762431,
      "grad_norm": 0.15354017913341522,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.3239,
      "step": 40
    },
    {
      "epoch": 0.4530386740331492,
      "grad_norm": 0.1443643420934677,
      "learning_rate": 0.00016,
      "loss": 1.2144,
      "step": 41
    },
    {
      "epoch": 0.46408839779005523,
      "grad_norm": 0.14062541723251343,
      "learning_rate": 0.000164,
      "loss": 1.2704,
      "step": 42
    },
    {
      "epoch": 0.47513812154696133,
      "grad_norm": 0.12639665603637695,
      "learning_rate": 0.000168,
      "loss": 1.2383,
      "step": 43
    },
    {
      "epoch": 0.4861878453038674,
      "grad_norm": 0.11591777205467224,
      "learning_rate": 0.000172,
      "loss": 1.139,
      "step": 44
    },
    {
      "epoch": 0.4972375690607735,
      "grad_norm": 0.12712736427783966,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.234,
      "step": 45
    },
    {
      "epoch": 0.5082872928176796,
      "grad_norm": 0.10963291674852371,
      "learning_rate": 0.00018,
      "loss": 1.1769,
      "step": 46
    },
    {
      "epoch": 0.5193370165745856,
      "grad_norm": 0.11092367768287659,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.158,
      "step": 47
    },
    {
      "epoch": 0.5303867403314917,
      "grad_norm": 0.09470458328723907,
      "learning_rate": 0.000188,
      "loss": 1.1757,
      "step": 48
    },
    {
      "epoch": 0.5414364640883977,
      "grad_norm": 0.0887175127863884,
      "learning_rate": 0.000192,
      "loss": 1.2074,
      "step": 49
    },
    {
      "epoch": 0.5524861878453039,
      "grad_norm": 0.08062684535980225,
      "learning_rate": 0.000196,
      "loss": 1.2311,
      "step": 50
    },
    {
      "epoch": 0.5524861878453039,
      "eval_loss": 1.1284396648406982,
      "eval_runtime": 196.1391,
      "eval_samples_per_second": 1.846,
      "eval_steps_per_second": 1.846,
      "step": 50
    },
    {
      "epoch": 0.56353591160221,
      "grad_norm": 0.07393889874219894,
      "learning_rate": 0.0002,
      "loss": 1.1502,
      "step": 51
    },
    {
      "epoch": 0.574585635359116,
      "grad_norm": 0.07660342752933502,
      "learning_rate": 0.0001991031390134529,
      "loss": 1.1939,
      "step": 52
    },
    {
      "epoch": 0.585635359116022,
      "grad_norm": 0.0708688348531723,
      "learning_rate": 0.00019820627802690584,
      "loss": 1.1113,
      "step": 53
    },
    {
      "epoch": 0.5966850828729282,
      "grad_norm": 0.06722494214773178,
      "learning_rate": 0.00019730941704035874,
      "loss": 1.0764,
      "step": 54
    },
    {
      "epoch": 0.6077348066298343,
      "grad_norm": 0.07138782739639282,
      "learning_rate": 0.00019641255605381167,
      "loss": 1.1625,
      "step": 55
    },
    {
      "epoch": 0.6187845303867403,
      "grad_norm": 0.07009558379650116,
      "learning_rate": 0.0001955156950672646,
      "loss": 1.2055,
      "step": 56
    },
    {
      "epoch": 0.6298342541436464,
      "grad_norm": 0.1019933819770813,
      "learning_rate": 0.0001946188340807175,
      "loss": 1.0534,
      "step": 57
    },
    {
      "epoch": 0.6408839779005525,
      "grad_norm": 0.06781931221485138,
      "learning_rate": 0.00019372197309417043,
      "loss": 1.2299,
      "step": 58
    },
    {
      "epoch": 0.6519337016574586,
      "grad_norm": 0.07869794964790344,
      "learning_rate": 0.00019282511210762333,
      "loss": 1.1894,
      "step": 59
    },
    {
      "epoch": 0.6629834254143646,
      "grad_norm": 0.06765113770961761,
      "learning_rate": 0.00019192825112107625,
      "loss": 1.1951,
      "step": 60
    },
    {
      "epoch": 0.6740331491712708,
      "grad_norm": 0.07119648158550262,
      "learning_rate": 0.00019103139013452916,
      "loss": 1.1086,
      "step": 61
    },
    {
      "epoch": 0.6850828729281768,
      "grad_norm": 0.07569568604230881,
      "learning_rate": 0.00019013452914798206,
      "loss": 1.0888,
      "step": 62
    },
    {
      "epoch": 0.6961325966850829,
      "grad_norm": 0.06660410761833191,
      "learning_rate": 0.00018923766816143498,
      "loss": 1.0086,
      "step": 63
    },
    {
      "epoch": 0.7071823204419889,
      "grad_norm": 0.06454320251941681,
      "learning_rate": 0.0001883408071748879,
      "loss": 1.077,
      "step": 64
    },
    {
      "epoch": 0.7182320441988951,
      "grad_norm": 0.06314978748559952,
      "learning_rate": 0.00018744394618834081,
      "loss": 1.0851,
      "step": 65
    },
    {
      "epoch": 0.7292817679558011,
      "grad_norm": 0.06801185756921768,
      "learning_rate": 0.00018654708520179374,
      "loss": 1.1116,
      "step": 66
    },
    {
      "epoch": 0.7403314917127072,
      "grad_norm": 0.06894047558307648,
      "learning_rate": 0.00018565022421524664,
      "loss": 1.1076,
      "step": 67
    },
    {
      "epoch": 0.7513812154696132,
      "grad_norm": 0.07364659011363983,
      "learning_rate": 0.00018475336322869957,
      "loss": 1.1648,
      "step": 68
    },
    {
      "epoch": 0.7624309392265194,
      "grad_norm": 0.06786784529685974,
      "learning_rate": 0.00018385650224215247,
      "loss": 1.1893,
      "step": 69
    },
    {
      "epoch": 0.7734806629834254,
      "grad_norm": 0.06965681165456772,
      "learning_rate": 0.00018295964125560537,
      "loss": 1.09,
      "step": 70
    },
    {
      "epoch": 0.7845303867403315,
      "grad_norm": 0.07375530898571014,
      "learning_rate": 0.0001820627802690583,
      "loss": 1.2054,
      "step": 71
    },
    {
      "epoch": 0.7955801104972375,
      "grad_norm": 0.07027318328619003,
      "learning_rate": 0.0001811659192825112,
      "loss": 1.1146,
      "step": 72
    },
    {
      "epoch": 0.8066298342541437,
      "grad_norm": 0.06907550245523453,
      "learning_rate": 0.00018026905829596416,
      "loss": 1.0969,
      "step": 73
    },
    {
      "epoch": 0.8176795580110497,
      "grad_norm": 0.06666383892297745,
      "learning_rate": 0.00017937219730941706,
      "loss": 1.1252,
      "step": 74
    },
    {
      "epoch": 0.8287292817679558,
      "grad_norm": 0.07551253587007523,
      "learning_rate": 0.00017847533632286996,
      "loss": 1.1459,
      "step": 75
    },
    {
      "epoch": 0.8397790055248618,
      "grad_norm": 0.07186184078454971,
      "learning_rate": 0.0001775784753363229,
      "loss": 1.1228,
      "step": 76
    },
    {
      "epoch": 0.850828729281768,
      "grad_norm": 0.09143509715795517,
      "learning_rate": 0.0001766816143497758,
      "loss": 1.1211,
      "step": 77
    },
    {
      "epoch": 0.861878453038674,
      "grad_norm": 0.06920807808637619,
      "learning_rate": 0.00017578475336322872,
      "loss": 1.0616,
      "step": 78
    },
    {
      "epoch": 0.8729281767955801,
      "grad_norm": 0.06825578212738037,
      "learning_rate": 0.00017488789237668162,
      "loss": 1.1314,
      "step": 79
    },
    {
      "epoch": 0.8839779005524862,
      "grad_norm": 0.07096888870000839,
      "learning_rate": 0.00017399103139013452,
      "loss": 1.0642,
      "step": 80
    },
    {
      "epoch": 0.8950276243093923,
      "grad_norm": 0.06910529732704163,
      "learning_rate": 0.00017309417040358745,
      "loss": 1.0416,
      "step": 81
    },
    {
      "epoch": 0.9060773480662984,
      "grad_norm": 0.06947244703769684,
      "learning_rate": 0.00017219730941704037,
      "loss": 1.0708,
      "step": 82
    },
    {
      "epoch": 0.9171270718232044,
      "grad_norm": 0.07143930345773697,
      "learning_rate": 0.00017130044843049328,
      "loss": 1.1025,
      "step": 83
    },
    {
      "epoch": 0.9281767955801105,
      "grad_norm": 0.08582806587219238,
      "learning_rate": 0.0001704035874439462,
      "loss": 1.1031,
      "step": 84
    },
    {
      "epoch": 0.9392265193370166,
      "grad_norm": 0.07310018688440323,
      "learning_rate": 0.0001695067264573991,
      "loss": 1.0533,
      "step": 85
    },
    {
      "epoch": 0.9502762430939227,
      "grad_norm": 0.0798049047589302,
      "learning_rate": 0.00016860986547085203,
      "loss": 1.1176,
      "step": 86
    },
    {
      "epoch": 0.9613259668508287,
      "grad_norm": 0.06922098994255066,
      "learning_rate": 0.00016771300448430493,
      "loss": 1.086,
      "step": 87
    },
    {
      "epoch": 0.9723756906077348,
      "grad_norm": 0.07321534305810928,
      "learning_rate": 0.00016681614349775784,
      "loss": 1.1156,
      "step": 88
    },
    {
      "epoch": 0.9834254143646409,
      "grad_norm": 0.07425346970558167,
      "learning_rate": 0.00016591928251121076,
      "loss": 1.0695,
      "step": 89
    },
    {
      "epoch": 0.994475138121547,
      "grad_norm": 0.07674412429332733,
      "learning_rate": 0.0001650224215246637,
      "loss": 1.0782,
      "step": 90
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.10088442265987396,
      "learning_rate": 0.00016412556053811662,
      "loss": 1.1591,
      "step": 91
    },
    {
      "epoch": 1.011049723756906,
      "grad_norm": 0.08395128697156906,
      "learning_rate": 0.00016322869955156952,
      "loss": 1.1554,
      "step": 92
    },
    {
      "epoch": 1.022099447513812,
      "grad_norm": 0.08225950598716736,
      "learning_rate": 0.00016233183856502242,
      "loss": 1.0609,
      "step": 93
    },
    {
      "epoch": 1.0331491712707181,
      "grad_norm": 0.07490037381649017,
      "learning_rate": 0.00016143497757847535,
      "loss": 1.113,
      "step": 94
    },
    {
      "epoch": 1.0441988950276242,
      "grad_norm": 0.07652940601110458,
      "learning_rate": 0.00016053811659192825,
      "loss": 1.1114,
      "step": 95
    },
    {
      "epoch": 1.0552486187845305,
      "grad_norm": 0.08144791424274445,
      "learning_rate": 0.00015964125560538118,
      "loss": 1.1199,
      "step": 96
    },
    {
      "epoch": 1.0662983425414365,
      "grad_norm": 0.08158517628908157,
      "learning_rate": 0.00015874439461883408,
      "loss": 1.1247,
      "step": 97
    },
    {
      "epoch": 1.0773480662983426,
      "grad_norm": 0.0816759392619133,
      "learning_rate": 0.00015784753363228698,
      "loss": 1.1049,
      "step": 98
    },
    {
      "epoch": 1.0883977900552486,
      "grad_norm": 0.07917842268943787,
      "learning_rate": 0.00015695067264573994,
      "loss": 1.0952,
      "step": 99
    },
    {
      "epoch": 1.0994475138121547,
      "grad_norm": 0.08081309497356415,
      "learning_rate": 0.00015605381165919284,
      "loss": 1.0922,
      "step": 100
    },
    {
      "epoch": 1.0994475138121547,
      "eval_loss": 1.0698304176330566,
      "eval_runtime": 196.3485,
      "eval_samples_per_second": 1.844,
      "eval_steps_per_second": 1.844,
      "step": 100
    },
    {
      "epoch": 1.1104972375690607,
      "grad_norm": 0.07791239023208618,
      "learning_rate": 0.00015515695067264574,
      "loss": 1.0506,
      "step": 101
    },
    {
      "epoch": 1.1215469613259668,
      "grad_norm": 0.08433792740106583,
      "learning_rate": 0.00015426008968609867,
      "loss": 1.042,
      "step": 102
    },
    {
      "epoch": 1.132596685082873,
      "grad_norm": 0.08766597509384155,
      "learning_rate": 0.00015336322869955157,
      "loss": 1.111,
      "step": 103
    },
    {
      "epoch": 1.143646408839779,
      "grad_norm": 0.07674898207187653,
      "learning_rate": 0.0001524663677130045,
      "loss": 1.0506,
      "step": 104
    },
    {
      "epoch": 1.1546961325966851,
      "grad_norm": 0.15225644409656525,
      "learning_rate": 0.0001515695067264574,
      "loss": 1.1385,
      "step": 105
    },
    {
      "epoch": 1.1657458563535912,
      "grad_norm": 0.08828436583280563,
      "learning_rate": 0.00015067264573991032,
      "loss": 1.1089,
      "step": 106
    },
    {
      "epoch": 1.1767955801104972,
      "grad_norm": 0.07867904752492905,
      "learning_rate": 0.00014977578475336325,
      "loss": 1.1057,
      "step": 107
    },
    {
      "epoch": 1.1878453038674033,
      "grad_norm": 0.0768054872751236,
      "learning_rate": 0.00014887892376681615,
      "loss": 1.0704,
      "step": 108
    },
    {
      "epoch": 1.1988950276243093,
      "grad_norm": 0.0851023718714714,
      "learning_rate": 0.00014798206278026908,
      "loss": 1.0634,
      "step": 109
    },
    {
      "epoch": 1.2099447513812154,
      "grad_norm": 0.08512851595878601,
      "learning_rate": 0.00014708520179372198,
      "loss": 1.1431,
      "step": 110
    },
    {
      "epoch": 1.2209944751381214,
      "grad_norm": 0.08315075933933258,
      "learning_rate": 0.00014618834080717488,
      "loss": 1.1053,
      "step": 111
    },
    {
      "epoch": 1.2320441988950277,
      "grad_norm": 0.08557603508234024,
      "learning_rate": 0.0001452914798206278,
      "loss": 1.0921,
      "step": 112
    },
    {
      "epoch": 1.2430939226519337,
      "grad_norm": 0.08810526132583618,
      "learning_rate": 0.0001443946188340807,
      "loss": 1.1092,
      "step": 113
    },
    {
      "epoch": 1.2541436464088398,
      "grad_norm": 0.08336842805147171,
      "learning_rate": 0.00014349775784753364,
      "loss": 1.1069,
      "step": 114
    },
    {
      "epoch": 1.2651933701657458,
      "grad_norm": 0.08287833631038666,
      "learning_rate": 0.00014260089686098654,
      "loss": 1.0151,
      "step": 115
    },
    {
      "epoch": 1.276243093922652,
      "grad_norm": 0.08537336438894272,
      "learning_rate": 0.00014170403587443947,
      "loss": 1.1406,
      "step": 116
    },
    {
      "epoch": 1.287292817679558,
      "grad_norm": 0.08540751039981842,
      "learning_rate": 0.0001408071748878924,
      "loss": 1.1055,
      "step": 117
    },
    {
      "epoch": 1.298342541436464,
      "grad_norm": 0.09516724199056625,
      "learning_rate": 0.0001399103139013453,
      "loss": 1.0831,
      "step": 118
    },
    {
      "epoch": 1.3093922651933703,
      "grad_norm": 0.0972689613699913,
      "learning_rate": 0.00013901345291479823,
      "loss": 1.1144,
      "step": 119
    },
    {
      "epoch": 1.3204419889502763,
      "grad_norm": 0.08587133884429932,
      "learning_rate": 0.00013811659192825113,
      "loss": 1.0767,
      "step": 120
    },
    {
      "epoch": 1.3314917127071824,
      "grad_norm": 0.0870128944516182,
      "learning_rate": 0.00013721973094170403,
      "loss": 1.0118,
      "step": 121
    },
    {
      "epoch": 1.3425414364640884,
      "grad_norm": 0.09338728338479996,
      "learning_rate": 0.00013632286995515696,
      "loss": 1.0774,
      "step": 122
    },
    {
      "epoch": 1.3535911602209945,
      "grad_norm": 0.08640685677528381,
      "learning_rate": 0.00013542600896860986,
      "loss": 1.0772,
      "step": 123
    },
    {
      "epoch": 1.3646408839779005,
      "grad_norm": 0.08938223123550415,
      "learning_rate": 0.0001345291479820628,
      "loss": 1.1188,
      "step": 124
    },
    {
      "epoch": 1.3756906077348066,
      "grad_norm": 0.09485076367855072,
      "learning_rate": 0.00013363228699551572,
      "loss": 1.1096,
      "step": 125
    },
    {
      "epoch": 1.3867403314917128,
      "grad_norm": 0.10800132900476456,
      "learning_rate": 0.00013273542600896862,
      "loss": 1.0758,
      "step": 126
    },
    {
      "epoch": 1.3977900552486187,
      "grad_norm": 0.09483454376459122,
      "learning_rate": 0.00013183856502242154,
      "loss": 1.1267,
      "step": 127
    },
    {
      "epoch": 1.408839779005525,
      "grad_norm": 0.09270817041397095,
      "learning_rate": 0.00013094170403587445,
      "loss": 1.0855,
      "step": 128
    },
    {
      "epoch": 1.419889502762431,
      "grad_norm": 0.0930078998208046,
      "learning_rate": 0.00013004484304932735,
      "loss": 1.1109,
      "step": 129
    },
    {
      "epoch": 1.430939226519337,
      "grad_norm": 0.0891241803765297,
      "learning_rate": 0.00012914798206278027,
      "loss": 1.0496,
      "step": 130
    },
    {
      "epoch": 1.441988950276243,
      "grad_norm": 0.09549945592880249,
      "learning_rate": 0.00012825112107623318,
      "loss": 1.1366,
      "step": 131
    },
    {
      "epoch": 1.4530386740331491,
      "grad_norm": 0.08934381604194641,
      "learning_rate": 0.0001273542600896861,
      "loss": 1.0707,
      "step": 132
    },
    {
      "epoch": 1.4640883977900552,
      "grad_norm": 0.09108389168977737,
      "learning_rate": 0.00012645739910313903,
      "loss": 1.144,
      "step": 133
    },
    {
      "epoch": 1.4751381215469612,
      "grad_norm": 0.09229829162359238,
      "learning_rate": 0.00012556053811659193,
      "loss": 1.0957,
      "step": 134
    },
    {
      "epoch": 1.4861878453038675,
      "grad_norm": 0.0975593626499176,
      "learning_rate": 0.00012466367713004486,
      "loss": 1.0443,
      "step": 135
    },
    {
      "epoch": 1.4972375690607735,
      "grad_norm": 0.08882373571395874,
      "learning_rate": 0.00012376681614349776,
      "loss": 1.0658,
      "step": 136
    },
    {
      "epoch": 1.5082872928176796,
      "grad_norm": 0.0853412002325058,
      "learning_rate": 0.0001228699551569507,
      "loss": 0.9431,
      "step": 137
    },
    {
      "epoch": 1.5193370165745856,
      "grad_norm": 0.10084964334964752,
      "learning_rate": 0.00012197309417040359,
      "loss": 1.0573,
      "step": 138
    },
    {
      "epoch": 1.5303867403314917,
      "grad_norm": 0.09576103091239929,
      "learning_rate": 0.0001210762331838565,
      "loss": 1.1113,
      "step": 139
    },
    {
      "epoch": 1.5414364640883977,
      "grad_norm": 0.08776248246431351,
      "learning_rate": 0.00012017937219730942,
      "loss": 1.0342,
      "step": 140
    },
    {
      "epoch": 1.5524861878453038,
      "grad_norm": 0.09618394821882248,
      "learning_rate": 0.00011928251121076232,
      "loss": 1.0544,
      "step": 141
    },
    {
      "epoch": 1.56353591160221,
      "grad_norm": 0.09483534097671509,
      "learning_rate": 0.00011838565022421526,
      "loss": 1.0866,
      "step": 142
    },
    {
      "epoch": 1.5745856353591159,
      "grad_norm": 0.10165651887655258,
      "learning_rate": 0.00011748878923766818,
      "loss": 1.1285,
      "step": 143
    },
    {
      "epoch": 1.5856353591160222,
      "grad_norm": 0.09163140505552292,
      "learning_rate": 0.00011659192825112109,
      "loss": 0.9569,
      "step": 144
    },
    {
      "epoch": 1.5966850828729282,
      "grad_norm": 0.09559987485408783,
      "learning_rate": 0.00011569506726457399,
      "loss": 0.9961,
      "step": 145
    },
    {
      "epoch": 1.6077348066298343,
      "grad_norm": 0.09640882909297943,
      "learning_rate": 0.00011479820627802691,
      "loss": 1.0688,
      "step": 146
    },
    {
      "epoch": 1.6187845303867403,
      "grad_norm": 0.10216327756643295,
      "learning_rate": 0.00011390134529147982,
      "loss": 1.0982,
      "step": 147
    },
    {
      "epoch": 1.6298342541436464,
      "grad_norm": 0.09288632124662399,
      "learning_rate": 0.00011300448430493274,
      "loss": 1.0875,
      "step": 148
    },
    {
      "epoch": 1.6408839779005526,
      "grad_norm": 0.08843878656625748,
      "learning_rate": 0.00011210762331838565,
      "loss": 1.0651,
      "step": 149
    },
    {
      "epoch": 1.6519337016574585,
      "grad_norm": 0.15075917541980743,
      "learning_rate": 0.00011121076233183858,
      "loss": 1.0847,
      "step": 150
    },
    {
      "epoch": 1.6519337016574585,
      "eval_loss": 1.0491883754730225,
      "eval_runtime": 196.0197,
      "eval_samples_per_second": 1.847,
      "eval_steps_per_second": 1.847,
      "step": 150
    },
    {
      "epoch": 1.6629834254143647,
      "grad_norm": 0.09165776520967484,
      "learning_rate": 0.0001103139013452915,
      "loss": 1.0934,
      "step": 151
    },
    {
      "epoch": 1.6740331491712708,
      "grad_norm": 0.0989050567150116,
      "learning_rate": 0.00010941704035874441,
      "loss": 1.1387,
      "step": 152
    },
    {
      "epoch": 1.6850828729281768,
      "grad_norm": 0.08816573023796082,
      "learning_rate": 0.00010852017937219732,
      "loss": 0.9844,
      "step": 153
    },
    {
      "epoch": 1.6961325966850829,
      "grad_norm": 0.09321477264165878,
      "learning_rate": 0.00010762331838565022,
      "loss": 1.0456,
      "step": 154
    },
    {
      "epoch": 1.707182320441989,
      "grad_norm": 0.097511425614357,
      "learning_rate": 0.00010672645739910314,
      "loss": 1.0986,
      "step": 155
    },
    {
      "epoch": 1.7182320441988952,
      "grad_norm": 0.09012976288795471,
      "learning_rate": 0.00010582959641255605,
      "loss": 0.9894,
      "step": 156
    },
    {
      "epoch": 1.729281767955801,
      "grad_norm": 0.10399129986763,
      "learning_rate": 0.00010493273542600897,
      "loss": 1.0919,
      "step": 157
    },
    {
      "epoch": 1.7403314917127073,
      "grad_norm": 0.10565455257892609,
      "learning_rate": 0.00010403587443946188,
      "loss": 1.0748,
      "step": 158
    },
    {
      "epoch": 1.7513812154696131,
      "grad_norm": 0.10637757182121277,
      "learning_rate": 0.00010313901345291481,
      "loss": 1.1125,
      "step": 159
    },
    {
      "epoch": 1.7624309392265194,
      "grad_norm": 0.09694668650627136,
      "learning_rate": 0.00010224215246636773,
      "loss": 1.0679,
      "step": 160
    },
    {
      "epoch": 1.7734806629834254,
      "grad_norm": 0.10034697502851486,
      "learning_rate": 0.00010134529147982064,
      "loss": 0.9748,
      "step": 161
    },
    {
      "epoch": 1.7845303867403315,
      "grad_norm": 0.1008165255188942,
      "learning_rate": 0.00010044843049327355,
      "loss": 1.0902,
      "step": 162
    },
    {
      "epoch": 1.7955801104972375,
      "grad_norm": 0.09568318724632263,
      "learning_rate": 9.955156950672646e-05,
      "loss": 1.0713,
      "step": 163
    },
    {
      "epoch": 1.8066298342541436,
      "grad_norm": 0.09654826670885086,
      "learning_rate": 9.865470852017937e-05,
      "loss": 1.0995,
      "step": 164
    },
    {
      "epoch": 1.8176795580110499,
      "grad_norm": 0.10072966665029526,
      "learning_rate": 9.77578475336323e-05,
      "loss": 1.0853,
      "step": 165
    },
    {
      "epoch": 1.8287292817679557,
      "grad_norm": 0.09797198325395584,
      "learning_rate": 9.686098654708521e-05,
      "loss": 1.0723,
      "step": 166
    },
    {
      "epoch": 1.839779005524862,
      "grad_norm": 0.09926045686006546,
      "learning_rate": 9.596412556053813e-05,
      "loss": 1.0704,
      "step": 167
    },
    {
      "epoch": 1.850828729281768,
      "grad_norm": 0.10456354916095734,
      "learning_rate": 9.506726457399103e-05,
      "loss": 1.1413,
      "step": 168
    },
    {
      "epoch": 1.861878453038674,
      "grad_norm": 0.10297433286905289,
      "learning_rate": 9.417040358744396e-05,
      "loss": 1.045,
      "step": 169
    },
    {
      "epoch": 1.87292817679558,
      "grad_norm": 0.10200558602809906,
      "learning_rate": 9.327354260089687e-05,
      "loss": 1.1155,
      "step": 170
    },
    {
      "epoch": 1.8839779005524862,
      "grad_norm": 0.09466667473316193,
      "learning_rate": 9.237668161434979e-05,
      "loss": 1.0711,
      "step": 171
    },
    {
      "epoch": 1.8950276243093924,
      "grad_norm": 0.1047334298491478,
      "learning_rate": 9.147982062780269e-05,
      "loss": 1.0351,
      "step": 172
    },
    {
      "epoch": 1.9060773480662982,
      "grad_norm": 0.10525660216808319,
      "learning_rate": 9.05829596412556e-05,
      "loss": 1.1139,
      "step": 173
    },
    {
      "epoch": 1.9171270718232045,
      "grad_norm": 0.1008479967713356,
      "learning_rate": 8.968609865470853e-05,
      "loss": 1.0295,
      "step": 174
    },
    {
      "epoch": 1.9281767955801103,
      "grad_norm": 0.09495870023965836,
      "learning_rate": 8.878923766816144e-05,
      "loss": 1.0504,
      "step": 175
    },
    {
      "epoch": 1.9392265193370166,
      "grad_norm": 0.10071958601474762,
      "learning_rate": 8.789237668161436e-05,
      "loss": 1.0669,
      "step": 176
    },
    {
      "epoch": 1.9502762430939227,
      "grad_norm": 0.13279655575752258,
      "learning_rate": 8.699551569506726e-05,
      "loss": 1.0534,
      "step": 177
    },
    {
      "epoch": 1.9613259668508287,
      "grad_norm": 0.25637784600257874,
      "learning_rate": 8.609865470852019e-05,
      "loss": 1.0169,
      "step": 178
    },
    {
      "epoch": 1.9723756906077348,
      "grad_norm": 0.10166747123003006,
      "learning_rate": 8.52017937219731e-05,
      "loss": 1.0477,
      "step": 179
    },
    {
      "epoch": 1.9834254143646408,
      "grad_norm": 0.09481671452522278,
      "learning_rate": 8.430493273542602e-05,
      "loss": 1.0325,
      "step": 180
    },
    {
      "epoch": 1.994475138121547,
      "grad_norm": 0.09973955154418945,
      "learning_rate": 8.340807174887892e-05,
      "loss": 1.0707,
      "step": 181
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.1506955772638321,
      "learning_rate": 8.251121076233185e-05,
      "loss": 0.9991,
      "step": 182
    },
    {
      "epoch": 2.0110497237569063,
      "grad_norm": 0.10544939339160919,
      "learning_rate": 8.161434977578476e-05,
      "loss": 1.0765,
      "step": 183
    },
    {
      "epoch": 2.022099447513812,
      "grad_norm": 0.09145115315914154,
      "learning_rate": 8.071748878923767e-05,
      "loss": 1.0133,
      "step": 184
    },
    {
      "epoch": 2.0331491712707184,
      "grad_norm": 0.10106106102466583,
      "learning_rate": 7.982062780269059e-05,
      "loss": 1.0416,
      "step": 185
    },
    {
      "epoch": 2.044198895027624,
      "grad_norm": 0.1057809516787529,
      "learning_rate": 7.892376681614349e-05,
      "loss": 1.082,
      "step": 186
    },
    {
      "epoch": 2.0552486187845305,
      "grad_norm": 0.10715121030807495,
      "learning_rate": 7.802690582959642e-05,
      "loss": 1.1097,
      "step": 187
    },
    {
      "epoch": 2.0662983425414363,
      "grad_norm": 0.10056540369987488,
      "learning_rate": 7.713004484304933e-05,
      "loss": 1.1265,
      "step": 188
    },
    {
      "epoch": 2.0773480662983426,
      "grad_norm": 0.09860879927873611,
      "learning_rate": 7.623318385650225e-05,
      "loss": 1.0712,
      "step": 189
    },
    {
      "epoch": 2.0883977900552484,
      "grad_norm": 0.09474512934684753,
      "learning_rate": 7.533632286995516e-05,
      "loss": 1.0265,
      "step": 190
    },
    {
      "epoch": 2.0994475138121547,
      "grad_norm": 0.10480877012014389,
      "learning_rate": 7.443946188340808e-05,
      "loss": 1.0689,
      "step": 191
    },
    {
      "epoch": 2.110497237569061,
      "grad_norm": 0.1001637727022171,
      "learning_rate": 7.354260089686099e-05,
      "loss": 1.0532,
      "step": 192
    },
    {
      "epoch": 2.1215469613259668,
      "grad_norm": 0.10083170980215073,
      "learning_rate": 7.26457399103139e-05,
      "loss": 1.0403,
      "step": 193
    },
    {
      "epoch": 2.132596685082873,
      "grad_norm": 0.1033153235912323,
      "learning_rate": 7.174887892376682e-05,
      "loss": 1.0142,
      "step": 194
    },
    {
      "epoch": 2.143646408839779,
      "grad_norm": 0.10589182376861572,
      "learning_rate": 7.085201793721974e-05,
      "loss": 1.088,
      "step": 195
    },
    {
      "epoch": 2.154696132596685,
      "grad_norm": 0.10256977379322052,
      "learning_rate": 6.995515695067265e-05,
      "loss": 1.0862,
      "step": 196
    },
    {
      "epoch": 2.165745856353591,
      "grad_norm": 0.10412579029798508,
      "learning_rate": 6.905829596412556e-05,
      "loss": 1.0838,
      "step": 197
    },
    {
      "epoch": 2.1767955801104972,
      "grad_norm": 0.0978027805685997,
      "learning_rate": 6.816143497757848e-05,
      "loss": 0.9911,
      "step": 198
    },
    {
      "epoch": 2.1878453038674035,
      "grad_norm": 0.12478429079055786,
      "learning_rate": 6.72645739910314e-05,
      "loss": 1.0947,
      "step": 199
    },
    {
      "epoch": 2.1988950276243093,
      "grad_norm": 0.10763724893331528,
      "learning_rate": 6.636771300448431e-05,
      "loss": 1.0135,
      "step": 200
    },
    {
      "epoch": 2.1988950276243093,
      "eval_loss": 1.038068413734436,
      "eval_runtime": 196.1613,
      "eval_samples_per_second": 1.845,
      "eval_steps_per_second": 1.845,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 273,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.25895512291627e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
