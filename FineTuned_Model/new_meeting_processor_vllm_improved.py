# !pip install vllm

import os, json, re
from glob import glob
from tqdm import tqdm
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import threading

# Import prompt generation functions from other modules
from meeting_analysis_prompts import (
    generate_meeting_analysis_user_prompt,
    MEETING_ANALYSIS_SCHEMA
)

#  1. ëª¨ë¸ ì„ íƒ
model_path = "Qwen/Qwen3-4B-AWQ"
print(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# ì „ì—­ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € (í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ì´ˆê¸°í™”)
llm = None
tokenizer = None
sampling_params = None

def initialize_model():
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”"""
    global llm, tokenizer, sampling_params
    
    if llm is None:
        print(f"ğŸ”§ í”„ë¡œì„¸ìŠ¤ {os.getpid()}ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # VLLM ì—”ì§„ ì´ˆê¸°í™”
        llm = LLM(
            model=model_path,
            quantization="awq_marlin" if "AWQ" in model_path else None,
            tensor_parallel_size=1,
            max_model_len=16384,
            gpu_memory_utilization=0.7,  # ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì •
            trust_remote_code=True,
            enforce_eager=False,
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        sampling_params = SamplingParams(
            temperature=0.2,
            max_tokens=2048,
            stop=None,
            skip_special_tokens=True,
        )
        print(f"âœ… í”„ë¡œì„¸ìŠ¤ {os.getpid()} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

def clean_text(text):
    if not text:
        return ""
    
    # íŠ¹ì • íƒœê·¸ë“¤ë§Œ ì œê±°
    text = re.sub(r'\[TGT\]', '', text)
    text = re.sub(r'\[/TGT\]', '', text)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_json_file(file_path):
    """JSON íŒŒì¼ì„ ë¡œë“œ (qwen3_lora_meeting_generator_vllm.py ë°©ì‹)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if not isinstance(data, list):
            print(f"âš ï¸  {file_path}ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return []
            
        # ê° í•­ëª©ì—ì„œ timestamp, speaker, text ì¶”ì¶œ
        processed_data = []
        for item in data:
            if isinstance(item, dict):
                processed_data.append({
                    "timestamp": item.get("timestamp", "Unknown"),
                    "speaker": item.get("speaker", "Unknown"),
                    "text": item.get("text", "")
                })
        
        return processed_data
                
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
        return []

def chunk_text_simple(text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœ ë¬¸ì ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹ (qwen3_lora_meeting_generator_vllm.py ë°©ì‹)"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunk = text[start:]
        else:
            chunk = text[start:end]
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ëŠê¸° ì‹œë„
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            break_point = max(last_period, last_newline)
            
            if break_point > start + chunk_size // 2:
                chunk = text[start:break_point + 1]
                end = break_point + 1
        
        chunks.append(chunk.strip())
        
        if end >= len(text):
            break
            
        start = end - overlap
    
    return chunks

def process_utterances_to_text(utterances, speakers_set=None):
    """ë°œí™” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (qwen3 ë°©ì‹ê³¼ ë™ì¼)"""
    if not utterances:
        return "", []
        
    meeting_lines = []
    speakers = speakers_set if speakers_set else set()
    
    for item in utterances:
        timestamp = item.get('timestamp', 'Unknown')
        speaker = item.get('speaker', 'Unknown')
        text = item.get('text', '')
        speakers.add(speaker)
        meeting_lines.append(f"[{timestamp}] {speaker}: {text}")
    
    full_text = '\n'.join(meeting_lines)
    return full_text, list(speakers)

# generate_chunk_summary í•¨ìˆ˜ëŠ” ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ë¨
# ê°œë³„ ì²­í¬ ì²˜ë¦¬ ëŒ€ì‹  process_single_file_parallelì—ì„œ ë°°ì¹˜ë¡œ ì²˜ë¦¬

def process_single_file_parallel(input_file_path, output_dir, model_used, folder_name, chunk_size=5000, overlap=512):
    """ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ë³„ë¡œ ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹)"""
    from pathlib import Path
    
    # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ
    rel_input_path = os.path.relpath(input_file_path, os.getcwd())
    print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {rel_input_path}")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    if llm is None:
        initialize_model()
    
    # ë°œí™” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    utterances = load_json_file(input_file_path)
    if not utterances:
        print(f"âš ï¸  {input_file_path}ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 1  # (ì„±ê³µ, ì‹¤íŒ¨) íŠœí”Œ ë°˜í™˜
    
    # ë°œí™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    full_text, speakers = process_utterances_to_text(utterances)
    
    if not full_text:
        print(f"âš ï¸  {input_file_path}ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 1
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± (ìƒëŒ€ ê²½ë¡œë¡œ ë³€ê²½)
    metadata = {
        "source_file": f"Raw_Data_val/{folder_name}/05_final_result.json",
        "utterance_count": len(utterances),
        "speakers": speakers,
        "original_length": len(full_text)
    }
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ì²­í‚¹ ê²°ì •
    if len(full_text) > chunk_size:
        print(f"ğŸ“Š ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ ({len(full_text)}ì) - ì²­í‚¹ ì²˜ë¦¬")
        chunks = chunk_text_simple(full_text, chunk_size, overlap)
        print(f"ğŸ“š {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        metadata["chunking_info"] = {
            "is_chunked": True,
            "total_chunks": len(chunks)
        }
    else:
        print(f"ğŸ“„ ì§§ì€ í…ìŠ¤íŠ¸ ({len(full_text)}ì) - ë‹¨ì¼ ì²˜ë¦¬")
        chunks = [full_text]
        metadata["chunking_info"] = {
            "is_chunked": False,
            "total_chunks": 1
        }
    
    success_count = 0
    fail_count = 0
    output_path = Path(output_dir)
    total_chunks = len(chunks)
    
    # ëª¨ë“  ì²­í¬ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë²ˆì— ì¤€ë¹„ (ë°°ì¹˜ ì²˜ë¦¬)
    all_prompts = []
    chunk_infos = []
    summary_accum = ""
    
    for chunk_idx, chunk_text in enumerate(chunks):
        # ì²­í¬ ì •ë³´ ì €ì¥ - folder_nameì´ ì´ë¯¸ result_ë¡œ ì‹œì‘í•¨
        if total_chunks == 1:
            chunk_dir = output_path / folder_name
            chunk_id = folder_name
        else:
            chunk_dir = output_path / f"{folder_name}_chunk_{chunk_idx+1}"
            chunk_id = f"{folder_name}_chunk_{chunk_idx+1}"
        
        chunk_infos.append({
            "chunk_dir": chunk_dir,
            "chunk_id": chunk_id,
            "chunk_idx": chunk_idx,
            "chunk_text": chunk_text,
            "summary_accum": summary_accum
        })
        
        # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        participants_str = ", ".join(speakers) if speakers else "ì•Œ ìˆ˜ ì—†ìŒ"
        meeting_transcript = f"""ì°¸ì—¬ì: {participants_str}

{chunk_text}"""
        
        system_prompt = """ë‹¹ì‹ ì€ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê¸°íšì•ˆì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì‘ë‹µì€ ë°˜ë“œì‹œ ìš”ì²­ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì œê³µí•˜ì„¸ìš”."""
        
        # meeting_analysis_user_prompt ì‚¬ìš©
        if chunk_idx == 0:
            user_prompt = generate_meeting_analysis_user_prompt(meeting_transcript)
        else:
            additional_context = f"ì´ì „ ë¶„ì„ ê²°ê³¼:\n{summary_accum}"
            user_prompt = generate_meeting_analysis_user_prompt(chunk_text, additional_context)
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        all_prompts.append(formatted_prompt)
        summary_accum += f"ì²­í¬ {chunk_idx+1} ì²˜ë¦¬ ì˜ˆì •\n"
    
    # ë°°ì¹˜ë¡œ ëª¨ë“  ì²­í¬ ì²˜ë¦¬
    print(f"ğŸš€ {len(all_prompts)}ê°œ ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    outputs = llm.generate(all_prompts, sampling_params)
    
    # ê²°ê³¼ ì €ì¥
    for idx, (output, chunk_info) in enumerate(zip(outputs, chunk_infos)):
        chunk_dir = chunk_info["chunk_dir"]
        chunk_id = chunk_info["chunk_id"]
        chunk_idx = chunk_info["chunk_idx"]
        
        chunk_dir.mkdir(parents=True, exist_ok=True)
        
        if output and output.outputs:
            result = output.outputs[0].text.strip()
            
            # JSON ì‘ë‹µ ì²˜ë¦¬ - raw_text ì—†ì´ JSONë§Œ ì¶”ì¶œ
            try:
                # <think> íƒœê·¸ê°€ ìˆìœ¼ë©´ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                if "<think>" in result and "{" in result:
                    # <think> íƒœê·¸ ì´í›„ì˜ JSON ë¶€ë¶„ ì°¾ê¸°
                    json_start = result.find("{", result.find("</think>") if "</think>" in result else 0)
                    if json_start != -1:
                        # JSON ë ì°¾ê¸°
                        bracket_count = 0
                        json_end = json_start
                        for i, char in enumerate(result[json_start:], json_start):
                            if char == '{':
                                bracket_count += 1
                            elif char == '}':
                                bracket_count -= 1
                                if bracket_count == 0:
                                    json_end = i + 1
                                    break
                        json_str = result[json_start:json_end]
                    else:
                        json_str = None
                elif "```json" in result:
                    start = result.find("```json") + 7
                    end = result.find("```", start)
                    if end != -1:
                        json_str = result[start:end].strip()
                    else:
                        json_str = result[start:].strip()
                elif result.startswith("{"):
                    json_str = result
                else:
                    json_str = None
                
                if json_str:
                    result_data = json.loads(json_str)
                else:
                    # ê¸°ë³¸ êµ¬ì¡° ìƒì„± (meeting_analysis ìŠ¤í‚¤ë§ˆ)
                    result_data = {
                        "summary": "íšŒì˜ ìš”ì•½ ë‚´ìš©",
                        "topics": ["ì£¼ìš” ì£¼ì œ 1", "ì£¼ìš” ì£¼ì œ 2"],
                        "decisions": ["ê²°ì •ì‚¬í•­ 1", "ê²°ì •ì‚¬í•­ 2"],
                        "action_items": [
                            {"task": "í•  ì¼ 1", "assignee": "ë‹´ë‹¹ì", "deadline": "ê¸°í•œ"},
                            {"task": "í•  ì¼ 2", "assignee": "ë‹´ë‹¹ì", "deadline": "ê¸°í•œ"}
                        ],
                        "key_discussions": ["í•µì‹¬ ë…¼ì˜ 1", "í•µì‹¬ ë…¼ì˜ 2"],
                        "next_steps": ["ë‹¤ìŒ ë‹¨ê³„ 1", "ë‹¤ìŒ ë‹¨ê³„ 2"]
                    }
            except (json.JSONDecodeError, Exception) as e:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ìƒì„± (meeting_analysis ìŠ¤í‚¤ë§ˆ)
                result_data = {
                    "summary": "íšŒì˜ ìš”ì•½ ë‚´ìš©",
                    "topics": ["ì£¼ìš” ì£¼ì œ 1", "ì£¼ìš” ì£¼ì œ 2"],
                    "decisions": ["ê²°ì •ì‚¬í•­ 1", "ê²°ì •ì‚¬í•­ 2"],
                    "action_items": [
                        {"task": "í•  ì¼ 1", "assignee": "ë‹´ë‹¹ì", "deadline": "ê¸°í•œ"},
                        {"task": "í•  ì¼ 2", "assignee": "ë‹´ë‹¹ì", "deadline": "ê¸°í•œ"}
                    ],
                    "key_discussions": ["í•µì‹¬ ë…¼ì˜ 1", "í•µì‹¬ ë…¼ì˜ 2"],
                    "next_steps": ["ë‹¤ìŒ ë‹¨ê³„ 1", "ë‹¤ìŒ ë‹¨ê³„ 2"]
                }
            
            # ê²°ê³¼ ì €ì¥ (ì›ë˜ êµ¬ì¡°ëŒ€ë¡œ ë³µì›)
            chunk_result = {
                "id": chunk_id,
                "source_dir": folder_name,  # folder_nameì´ ì´ë¯¸ result_ë¡œ ì‹œì‘í•¨
                "notion_output": result_data,
                "metadata": {
                    **metadata,
                    "is_chunk": total_chunks > 1,
                    "chunk_index": chunk_idx + 1 if total_chunks > 1 else None,
                    "processing_date": datetime.now().isoformat()
                }
            }
            
            with open(chunk_dir / "result.json", 'w', encoding='utf-8') as f:
                json.dump(chunk_result, f, ensure_ascii=False, indent=2)
            
            success_count += 1
            if total_chunks > 1:
                print(f"âœ… ì²­í¬ {chunk_idx+1}/{total_chunks} ì €ì¥ ì™„ë£Œ: {chunk_dir.name}")
            else:
                print(f"âœ… ì €ì¥ ì™„ë£Œ: {chunk_dir.name}")
        else:
            fail_count += 1
            if total_chunks > 1:
                print(f"âŒ ì²­í¬ {chunk_idx+1}/{total_chunks} ìƒì„± ì‹¤íŒ¨")
            else:
                print(f"âŒ ìƒì„± ì‹¤íŒ¨")
    
    return success_count, fail_count

# save_final_result_as_txt í•¨ìˆ˜ëŠ” ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ (ê° ì²­í¬ë³„ë¡œ ê°œë³„ JSON ì €ì¥)
# qwen3_lora_meeting_generator_vllm.py ë°©ì‹ìœ¼ë¡œ ì €ì¥

def process_file_wrapper(args):
    """ThreadPoolExecutorë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    folder_name, folder_path, json_file, model_used = args
    
    try:
        # ì²­í‚¹ ì„¤ì • (qwen3_lora_meeting_generator_vllm.pyì™€ ë™ì¼í•˜ê²Œ)
        success_count, fail_count = process_single_file_parallel(
            json_file, 
            folder_path, 
            model_used, 
            folder_name,
            chunk_size=5000,  # ë¬¸ì ê¸°ë°˜ ì²­í‚¹ í¬ê¸°
            overlap=512        # ì˜¤ë²„ë© í¬ê¸°
        )
        
        if success_count > 0:
            return (folder_name, True, f"ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count}")
        else:
            return (folder_name, False, f"ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨ (ì´ {fail_count}ê°œ)")
    except Exception as e:
        return (folder_name, False, str(e))

def batch_process_folders_parallel(base_dir, model_used, output_base_dir=None, max_workers=None):
    """ìˆœì°¨ì ìœ¼ë¡œ ì—¬ëŸ¬ í´ë” ì²˜ë¦¬ (vLLM ì•ˆì •ì„± í™•ë³´)
    
    Args:
        base_dir: ì…ë ¥ ë°ì´í„° ë””ë ‰í† ë¦¬
        model_used: ì‚¬ìš© ëª¨ë¸ëª…
        output_base_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ì…ë ¥ ë””ë ‰í† ë¦¬ì™€ ë™ì¼í•œ ìœ„ì¹˜ì— ìƒì„±)
        max_workers: (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, í˜¸í™˜ì„± ìœ ì§€)
    """
    
    if not os.path.exists(base_dir):
        # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ
        rel_base_dir = os.path.relpath(base_dir, os.getcwd())
        print(f"âŒ ê¸°ë³¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {rel_base_dir}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if output_base_dir is None:
        # ì…ë ¥ ë””ë ‰í† ë¦¬ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì— ê²°ê³¼ í´ë” ìƒì„±
        parent_dir = os.path.dirname(base_dir)
        output_base_dir = os.path.join(parent_dir, f"results_{model_used}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_base_dir, exist_ok=True)
    
    # ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ
    rel_output_dir = os.path.relpath(output_base_dir, os.getcwd())
    print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {rel_output_dir}")
    
    # í•˜ìœ„ í´ë”ë“¤ ì°¾ê¸°
    subfolders = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            json_file = os.path.join(item_path, "05_final_result.json")
            if os.path.exists(json_file):
                # ì¶œë ¥ ê²½ë¡œë¥¼ output_base_dirë¡œ ë³€ê²½
                subfolders.append((item, output_base_dir, json_file, model_used))
            else:
                print(f"âš ï¸  {item} í´ë”ì— 05_final_result.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    if not subfolders:
        print(f"âŒ {base_dir}ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ì´ {len(subfolders)}ê°œ í´ë”ë¥¼ ìˆœì°¨ ì²˜ë¦¬í•©ë‹ˆë‹¤:")
    for folder_name, _, _, _ in subfolders:
        print(f"  - {folder_name}")
    
    print(f"ğŸš€ ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”
    initialize_model()
    
    success_count = 0
    failed_folders = []
    
    # ìˆœì°¨ ì²˜ë¦¬ë¡œ ë³€ê²½ (vLLM ì•ˆì •ì„± í™•ë³´)
    total_chunks_processed = 0
    with tqdm(total=len(subfolders), desc="ğŸ“ ì „ì²´ í´ë” ì²˜ë¦¬", unit="folder") as pbar:
        for args in subfolders:
            folder_name = args[0]
            try:
                folder_name, success, message = process_file_wrapper(args)
                if success:
                    success_count += 1
                    # ì„±ê³µ/ì‹¤íŒ¨ ì²­í¬ ìˆ˜ íŒŒì‹±
                    if "ì„±ê³µ:" in message:
                        chunk_count = int(message.split("ì„±ê³µ: ")[1].split(",")[0])
                        total_chunks_processed += chunk_count
                else:
                    failed_folders.append((folder_name, message))
            except Exception as e:
                failed_folders.append((folder_name, str(e)))
            
            pbar.update(1)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µí•œ í´ë”: {success_count}/{len(subfolders)}")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ì´ ì²­í¬ ìˆ˜: {total_chunks_processed}")
    
    if failed_folders:
        print(f"\nâŒ ì‹¤íŒ¨í•œ í´ë”ë“¤:")
        for folder, error in failed_folders:
            print(f"  - {folder}: {error}")

# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ê°„ë‹¨í•œ ëª¨ë¸ëª… ì„¤ì • (í•„ìš”ì‹œ ì§ì ‘ ìˆ˜ì •)
    model_used = "Qwen3_4B"  # ì¶œë ¥ í´ë”ëª…ì— ì‚¬ìš©ë  ì´ë¦„
    
    # ë°°ì¹˜ ì²˜ë¦¬í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬ (ìƒëŒ€ ê²½ë¡œë¡œ ì„¤ì •)
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜: FineTuned_Model/meeting_processor_vllm_improved.py
    # Raw_Data_val ìœ„ì¹˜: Performance_Evaluation/Raw_Data_val
    base_directory = "../Raw_Data_val"
    
    print(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {model_path} ëª¨ë¸ ì‚¬ìš©")
    print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: Meeting Analysis")
    print(f"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {base_directory}")
    
    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    if not os.path.exists(base_directory):
        print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_directory}")
        print(f"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.path.relpath(os.getcwd(), '.')}")
        exit(1)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • - Pre_Training í´ë”ì— ì €ì¥ (í•™ìŠµ ì „ base model ê²°ê³¼)
    output_directory = "../Pre_Training/4B_base_model_results"
    
    # ìˆœì°¨ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    batch_process_folders_parallel(
        base_dir=base_directory, 
        model_used=model_used, 
        output_base_dir=output_directory
    )