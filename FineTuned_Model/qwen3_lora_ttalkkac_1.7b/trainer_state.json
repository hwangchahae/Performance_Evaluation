{
  "best_global_step": 250,
  "best_metric": 1.1783573627471924,
  "best_model_checkpoint": "./qwen3_lora_ttalkkac_20250805_120604/checkpoint-200",
  "epoch": 3.0,
  "eval_steps": 50,
  "global_step": 273,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011049723756906077,
      "grad_norm": 0.8050937652587891,
      "learning_rate": 0.0,
      "loss": 1.9718,
      "step": 1
    },
    {
      "epoch": 0.022099447513812154,
      "grad_norm": 0.783647894859314,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.8803,
      "step": 2
    },
    {
      "epoch": 0.03314917127071823,
      "grad_norm": 0.8382420539855957,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.0991,
      "step": 3
    },
    {
      "epoch": 0.04419889502762431,
      "grad_norm": 0.776678740978241,
      "learning_rate": 1.2e-05,
      "loss": 1.9698,
      "step": 4
    },
    {
      "epoch": 0.055248618784530384,
      "grad_norm": 0.8187858462333679,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.9805,
      "step": 5
    },
    {
      "epoch": 0.06629834254143646,
      "grad_norm": 0.7789337038993835,
      "learning_rate": 2e-05,
      "loss": 1.9071,
      "step": 6
    },
    {
      "epoch": 0.07734806629834254,
      "grad_norm": 0.6915760040283203,
      "learning_rate": 2.4e-05,
      "loss": 1.9104,
      "step": 7
    },
    {
      "epoch": 0.08839779005524862,
      "grad_norm": 0.6730450987815857,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.9616,
      "step": 8
    },
    {
      "epoch": 0.09944751381215469,
      "grad_norm": 0.6336618661880493,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.9094,
      "step": 9
    },
    {
      "epoch": 0.11049723756906077,
      "grad_norm": 0.6026194095611572,
      "learning_rate": 3.6e-05,
      "loss": 1.9129,
      "step": 10
    },
    {
      "epoch": 0.12154696132596685,
      "grad_norm": 0.496216356754303,
      "learning_rate": 4e-05,
      "loss": 1.7115,
      "step": 11
    },
    {
      "epoch": 0.13259668508287292,
      "grad_norm": 0.49307385087013245,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.9249,
      "step": 12
    },
    {
      "epoch": 0.143646408839779,
      "grad_norm": 0.4522749185562134,
      "learning_rate": 4.8e-05,
      "loss": 1.9028,
      "step": 13
    },
    {
      "epoch": 0.15469613259668508,
      "grad_norm": 0.4046139419078827,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.7754,
      "step": 14
    },
    {
      "epoch": 0.16574585635359115,
      "grad_norm": 0.35902127623558044,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.7391,
      "step": 15
    },
    {
      "epoch": 0.17679558011049723,
      "grad_norm": 0.34844568371772766,
      "learning_rate": 6e-05,
      "loss": 1.9459,
      "step": 16
    },
    {
      "epoch": 0.1878453038674033,
      "grad_norm": 0.27632206678390503,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.7897,
      "step": 17
    },
    {
      "epoch": 0.19889502762430938,
      "grad_norm": 0.2601373493671417,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.8725,
      "step": 18
    },
    {
      "epoch": 0.20994475138121546,
      "grad_norm": 0.22244881093502045,
      "learning_rate": 7.2e-05,
      "loss": 1.7536,
      "step": 19
    },
    {
      "epoch": 0.22099447513812154,
      "grad_norm": 0.21765777468681335,
      "learning_rate": 7.6e-05,
      "loss": 1.7538,
      "step": 20
    },
    {
      "epoch": 0.23204419889502761,
      "grad_norm": 0.21069486439228058,
      "learning_rate": 8e-05,
      "loss": 1.7913,
      "step": 21
    },
    {
      "epoch": 0.2430939226519337,
      "grad_norm": 0.2211931049823761,
      "learning_rate": 8.4e-05,
      "loss": 1.7766,
      "step": 22
    },
    {
      "epoch": 0.2541436464088398,
      "grad_norm": 0.2166484296321869,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.7039,
      "step": 23
    },
    {
      "epoch": 0.26519337016574585,
      "grad_norm": 0.23559679090976715,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.6774,
      "step": 24
    },
    {
      "epoch": 0.27624309392265195,
      "grad_norm": 0.2678490877151489,
      "learning_rate": 9.6e-05,
      "loss": 1.7675,
      "step": 25
    },
    {
      "epoch": 0.287292817679558,
      "grad_norm": 0.23928357660770416,
      "learning_rate": 0.0001,
      "loss": 1.5143,
      "step": 26
    },
    {
      "epoch": 0.2983425414364641,
      "grad_norm": 0.2450963258743286,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.5202,
      "step": 27
    },
    {
      "epoch": 0.30939226519337015,
      "grad_norm": 0.24242660403251648,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.6747,
      "step": 28
    },
    {
      "epoch": 0.32044198895027626,
      "grad_norm": 0.2323060780763626,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.6068,
      "step": 29
    },
    {
      "epoch": 0.3314917127071823,
      "grad_norm": 0.22171112895011902,
      "learning_rate": 0.000116,
      "loss": 1.603,
      "step": 30
    },
    {
      "epoch": 0.3425414364640884,
      "grad_norm": 0.20723044872283936,
      "learning_rate": 0.00012,
      "loss": 1.6364,
      "step": 31
    },
    {
      "epoch": 0.35359116022099446,
      "grad_norm": 0.2039581686258316,
      "learning_rate": 0.000124,
      "loss": 1.6008,
      "step": 32
    },
    {
      "epoch": 0.36464088397790057,
      "grad_norm": 0.19323371350765228,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.4464,
      "step": 33
    },
    {
      "epoch": 0.3756906077348066,
      "grad_norm": 0.19653932750225067,
      "learning_rate": 0.000132,
      "loss": 1.6088,
      "step": 34
    },
    {
      "epoch": 0.3867403314917127,
      "grad_norm": 0.1978292018175125,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.4755,
      "step": 35
    },
    {
      "epoch": 0.39779005524861877,
      "grad_norm": 0.20726706087589264,
      "learning_rate": 0.00014,
      "loss": 1.3914,
      "step": 36
    },
    {
      "epoch": 0.4088397790055249,
      "grad_norm": 0.22427748143672943,
      "learning_rate": 0.000144,
      "loss": 1.5534,
      "step": 37
    },
    {
      "epoch": 0.4198895027624309,
      "grad_norm": 0.2247142493724823,
      "learning_rate": 0.000148,
      "loss": 1.551,
      "step": 38
    },
    {
      "epoch": 0.430939226519337,
      "grad_norm": 0.21974141895771027,
      "learning_rate": 0.000152,
      "loss": 1.4617,
      "step": 39
    },
    {
      "epoch": 0.4419889502762431,
      "grad_norm": 0.21829019486904144,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.4412,
      "step": 40
    },
    {
      "epoch": 0.4530386740331492,
      "grad_norm": 0.21810531616210938,
      "learning_rate": 0.00016,
      "loss": 1.409,
      "step": 41
    },
    {
      "epoch": 0.46408839779005523,
      "grad_norm": 0.22306124866008759,
      "learning_rate": 0.000164,
      "loss": 1.391,
      "step": 42
    },
    {
      "epoch": 0.47513812154696133,
      "grad_norm": 0.21743208169937134,
      "learning_rate": 0.000168,
      "loss": 1.45,
      "step": 43
    },
    {
      "epoch": 0.4861878453038674,
      "grad_norm": 0.20741774141788483,
      "learning_rate": 0.000172,
      "loss": 1.3408,
      "step": 44
    },
    {
      "epoch": 0.4972375690607735,
      "grad_norm": 0.19461360573768616,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.3175,
      "step": 45
    },
    {
      "epoch": 0.5082872928176796,
      "grad_norm": 0.19892342388629913,
      "learning_rate": 0.00018,
      "loss": 1.3643,
      "step": 46
    },
    {
      "epoch": 0.5193370165745856,
      "grad_norm": 0.19648012518882751,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.4091,
      "step": 47
    },
    {
      "epoch": 0.5303867403314917,
      "grad_norm": 0.1615721583366394,
      "learning_rate": 0.000188,
      "loss": 1.3592,
      "step": 48
    },
    {
      "epoch": 0.5414364640883977,
      "grad_norm": 0.14619290828704834,
      "learning_rate": 0.000192,
      "loss": 1.3662,
      "step": 49
    },
    {
      "epoch": 0.5524861878453039,
      "grad_norm": 0.13449198007583618,
      "learning_rate": 0.000196,
      "loss": 1.218,
      "step": 50
    },
    {
      "epoch": 0.5524861878453039,
      "eval_loss": 1.3047354221343994,
      "eval_runtime": 112.2311,
      "eval_samples_per_second": 3.225,
      "eval_steps_per_second": 3.225,
      "step": 50
    },
    {
      "epoch": 0.56353591160221,
      "grad_norm": 0.1270005702972412,
      "learning_rate": 0.0002,
      "loss": 1.3889,
      "step": 51
    },
    {
      "epoch": 0.574585635359116,
      "grad_norm": 0.1246139258146286,
      "learning_rate": 0.0001991031390134529,
      "loss": 1.1944,
      "step": 52
    },
    {
      "epoch": 0.585635359116022,
      "grad_norm": 0.11650431901216507,
      "learning_rate": 0.00019820627802690584,
      "loss": 1.1642,
      "step": 53
    },
    {
      "epoch": 0.5966850828729282,
      "grad_norm": 0.11760511994361877,
      "learning_rate": 0.00019730941704035874,
      "loss": 1.2545,
      "step": 54
    },
    {
      "epoch": 0.6077348066298343,
      "grad_norm": 0.11702489852905273,
      "learning_rate": 0.00019641255605381167,
      "loss": 1.3781,
      "step": 55
    },
    {
      "epoch": 0.6187845303867403,
      "grad_norm": 0.09725991636514664,
      "learning_rate": 0.0001955156950672646,
      "loss": 1.1918,
      "step": 56
    },
    {
      "epoch": 0.6298342541436464,
      "grad_norm": 0.1004641205072403,
      "learning_rate": 0.0001946188340807175,
      "loss": 1.2774,
      "step": 57
    },
    {
      "epoch": 0.6408839779005525,
      "grad_norm": 0.09897790104150772,
      "learning_rate": 0.00019372197309417043,
      "loss": 1.2923,
      "step": 58
    },
    {
      "epoch": 0.6519337016574586,
      "grad_norm": 0.09456776082515717,
      "learning_rate": 0.00019282511210762333,
      "loss": 1.2228,
      "step": 59
    },
    {
      "epoch": 0.6629834254143646,
      "grad_norm": 0.09857626259326935,
      "learning_rate": 0.00019192825112107625,
      "loss": 1.1643,
      "step": 60
    },
    {
      "epoch": 0.6740331491712708,
      "grad_norm": 0.11049732565879822,
      "learning_rate": 0.00019103139013452916,
      "loss": 1.3649,
      "step": 61
    },
    {
      "epoch": 0.6850828729281768,
      "grad_norm": 0.09953095763921738,
      "learning_rate": 0.00019013452914798206,
      "loss": 1.279,
      "step": 62
    },
    {
      "epoch": 0.6961325966850829,
      "grad_norm": 0.09779244661331177,
      "learning_rate": 0.00018923766816143498,
      "loss": 1.2288,
      "step": 63
    },
    {
      "epoch": 0.7071823204419889,
      "grad_norm": 0.10877886414527893,
      "learning_rate": 0.0001883408071748879,
      "loss": 1.2953,
      "step": 64
    },
    {
      "epoch": 0.7182320441988951,
      "grad_norm": 0.10838335752487183,
      "learning_rate": 0.00018744394618834081,
      "loss": 1.3134,
      "step": 65
    },
    {
      "epoch": 0.7292817679558011,
      "grad_norm": 0.09939820319414139,
      "learning_rate": 0.00018654708520179374,
      "loss": 1.2029,
      "step": 66
    },
    {
      "epoch": 0.7403314917127072,
      "grad_norm": 0.10330358147621155,
      "learning_rate": 0.00018565022421524664,
      "loss": 1.2685,
      "step": 67
    },
    {
      "epoch": 0.7513812154696132,
      "grad_norm": 0.11131030321121216,
      "learning_rate": 0.00018475336322869957,
      "loss": 1.3789,
      "step": 68
    },
    {
      "epoch": 0.7624309392265194,
      "grad_norm": 0.10220775008201599,
      "learning_rate": 0.00018385650224215247,
      "loss": 1.3269,
      "step": 69
    },
    {
      "epoch": 0.7734806629834254,
      "grad_norm": 0.10034164786338806,
      "learning_rate": 0.00018295964125560537,
      "loss": 1.2309,
      "step": 70
    },
    {
      "epoch": 0.7845303867403315,
      "grad_norm": 0.10462196171283722,
      "learning_rate": 0.0001820627802690583,
      "loss": 1.3047,
      "step": 71
    },
    {
      "epoch": 0.7955801104972375,
      "grad_norm": 0.09928270429372787,
      "learning_rate": 0.0001811659192825112,
      "loss": 1.2559,
      "step": 72
    },
    {
      "epoch": 0.8066298342541437,
      "grad_norm": 0.10110846906900406,
      "learning_rate": 0.00018026905829596416,
      "loss": 1.269,
      "step": 73
    },
    {
      "epoch": 0.8176795580110497,
      "grad_norm": 0.10582263022661209,
      "learning_rate": 0.00017937219730941706,
      "loss": 1.317,
      "step": 74
    },
    {
      "epoch": 0.8287292817679558,
      "grad_norm": 0.10539516806602478,
      "learning_rate": 0.00017847533632286996,
      "loss": 1.276,
      "step": 75
    },
    {
      "epoch": 0.8397790055248618,
      "grad_norm": 0.09743709862232208,
      "learning_rate": 0.0001775784753363229,
      "loss": 1.1933,
      "step": 76
    },
    {
      "epoch": 0.850828729281768,
      "grad_norm": 0.10424637794494629,
      "learning_rate": 0.0001766816143497758,
      "loss": 1.2847,
      "step": 77
    },
    {
      "epoch": 0.861878453038674,
      "grad_norm": 0.09660421311855316,
      "learning_rate": 0.00017578475336322872,
      "loss": 1.1611,
      "step": 78
    },
    {
      "epoch": 0.8729281767955801,
      "grad_norm": 0.10630720853805542,
      "learning_rate": 0.00017488789237668162,
      "loss": 1.2875,
      "step": 79
    },
    {
      "epoch": 0.8839779005524862,
      "grad_norm": 0.11577364802360535,
      "learning_rate": 0.00017399103139013452,
      "loss": 1.3144,
      "step": 80
    },
    {
      "epoch": 0.8950276243093923,
      "grad_norm": 0.10460484027862549,
      "learning_rate": 0.00017309417040358745,
      "loss": 1.2684,
      "step": 81
    },
    {
      "epoch": 0.9060773480662984,
      "grad_norm": 0.10320644825696945,
      "learning_rate": 0.00017219730941704037,
      "loss": 1.339,
      "step": 82
    },
    {
      "epoch": 0.9171270718232044,
      "grad_norm": 0.10102873295545578,
      "learning_rate": 0.00017130044843049328,
      "loss": 1.1617,
      "step": 83
    },
    {
      "epoch": 0.9281767955801105,
      "grad_norm": 0.10631421208381653,
      "learning_rate": 0.0001704035874439462,
      "loss": 1.2673,
      "step": 84
    },
    {
      "epoch": 0.9392265193370166,
      "grad_norm": 0.10983077436685562,
      "learning_rate": 0.0001695067264573991,
      "loss": 1.3482,
      "step": 85
    },
    {
      "epoch": 0.9502762430939227,
      "grad_norm": 0.09733729809522629,
      "learning_rate": 0.00016860986547085203,
      "loss": 1.2261,
      "step": 86
    },
    {
      "epoch": 0.9613259668508287,
      "grad_norm": 0.10589679330587387,
      "learning_rate": 0.00016771300448430493,
      "loss": 1.2425,
      "step": 87
    },
    {
      "epoch": 0.9723756906077348,
      "grad_norm": 0.10965892672538757,
      "learning_rate": 0.00016681614349775784,
      "loss": 1.3343,
      "step": 88
    },
    {
      "epoch": 0.9834254143646409,
      "grad_norm": 0.10756739974021912,
      "learning_rate": 0.00016591928251121076,
      "loss": 1.2819,
      "step": 89
    },
    {
      "epoch": 0.994475138121547,
      "grad_norm": 0.1083083227276802,
      "learning_rate": 0.0001650224215246637,
      "loss": 1.2418,
      "step": 90
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.14190952479839325,
      "learning_rate": 0.00016412556053811662,
      "loss": 1.2985,
      "step": 91
    },
    {
      "epoch": 1.011049723756906,
      "grad_norm": 0.11823996901512146,
      "learning_rate": 0.00016322869955156952,
      "loss": 1.2649,
      "step": 92
    },
    {
      "epoch": 1.022099447513812,
      "grad_norm": 0.10947780311107635,
      "learning_rate": 0.00016233183856502242,
      "loss": 1.3003,
      "step": 93
    },
    {
      "epoch": 1.0331491712707181,
      "grad_norm": 0.10913810133934021,
      "learning_rate": 0.00016143497757847535,
      "loss": 1.226,
      "step": 94
    },
    {
      "epoch": 1.0441988950276242,
      "grad_norm": 0.10920708626508713,
      "learning_rate": 0.00016053811659192825,
      "loss": 1.3076,
      "step": 95
    },
    {
      "epoch": 1.0552486187845305,
      "grad_norm": 0.12162703275680542,
      "learning_rate": 0.00015964125560538118,
      "loss": 1.2493,
      "step": 96
    },
    {
      "epoch": 1.0662983425414365,
      "grad_norm": 0.11356040835380554,
      "learning_rate": 0.00015874439461883408,
      "loss": 1.2267,
      "step": 97
    },
    {
      "epoch": 1.0773480662983426,
      "grad_norm": 0.10394603759050369,
      "learning_rate": 0.00015784753363228698,
      "loss": 1.2516,
      "step": 98
    },
    {
      "epoch": 1.0883977900552486,
      "grad_norm": 0.1077999621629715,
      "learning_rate": 0.00015695067264573994,
      "loss": 1.1723,
      "step": 99
    },
    {
      "epoch": 1.0994475138121547,
      "grad_norm": 0.1133560985326767,
      "learning_rate": 0.00015605381165919284,
      "loss": 1.2949,
      "step": 100
    },
    {
      "epoch": 1.0994475138121547,
      "eval_loss": 1.2268726825714111,
      "eval_runtime": 112.1758,
      "eval_samples_per_second": 3.227,
      "eval_steps_per_second": 3.227,
      "step": 100
    },
    {
      "epoch": 1.1104972375690607,
      "grad_norm": 0.1102006658911705,
      "learning_rate": 0.00015515695067264574,
      "loss": 1.2293,
      "step": 101
    },
    {
      "epoch": 1.1215469613259668,
      "grad_norm": 0.11989273130893707,
      "learning_rate": 0.00015426008968609867,
      "loss": 1.2677,
      "step": 102
    },
    {
      "epoch": 1.132596685082873,
      "grad_norm": 0.12126465141773224,
      "learning_rate": 0.00015336322869955157,
      "loss": 1.3643,
      "step": 103
    },
    {
      "epoch": 1.143646408839779,
      "grad_norm": 0.12412115931510925,
      "learning_rate": 0.0001524663677130045,
      "loss": 1.2982,
      "step": 104
    },
    {
      "epoch": 1.1546961325966851,
      "grad_norm": 0.115298792719841,
      "learning_rate": 0.0001515695067264574,
      "loss": 1.225,
      "step": 105
    },
    {
      "epoch": 1.1657458563535912,
      "grad_norm": 0.1200406551361084,
      "learning_rate": 0.00015067264573991032,
      "loss": 1.1745,
      "step": 106
    },
    {
      "epoch": 1.1767955801104972,
      "grad_norm": 0.11191872507333755,
      "learning_rate": 0.00014977578475336325,
      "loss": 1.1573,
      "step": 107
    },
    {
      "epoch": 1.1878453038674033,
      "grad_norm": 0.12323972582817078,
      "learning_rate": 0.00014887892376681615,
      "loss": 1.1858,
      "step": 108
    },
    {
      "epoch": 1.1988950276243093,
      "grad_norm": 0.1215554028749466,
      "learning_rate": 0.00014798206278026908,
      "loss": 1.2467,
      "step": 109
    },
    {
      "epoch": 1.2099447513812154,
      "grad_norm": 0.12175793200731277,
      "learning_rate": 0.00014708520179372198,
      "loss": 1.2411,
      "step": 110
    },
    {
      "epoch": 1.2209944751381214,
      "grad_norm": 0.11950774490833282,
      "learning_rate": 0.00014618834080717488,
      "loss": 1.1991,
      "step": 111
    },
    {
      "epoch": 1.2320441988950277,
      "grad_norm": 0.1221027821302414,
      "learning_rate": 0.0001452914798206278,
      "loss": 1.2685,
      "step": 112
    },
    {
      "epoch": 1.2430939226519337,
      "grad_norm": 0.13171248137950897,
      "learning_rate": 0.0001443946188340807,
      "loss": 1.1732,
      "step": 113
    },
    {
      "epoch": 1.2541436464088398,
      "grad_norm": 0.11520886421203613,
      "learning_rate": 0.00014349775784753364,
      "loss": 1.1966,
      "step": 114
    },
    {
      "epoch": 1.2651933701657458,
      "grad_norm": 0.11441383510828018,
      "learning_rate": 0.00014260089686098654,
      "loss": 1.2134,
      "step": 115
    },
    {
      "epoch": 1.276243093922652,
      "grad_norm": 0.12434987723827362,
      "learning_rate": 0.00014170403587443947,
      "loss": 1.3307,
      "step": 116
    },
    {
      "epoch": 1.287292817679558,
      "grad_norm": 0.1191801130771637,
      "learning_rate": 0.0001408071748878924,
      "loss": 1.2141,
      "step": 117
    },
    {
      "epoch": 1.298342541436464,
      "grad_norm": 0.1270572394132614,
      "learning_rate": 0.0001399103139013453,
      "loss": 1.2872,
      "step": 118
    },
    {
      "epoch": 1.3093922651933703,
      "grad_norm": 0.13114748895168304,
      "learning_rate": 0.00013901345291479823,
      "loss": 1.2731,
      "step": 119
    },
    {
      "epoch": 1.3204419889502763,
      "grad_norm": 0.12247284501791,
      "learning_rate": 0.00013811659192825113,
      "loss": 1.2591,
      "step": 120
    },
    {
      "epoch": 1.3314917127071824,
      "grad_norm": 0.12473781406879425,
      "learning_rate": 0.00013721973094170403,
      "loss": 1.2078,
      "step": 121
    },
    {
      "epoch": 1.3425414364640884,
      "grad_norm": 0.12321807444095612,
      "learning_rate": 0.00013632286995515696,
      "loss": 1.2282,
      "step": 122
    },
    {
      "epoch": 1.3535911602209945,
      "grad_norm": 0.11101489514112473,
      "learning_rate": 0.00013542600896860986,
      "loss": 1.1252,
      "step": 123
    },
    {
      "epoch": 1.3646408839779005,
      "grad_norm": 0.1358434110879898,
      "learning_rate": 0.0001345291479820628,
      "loss": 1.3191,
      "step": 124
    },
    {
      "epoch": 1.3756906077348066,
      "grad_norm": 0.12130419909954071,
      "learning_rate": 0.00013363228699551572,
      "loss": 1.1423,
      "step": 125
    },
    {
      "epoch": 1.3867403314917128,
      "grad_norm": 0.11979714035987854,
      "learning_rate": 0.00013273542600896862,
      "loss": 1.1856,
      "step": 126
    },
    {
      "epoch": 1.3977900552486187,
      "grad_norm": 0.1226557195186615,
      "learning_rate": 0.00013183856502242154,
      "loss": 1.2061,
      "step": 127
    },
    {
      "epoch": 1.408839779005525,
      "grad_norm": 0.12149670720100403,
      "learning_rate": 0.00013094170403587445,
      "loss": 1.2172,
      "step": 128
    },
    {
      "epoch": 1.419889502762431,
      "grad_norm": 0.13425791263580322,
      "learning_rate": 0.00013004484304932735,
      "loss": 1.2691,
      "step": 129
    },
    {
      "epoch": 1.430939226519337,
      "grad_norm": 0.1334221065044403,
      "learning_rate": 0.00012914798206278027,
      "loss": 1.256,
      "step": 130
    },
    {
      "epoch": 1.441988950276243,
      "grad_norm": 0.12522320449352264,
      "learning_rate": 0.00012825112107623318,
      "loss": 1.2372,
      "step": 131
    },
    {
      "epoch": 1.4530386740331491,
      "grad_norm": 0.12698671221733093,
      "learning_rate": 0.0001273542600896861,
      "loss": 1.164,
      "step": 132
    },
    {
      "epoch": 1.4640883977900552,
      "grad_norm": 0.1313173621892929,
      "learning_rate": 0.00012645739910313903,
      "loss": 1.2568,
      "step": 133
    },
    {
      "epoch": 1.4751381215469612,
      "grad_norm": 0.12395651638507843,
      "learning_rate": 0.00012556053811659193,
      "loss": 1.1836,
      "step": 134
    },
    {
      "epoch": 1.4861878453038675,
      "grad_norm": 0.13659042119979858,
      "learning_rate": 0.00012466367713004486,
      "loss": 1.2625,
      "step": 135
    },
    {
      "epoch": 1.4972375690607735,
      "grad_norm": 0.13241873681545258,
      "learning_rate": 0.00012376681614349776,
      "loss": 1.2672,
      "step": 136
    },
    {
      "epoch": 1.5082872928176796,
      "grad_norm": 0.1183437705039978,
      "learning_rate": 0.0001228699551569507,
      "loss": 1.1512,
      "step": 137
    },
    {
      "epoch": 1.5193370165745856,
      "grad_norm": 0.13731707632541656,
      "learning_rate": 0.00012197309417040359,
      "loss": 1.2998,
      "step": 138
    },
    {
      "epoch": 1.5303867403314917,
      "grad_norm": 0.1281031370162964,
      "learning_rate": 0.0001210762331838565,
      "loss": 1.219,
      "step": 139
    },
    {
      "epoch": 1.5414364640883977,
      "grad_norm": 0.12720882892608643,
      "learning_rate": 0.00012017937219730942,
      "loss": 1.2121,
      "step": 140
    },
    {
      "epoch": 1.5524861878453038,
      "grad_norm": 0.1271226704120636,
      "learning_rate": 0.00011928251121076232,
      "loss": 1.2289,
      "step": 141
    },
    {
      "epoch": 1.56353591160221,
      "grad_norm": 0.12298037856817245,
      "learning_rate": 0.00011838565022421526,
      "loss": 1.2162,
      "step": 142
    },
    {
      "epoch": 1.5745856353591159,
      "grad_norm": 0.1289425492286682,
      "learning_rate": 0.00011748878923766818,
      "loss": 1.1981,
      "step": 143
    },
    {
      "epoch": 1.5856353591160222,
      "grad_norm": 0.12834690511226654,
      "learning_rate": 0.00011659192825112109,
      "loss": 1.2503,
      "step": 144
    },
    {
      "epoch": 1.5966850828729282,
      "grad_norm": 0.12969249486923218,
      "learning_rate": 0.00011569506726457399,
      "loss": 1.1355,
      "step": 145
    },
    {
      "epoch": 1.6077348066298343,
      "grad_norm": 0.1314995139837265,
      "learning_rate": 0.00011479820627802691,
      "loss": 1.1983,
      "step": 146
    },
    {
      "epoch": 1.6187845303867403,
      "grad_norm": 0.13541287183761597,
      "learning_rate": 0.00011390134529147982,
      "loss": 1.2097,
      "step": 147
    },
    {
      "epoch": 1.6298342541436464,
      "grad_norm": 0.13143204152584076,
      "learning_rate": 0.00011300448430493274,
      "loss": 1.2267,
      "step": 148
    },
    {
      "epoch": 1.6408839779005526,
      "grad_norm": 0.12427765876054764,
      "learning_rate": 0.00011210762331838565,
      "loss": 1.1931,
      "step": 149
    },
    {
      "epoch": 1.6519337016574585,
      "grad_norm": 0.12576225399971008,
      "learning_rate": 0.00011121076233183858,
      "loss": 1.1961,
      "step": 150
    },
    {
      "epoch": 1.6519337016574585,
      "eval_loss": 1.199355125427246,
      "eval_runtime": 112.0828,
      "eval_samples_per_second": 3.23,
      "eval_steps_per_second": 3.23,
      "step": 150
    },
    {
      "epoch": 1.6629834254143647,
      "grad_norm": 0.1321815848350525,
      "learning_rate": 0.0001103139013452915,
      "loss": 1.196,
      "step": 151
    },
    {
      "epoch": 1.6740331491712708,
      "grad_norm": 0.12010850757360458,
      "learning_rate": 0.00010941704035874441,
      "loss": 1.1733,
      "step": 152
    },
    {
      "epoch": 1.6850828729281768,
      "grad_norm": 0.12310425192117691,
      "learning_rate": 0.00010852017937219732,
      "loss": 1.1932,
      "step": 153
    },
    {
      "epoch": 1.6961325966850829,
      "grad_norm": 0.12821631133556366,
      "learning_rate": 0.00010762331838565022,
      "loss": 1.2078,
      "step": 154
    },
    {
      "epoch": 1.707182320441989,
      "grad_norm": 0.1332894265651703,
      "learning_rate": 0.00010672645739910314,
      "loss": 1.2034,
      "step": 155
    },
    {
      "epoch": 1.7182320441988952,
      "grad_norm": 0.12569184601306915,
      "learning_rate": 0.00010582959641255605,
      "loss": 1.2043,
      "step": 156
    },
    {
      "epoch": 1.729281767955801,
      "grad_norm": 0.13226279616355896,
      "learning_rate": 0.00010493273542600897,
      "loss": 1.2455,
      "step": 157
    },
    {
      "epoch": 1.7403314917127073,
      "grad_norm": 0.13589820265769958,
      "learning_rate": 0.00010403587443946188,
      "loss": 1.2364,
      "step": 158
    },
    {
      "epoch": 1.7513812154696131,
      "grad_norm": 0.129287987947464,
      "learning_rate": 0.00010313901345291481,
      "loss": 1.1408,
      "step": 159
    },
    {
      "epoch": 1.7624309392265194,
      "grad_norm": 0.13098253309726715,
      "learning_rate": 0.00010224215246636773,
      "loss": 1.1636,
      "step": 160
    },
    {
      "epoch": 1.7734806629834254,
      "grad_norm": 0.13531069457530975,
      "learning_rate": 0.00010134529147982064,
      "loss": 1.2083,
      "step": 161
    },
    {
      "epoch": 1.7845303867403315,
      "grad_norm": 0.14166946709156036,
      "learning_rate": 0.00010044843049327355,
      "loss": 1.2039,
      "step": 162
    },
    {
      "epoch": 1.7955801104972375,
      "grad_norm": 0.15599294006824493,
      "learning_rate": 9.955156950672646e-05,
      "loss": 1.3335,
      "step": 163
    },
    {
      "epoch": 1.8066298342541436,
      "grad_norm": 0.13151581585407257,
      "learning_rate": 9.865470852017937e-05,
      "loss": 1.1611,
      "step": 164
    },
    {
      "epoch": 1.8176795580110499,
      "grad_norm": 0.12068072706460953,
      "learning_rate": 9.77578475336323e-05,
      "loss": 1.0745,
      "step": 165
    },
    {
      "epoch": 1.8287292817679557,
      "grad_norm": 0.14391185343265533,
      "learning_rate": 9.686098654708521e-05,
      "loss": 1.2155,
      "step": 166
    },
    {
      "epoch": 1.839779005524862,
      "grad_norm": 0.13083644211292267,
      "learning_rate": 9.596412556053813e-05,
      "loss": 1.1355,
      "step": 167
    },
    {
      "epoch": 1.850828729281768,
      "grad_norm": 0.13516850769519806,
      "learning_rate": 9.506726457399103e-05,
      "loss": 1.1867,
      "step": 168
    },
    {
      "epoch": 1.861878453038674,
      "grad_norm": 0.1375536322593689,
      "learning_rate": 9.417040358744396e-05,
      "loss": 1.19,
      "step": 169
    },
    {
      "epoch": 1.87292817679558,
      "grad_norm": 0.14014975726604462,
      "learning_rate": 9.327354260089687e-05,
      "loss": 1.2161,
      "step": 170
    },
    {
      "epoch": 1.8839779005524862,
      "grad_norm": 0.13965986669063568,
      "learning_rate": 9.237668161434979e-05,
      "loss": 1.1943,
      "step": 171
    },
    {
      "epoch": 1.8950276243093924,
      "grad_norm": 0.13282392919063568,
      "learning_rate": 9.147982062780269e-05,
      "loss": 1.2177,
      "step": 172
    },
    {
      "epoch": 1.9060773480662982,
      "grad_norm": 0.13778401911258698,
      "learning_rate": 9.05829596412556e-05,
      "loss": 1.1966,
      "step": 173
    },
    {
      "epoch": 1.9171270718232045,
      "grad_norm": 0.1436810940504074,
      "learning_rate": 8.968609865470853e-05,
      "loss": 1.237,
      "step": 174
    },
    {
      "epoch": 1.9281767955801103,
      "grad_norm": 0.13459424674510956,
      "learning_rate": 8.878923766816144e-05,
      "loss": 1.1587,
      "step": 175
    },
    {
      "epoch": 1.9392265193370166,
      "grad_norm": 0.13356249034404755,
      "learning_rate": 8.789237668161436e-05,
      "loss": 1.1723,
      "step": 176
    },
    {
      "epoch": 1.9502762430939227,
      "grad_norm": 0.14552068710327148,
      "learning_rate": 8.699551569506726e-05,
      "loss": 1.2061,
      "step": 177
    },
    {
      "epoch": 1.9613259668508287,
      "grad_norm": 0.13845627009868622,
      "learning_rate": 8.609865470852019e-05,
      "loss": 1.2093,
      "step": 178
    },
    {
      "epoch": 1.9723756906077348,
      "grad_norm": 0.12611214816570282,
      "learning_rate": 8.52017937219731e-05,
      "loss": 1.1422,
      "step": 179
    },
    {
      "epoch": 1.9834254143646408,
      "grad_norm": 0.13590598106384277,
      "learning_rate": 8.430493273542602e-05,
      "loss": 1.1786,
      "step": 180
    },
    {
      "epoch": 1.994475138121547,
      "grad_norm": 0.14066864550113678,
      "learning_rate": 8.340807174887892e-05,
      "loss": 1.2163,
      "step": 181
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.18825817108154297,
      "learning_rate": 8.251121076233185e-05,
      "loss": 1.2096,
      "step": 182
    },
    {
      "epoch": 2.0110497237569063,
      "grad_norm": 0.13236920535564423,
      "learning_rate": 8.161434977578476e-05,
      "loss": 1.1066,
      "step": 183
    },
    {
      "epoch": 2.022099447513812,
      "grad_norm": 0.13936704397201538,
      "learning_rate": 8.071748878923767e-05,
      "loss": 1.1976,
      "step": 184
    },
    {
      "epoch": 2.0331491712707184,
      "grad_norm": 0.1510324478149414,
      "learning_rate": 7.982062780269059e-05,
      "loss": 1.2065,
      "step": 185
    },
    {
      "epoch": 2.044198895027624,
      "grad_norm": 0.1316072791814804,
      "learning_rate": 7.892376681614349e-05,
      "loss": 1.2548,
      "step": 186
    },
    {
      "epoch": 2.0552486187845305,
      "grad_norm": 0.14278744161128998,
      "learning_rate": 7.802690582959642e-05,
      "loss": 1.1839,
      "step": 187
    },
    {
      "epoch": 2.0662983425414363,
      "grad_norm": 0.13914045691490173,
      "learning_rate": 7.713004484304933e-05,
      "loss": 1.1846,
      "step": 188
    },
    {
      "epoch": 2.0773480662983426,
      "grad_norm": 0.1495669037103653,
      "learning_rate": 7.623318385650225e-05,
      "loss": 1.1879,
      "step": 189
    },
    {
      "epoch": 2.0883977900552484,
      "grad_norm": 0.12901249527931213,
      "learning_rate": 7.533632286995516e-05,
      "loss": 1.2071,
      "step": 190
    },
    {
      "epoch": 2.0994475138121547,
      "grad_norm": 0.1301012933254242,
      "learning_rate": 7.443946188340808e-05,
      "loss": 1.1631,
      "step": 191
    },
    {
      "epoch": 2.110497237569061,
      "grad_norm": 0.13661208748817444,
      "learning_rate": 7.354260089686099e-05,
      "loss": 1.192,
      "step": 192
    },
    {
      "epoch": 2.1215469613259668,
      "grad_norm": 0.1296112835407257,
      "learning_rate": 7.26457399103139e-05,
      "loss": 1.1722,
      "step": 193
    },
    {
      "epoch": 2.132596685082873,
      "grad_norm": 0.12827247381210327,
      "learning_rate": 7.174887892376682e-05,
      "loss": 1.0448,
      "step": 194
    },
    {
      "epoch": 2.143646408839779,
      "grad_norm": 0.14196033775806427,
      "learning_rate": 7.085201793721974e-05,
      "loss": 1.2324,
      "step": 195
    },
    {
      "epoch": 2.154696132596685,
      "grad_norm": 0.14312300086021423,
      "learning_rate": 6.995515695067265e-05,
      "loss": 1.1752,
      "step": 196
    },
    {
      "epoch": 2.165745856353591,
      "grad_norm": 0.14513345062732697,
      "learning_rate": 6.905829596412556e-05,
      "loss": 1.1467,
      "step": 197
    },
    {
      "epoch": 2.1767955801104972,
      "grad_norm": 0.14220775663852692,
      "learning_rate": 6.816143497757848e-05,
      "loss": 1.2419,
      "step": 198
    },
    {
      "epoch": 2.1878453038674035,
      "grad_norm": 0.13592220842838287,
      "learning_rate": 6.72645739910314e-05,
      "loss": 1.148,
      "step": 199
    },
    {
      "epoch": 2.1988950276243093,
      "grad_norm": 0.13960731029510498,
      "learning_rate": 6.636771300448431e-05,
      "loss": 1.1428,
      "step": 200
    },
    {
      "epoch": 2.1988950276243093,
      "eval_loss": 1.1852086782455444,
      "eval_runtime": 112.0349,
      "eval_samples_per_second": 3.231,
      "eval_steps_per_second": 3.231,
      "step": 200
    },
    {
      "epoch": 2.2099447513812156,
      "grad_norm": 0.14085990190505981,
      "learning_rate": 6.547085201793722e-05,
      "loss": 1.2047,
      "step": 201
    },
    {
      "epoch": 2.2209944751381214,
      "grad_norm": 0.15022090077400208,
      "learning_rate": 6.457399103139014e-05,
      "loss": 1.2512,
      "step": 202
    },
    {
      "epoch": 2.2320441988950277,
      "grad_norm": 0.13210399448871613,
      "learning_rate": 6.367713004484305e-05,
      "loss": 1.1659,
      "step": 203
    },
    {
      "epoch": 2.2430939226519335,
      "grad_norm": 0.14121820032596588,
      "learning_rate": 6.278026905829597e-05,
      "loss": 1.1449,
      "step": 204
    },
    {
      "epoch": 2.25414364640884,
      "grad_norm": 0.14021040499210358,
      "learning_rate": 6.188340807174888e-05,
      "loss": 1.2239,
      "step": 205
    },
    {
      "epoch": 2.265193370165746,
      "grad_norm": 0.1444810926914215,
      "learning_rate": 6.0986547085201795e-05,
      "loss": 1.239,
      "step": 206
    },
    {
      "epoch": 2.276243093922652,
      "grad_norm": 0.14589659869670868,
      "learning_rate": 6.008968609865471e-05,
      "loss": 1.2169,
      "step": 207
    },
    {
      "epoch": 2.287292817679558,
      "grad_norm": 0.13419482111930847,
      "learning_rate": 5.919282511210763e-05,
      "loss": 1.2049,
      "step": 208
    },
    {
      "epoch": 2.298342541436464,
      "grad_norm": 0.14080733060836792,
      "learning_rate": 5.8295964125560546e-05,
      "loss": 1.1971,
      "step": 209
    },
    {
      "epoch": 2.3093922651933703,
      "grad_norm": 0.1432117372751236,
      "learning_rate": 5.7399103139013454e-05,
      "loss": 1.2528,
      "step": 210
    },
    {
      "epoch": 2.320441988950276,
      "grad_norm": 0.14047624170780182,
      "learning_rate": 5.650224215246637e-05,
      "loss": 1.2211,
      "step": 211
    },
    {
      "epoch": 2.3314917127071824,
      "grad_norm": 0.13742227852344513,
      "learning_rate": 5.560538116591929e-05,
      "loss": 1.1597,
      "step": 212
    },
    {
      "epoch": 2.3425414364640886,
      "grad_norm": 0.14156349003314972,
      "learning_rate": 5.4708520179372204e-05,
      "loss": 1.0704,
      "step": 213
    },
    {
      "epoch": 2.3535911602209945,
      "grad_norm": 0.13133397698402405,
      "learning_rate": 5.381165919282511e-05,
      "loss": 1.1563,
      "step": 214
    },
    {
      "epoch": 2.3646408839779007,
      "grad_norm": 0.12751103937625885,
      "learning_rate": 5.291479820627803e-05,
      "loss": 1.0748,
      "step": 215
    },
    {
      "epoch": 2.3756906077348066,
      "grad_norm": 0.14878465235233307,
      "learning_rate": 5.201793721973094e-05,
      "loss": 1.1929,
      "step": 216
    },
    {
      "epoch": 2.386740331491713,
      "grad_norm": 0.13583610951900482,
      "learning_rate": 5.112107623318386e-05,
      "loss": 1.173,
      "step": 217
    },
    {
      "epoch": 2.3977900552486187,
      "grad_norm": 0.1332387775182724,
      "learning_rate": 5.022421524663678e-05,
      "loss": 1.1251,
      "step": 218
    },
    {
      "epoch": 2.408839779005525,
      "grad_norm": 0.1355777531862259,
      "learning_rate": 4.9327354260089685e-05,
      "loss": 1.188,
      "step": 219
    },
    {
      "epoch": 2.4198895027624308,
      "grad_norm": 0.14754576981067657,
      "learning_rate": 4.8430493273542606e-05,
      "loss": 1.1978,
      "step": 220
    },
    {
      "epoch": 2.430939226519337,
      "grad_norm": 0.14706197381019592,
      "learning_rate": 4.7533632286995514e-05,
      "loss": 1.2347,
      "step": 221
    },
    {
      "epoch": 2.441988950276243,
      "grad_norm": 0.1406160593032837,
      "learning_rate": 4.6636771300448435e-05,
      "loss": 1.2344,
      "step": 222
    },
    {
      "epoch": 2.453038674033149,
      "grad_norm": 0.14228396117687225,
      "learning_rate": 4.573991031390134e-05,
      "loss": 1.1881,
      "step": 223
    },
    {
      "epoch": 2.4640883977900554,
      "grad_norm": 0.14881455898284912,
      "learning_rate": 4.4843049327354265e-05,
      "loss": 1.1996,
      "step": 224
    },
    {
      "epoch": 2.4751381215469612,
      "grad_norm": 0.13599319756031036,
      "learning_rate": 4.394618834080718e-05,
      "loss": 1.1819,
      "step": 225
    },
    {
      "epoch": 2.4861878453038675,
      "grad_norm": 0.14460107684135437,
      "learning_rate": 4.3049327354260094e-05,
      "loss": 1.1756,
      "step": 226
    },
    {
      "epoch": 2.4972375690607733,
      "grad_norm": 0.13818028569221497,
      "learning_rate": 4.215246636771301e-05,
      "loss": 1.1789,
      "step": 227
    },
    {
      "epoch": 2.5082872928176796,
      "grad_norm": 0.1392141878604889,
      "learning_rate": 4.125560538116592e-05,
      "loss": 1.1895,
      "step": 228
    },
    {
      "epoch": 2.5193370165745854,
      "grad_norm": 0.15138228237628937,
      "learning_rate": 4.035874439461884e-05,
      "loss": 1.239,
      "step": 229
    },
    {
      "epoch": 2.5303867403314917,
      "grad_norm": 0.1476040482521057,
      "learning_rate": 3.9461883408071745e-05,
      "loss": 1.1684,
      "step": 230
    },
    {
      "epoch": 2.541436464088398,
      "grad_norm": 0.1321871280670166,
      "learning_rate": 3.8565022421524667e-05,
      "loss": 1.1213,
      "step": 231
    },
    {
      "epoch": 2.552486187845304,
      "grad_norm": 0.14683537185192108,
      "learning_rate": 3.766816143497758e-05,
      "loss": 1.2498,
      "step": 232
    },
    {
      "epoch": 2.56353591160221,
      "grad_norm": 0.14491155743598938,
      "learning_rate": 3.6771300448430496e-05,
      "loss": 1.2925,
      "step": 233
    },
    {
      "epoch": 2.574585635359116,
      "grad_norm": 0.1462743729352951,
      "learning_rate": 3.587443946188341e-05,
      "loss": 1.107,
      "step": 234
    },
    {
      "epoch": 2.585635359116022,
      "grad_norm": 0.14196184277534485,
      "learning_rate": 3.4977578475336325e-05,
      "loss": 1.2043,
      "step": 235
    },
    {
      "epoch": 2.596685082872928,
      "grad_norm": 0.14114074409008026,
      "learning_rate": 3.408071748878924e-05,
      "loss": 1.2082,
      "step": 236
    },
    {
      "epoch": 2.6077348066298343,
      "grad_norm": 0.134092777967453,
      "learning_rate": 3.3183856502242154e-05,
      "loss": 1.1984,
      "step": 237
    },
    {
      "epoch": 2.6187845303867405,
      "grad_norm": 0.13749079406261444,
      "learning_rate": 3.228699551569507e-05,
      "loss": 1.1391,
      "step": 238
    },
    {
      "epoch": 2.6298342541436464,
      "grad_norm": 0.13297322392463684,
      "learning_rate": 3.139013452914798e-05,
      "loss": 1.1327,
      "step": 239
    },
    {
      "epoch": 2.6408839779005526,
      "grad_norm": 0.14756186306476593,
      "learning_rate": 3.0493273542600898e-05,
      "loss": 1.2524,
      "step": 240
    },
    {
      "epoch": 2.6519337016574585,
      "grad_norm": 0.1398114114999771,
      "learning_rate": 2.9596412556053816e-05,
      "loss": 1.1877,
      "step": 241
    },
    {
      "epoch": 2.6629834254143647,
      "grad_norm": 0.13789185881614685,
      "learning_rate": 2.8699551569506727e-05,
      "loss": 1.1856,
      "step": 242
    },
    {
      "epoch": 2.6740331491712706,
      "grad_norm": 0.13744238018989563,
      "learning_rate": 2.7802690582959645e-05,
      "loss": 1.1945,
      "step": 243
    },
    {
      "epoch": 2.685082872928177,
      "grad_norm": 0.14135535061359406,
      "learning_rate": 2.6905829596412556e-05,
      "loss": 1.2109,
      "step": 244
    },
    {
      "epoch": 2.696132596685083,
      "grad_norm": 0.1481398046016693,
      "learning_rate": 2.600896860986547e-05,
      "loss": 1.2689,
      "step": 245
    },
    {
      "epoch": 2.707182320441989,
      "grad_norm": 0.1440693736076355,
      "learning_rate": 2.511210762331839e-05,
      "loss": 1.1987,
      "step": 246
    },
    {
      "epoch": 2.718232044198895,
      "grad_norm": 0.14274674654006958,
      "learning_rate": 2.4215246636771303e-05,
      "loss": 1.2367,
      "step": 247
    },
    {
      "epoch": 2.729281767955801,
      "grad_norm": 0.15409952402114868,
      "learning_rate": 2.3318385650224218e-05,
      "loss": 1.1369,
      "step": 248
    },
    {
      "epoch": 2.7403314917127073,
      "grad_norm": 0.16232196986675262,
      "learning_rate": 2.2421524663677132e-05,
      "loss": 1.2475,
      "step": 249
    },
    {
      "epoch": 2.751381215469613,
      "grad_norm": 0.14239932596683502,
      "learning_rate": 2.1524663677130047e-05,
      "loss": 1.2698,
      "step": 250
    },
    {
      "epoch": 2.751381215469613,
      "eval_loss": 1.1783573627471924,
      "eval_runtime": 111.6597,
      "eval_samples_per_second": 3.242,
      "eval_steps_per_second": 3.242,
      "step": 250
    },
    {
      "epoch": 2.7624309392265194,
      "grad_norm": 0.14072349667549133,
      "learning_rate": 2.062780269058296e-05,
      "loss": 1.1897,
      "step": 251
    },
    {
      "epoch": 2.7734806629834257,
      "grad_norm": 0.13499389588832855,
      "learning_rate": 1.9730941704035873e-05,
      "loss": 1.2034,
      "step": 252
    },
    {
      "epoch": 2.7845303867403315,
      "grad_norm": 0.14412154257297516,
      "learning_rate": 1.883408071748879e-05,
      "loss": 1.198,
      "step": 253
    },
    {
      "epoch": 2.7955801104972373,
      "grad_norm": 0.1348651647567749,
      "learning_rate": 1.7937219730941705e-05,
      "loss": 1.1277,
      "step": 254
    },
    {
      "epoch": 2.8066298342541436,
      "grad_norm": 0.13379529118537903,
      "learning_rate": 1.704035874439462e-05,
      "loss": 1.1363,
      "step": 255
    },
    {
      "epoch": 2.81767955801105,
      "grad_norm": 0.13968749344348907,
      "learning_rate": 1.6143497757847534e-05,
      "loss": 1.2286,
      "step": 256
    },
    {
      "epoch": 2.8287292817679557,
      "grad_norm": 0.1438114196062088,
      "learning_rate": 1.5246636771300449e-05,
      "loss": 1.1932,
      "step": 257
    },
    {
      "epoch": 2.839779005524862,
      "grad_norm": 0.13975466787815094,
      "learning_rate": 1.4349775784753363e-05,
      "loss": 1.1782,
      "step": 258
    },
    {
      "epoch": 2.8508287292817682,
      "grad_norm": 0.14949047565460205,
      "learning_rate": 1.3452914798206278e-05,
      "loss": 1.2172,
      "step": 259
    },
    {
      "epoch": 2.861878453038674,
      "grad_norm": 0.1422276347875595,
      "learning_rate": 1.2556053811659194e-05,
      "loss": 1.151,
      "step": 260
    },
    {
      "epoch": 2.87292817679558,
      "grad_norm": 0.13477963209152222,
      "learning_rate": 1.1659192825112109e-05,
      "loss": 1.0912,
      "step": 261
    },
    {
      "epoch": 2.883977900552486,
      "grad_norm": 0.13753247261047363,
      "learning_rate": 1.0762331838565023e-05,
      "loss": 1.2073,
      "step": 262
    },
    {
      "epoch": 2.8950276243093924,
      "grad_norm": 0.14829720556735992,
      "learning_rate": 9.865470852017936e-06,
      "loss": 1.1837,
      "step": 263
    },
    {
      "epoch": 2.9060773480662982,
      "grad_norm": 0.14303740859031677,
      "learning_rate": 8.968609865470853e-06,
      "loss": 1.163,
      "step": 264
    },
    {
      "epoch": 2.9171270718232045,
      "grad_norm": 0.14361609518527985,
      "learning_rate": 8.071748878923767e-06,
      "loss": 1.1008,
      "step": 265
    },
    {
      "epoch": 2.9281767955801103,
      "grad_norm": 0.1420997530221939,
      "learning_rate": 7.174887892376682e-06,
      "loss": 1.2255,
      "step": 266
    },
    {
      "epoch": 2.9392265193370166,
      "grad_norm": 0.13958905637264252,
      "learning_rate": 6.278026905829597e-06,
      "loss": 1.1483,
      "step": 267
    },
    {
      "epoch": 2.9502762430939224,
      "grad_norm": 0.13999125361442566,
      "learning_rate": 5.381165919282512e-06,
      "loss": 1.1166,
      "step": 268
    },
    {
      "epoch": 2.9613259668508287,
      "grad_norm": 0.14035522937774658,
      "learning_rate": 4.484304932735426e-06,
      "loss": 1.1254,
      "step": 269
    },
    {
      "epoch": 2.972375690607735,
      "grad_norm": 0.14040210843086243,
      "learning_rate": 3.587443946188341e-06,
      "loss": 1.1617,
      "step": 270
    },
    {
      "epoch": 2.983425414364641,
      "grad_norm": 0.1448194682598114,
      "learning_rate": 2.690582959641256e-06,
      "loss": 1.2789,
      "step": 271
    },
    {
      "epoch": 2.994475138121547,
      "grad_norm": 0.1375935971736908,
      "learning_rate": 1.7937219730941704e-06,
      "loss": 1.2261,
      "step": 272
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.19693993031978607,
      "learning_rate": 8.968609865470852e-07,
      "loss": 1.0488,
      "step": 273
    },
    {
      "epoch": 3.0,
      "step": 273,
      "total_flos": 1.5930738885097267e+17,
      "train_loss": 1.2958359211792438,
      "train_runtime": 3903.0143,
      "train_samples_per_second": 1.113,
      "train_steps_per_second": 0.07
    }
  ],
  "logging_steps": 1,
  "max_steps": 273,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.5930738885097267e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
