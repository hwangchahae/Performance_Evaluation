# Optimized version with better error handling and performance
import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import logging
import gc
import traceback

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ëª¨ë¸ ì„ íƒ
model_path = "Qwen/Qwen3-4B-AWQ"
logger.info(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# ì „ì—­ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €
llm = None
tokenizer = None
sampling_params = None

def initialize_model():
    """ëª¨ë¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)"""
    global llm, tokenizer, sampling_params
    
    if llm is None:
        logger.info(f"ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # VLLM ì—”ì§„ ì´ˆê¸°í™”
            llm = LLM(
                model=model_path,
                quantization="awq" if "AWQ" in model_path else None,
                tensor_parallel_size=1,
                max_model_len=16384,
                gpu_memory_utilization=0.9,
                trust_remote_code=True,
                enforce_eager=False,
                max_num_seqs=256,
            )
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
            sampling_params = SamplingParams(
                temperature=0.2,
                max_tokens=2048,
                skip_special_tokens=True,
            )
            logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

def generate_notion_project_prompt(meeting_transcript: str) -> str:
    """ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„± í”„ë¡¬í”„íŠ¸"""
    return f"""ë‹¤ìŒ íšŒì˜ ì „ì‚¬ë³¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì— ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.

**íšŒì˜ ì „ì‚¬ë³¸:**
{meeting_transcript}

**ì‘ì„± ì§€ì¹¨:**
1. íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ê¸°íšì•ˆì„ ì‘ì„±
2. í”„ë¡œì íŠ¸ëª…ì€ íšŒì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆíˆ ëª…ëª…
3. ëª©ì ê³¼ ëª©í‘œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
4. ì‹¤í–‰ ê³„íšì€ ì‹¤í˜„ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ë¡œ êµ¬ì„±
5. ê¸°ëŒ€ íš¨ê³¼ëŠ” ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ë¥¼ í¬í•¨
6. ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±

**ì‘ë‹µ í˜•ì‹:**
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "project_name": "í”„ë¡œì íŠ¸ëª…",
    "project_purpose": "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ",
    "project_period": "ì˜ˆìƒ ìˆ˜í–‰ ê¸°ê°„",
    "project_manager": "ë‹´ë‹¹ìëª…",
    "core_objectives": ["ëª©í‘œ 1", "ëª©í‘œ 2", "ëª©í‘œ 3"],
    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´",
    "idea_description": "ì•„ì´ë””ì–´ ì„¤ëª…",
    "execution_plan": "ì‹¤í–‰ ê³„íš",
    "expected_effects": ["íš¨ê³¼ 1", "íš¨ê³¼ 2", "íš¨ê³¼ 3"]
}}"""

def chunk_text(text: str, chunk_size: int = 5000, overlap: int = 512) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ì—¬ ë‚˜ëˆ„ê¸° (ë¬¸ì ë‹¨ìœ„)"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunk = text[start:]
        else:
            chunk = text[start:end]
            
            # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ì—ì„œ ëŠê¸° ì‹œë„
            last_period = chunk.rfind('.')
            last_newline = chunk.rfind('\n')
            break_point = max(last_period, last_newline)
            
            if break_point > start + chunk_size // 2:
                chunk = text[start:break_point + 1]
                end = break_point + 1
        
        chunks.append(chunk.strip())
        
        if end >= len(text):
            break
            
        start = end - overlap
    
    logger.info(f"ğŸ“Š {len(chunks)}ê°œ ì²­í¬ ìƒì„± (ë¬¸ì ê¸°ë°˜ 5000ì)")
    return chunks

def load_json_file(file_path: str) -> List[Dict]:
    """JSON íŒŒì¼ ë¡œë“œ - timestampì™€ speaker ì •ë³´ í¬í•¨"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
            if isinstance(data, list):
                return [{"timestamp": item.get("timestamp", "Unknown"),
                        "speaker": item.get("speaker", "Unknown"), 
                        "text": item.get("text", "")} 
                       for item in data if "text" in item]
            return []
    except Exception as e:
        logger.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
        return []

def batch_generate_responses(prompts: List[str]) -> List[str]:
    """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë™ì‹œ ìƒì„±"""
    if not prompts:
        return []
    
    try:
        logger.info(f"ğŸš€ {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
        outputs = llm.generate(prompts, sampling_params)
        
        results = []
        for output in outputs:
            if output.outputs:
                results.append(output.outputs[0].text.strip())
            else:
                results.append("{}")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return ["{}" for _ in prompts]

def parse_json_response(response: str) -> Dict:
    """JSON ì‘ë‹µ íŒŒì‹± with better error handling"""
    try:
        # JSON ë¸”ë¡ ì¶”ì¶œ
        if "```json" in response:
            start = response.find("```json") + 7
            end = response.find("```", start)
            json_str = response[start:end].strip()
        elif "{" in response and "}" in response:
            start = response.find("{")
            end = response.rfind("}") + 1
            json_str = response[start:end]
        else:
            return {"error": "No JSON found", "raw_text": response}
        
        # JSON íŒŒì‹±
        parsed = json.loads(json_str)
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ["project_name", "project_purpose"]
        for field in required_fields:
            if field not in parsed:
                parsed[field] = "ë¯¸ì •"
        
        return parsed
        
    except json.JSONDecodeError as e:
        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {"error": "JSON parsing failed", "raw_text": response}
    except Exception as e:
        logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return {"error": str(e), "raw_text": response}

def process_files_batch(files_data: List[Tuple[str, str, List[str]]]) -> List[Dict]:
    """ì—¬ëŸ¬ íŒŒì¼ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
    all_prompts = []
    metadata = []
    
    # ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    for folder_name, file_path, chunks in files_data:
        for idx, chunk in enumerate(chunks):
            prompt = generate_notion_project_prompt(chunk)
            all_prompts.append(prompt)
            metadata.append({
                "folder_name": folder_name,
                "file_path": file_path,
                "chunk_idx": idx,
                "total_chunks": len(chunks),
                "is_chunked": len(chunks) > 1
            })
    
    # ë°°ì¹˜ ìƒì„±
    responses = batch_generate_responses(all_prompts)
    
    # ê²°ê³¼ ì •ë¦¬
    results = []
    for response, meta in zip(responses, metadata):
        result = {
            "folder_name": meta["folder_name"],
            "chunk_idx": meta["chunk_idx"],
            "total_chunks": meta["total_chunks"],
            "response": parse_json_response(response),
            "metadata": meta
        }
        results.append(result)
    
    return results

def save_results(results: List[Dict], output_dir: str):
    """ê²°ê³¼ ì €ì¥ with better error handling"""
    try:
        os.makedirs(output_dir, exist_ok=True)
        
        # í´ë”ë³„ë¡œ ê·¸ë£¹í™”
        grouped = {}
        for result in results:
            folder = result["folder_name"]
            if folder not in grouped:
                grouped[folder] = []
            grouped[folder].append(result)
        
        # ì €ì¥
        saved_count = 0
        for folder_name, folder_results in grouped.items():
            for result in folder_results:
                try:
                    if result["total_chunks"] == 1:
                        # ë‹¨ì¼ íŒŒì¼
                        chunk_dir = os.path.join(output_dir, folder_name)
                        chunk_id = folder_name
                    else:
                        # ì²­í‚¹ëœ íŒŒì¼
                        chunk_dir = os.path.join(output_dir, f"{folder_name}_chunk_{result['chunk_idx']+1}")
                        chunk_id = f"{folder_name}_chunk_{result['chunk_idx']+1}"
                    
                    os.makedirs(chunk_dir, exist_ok=True)
                    
                    output_data = {
                        "id": chunk_id,
                        "source_dir": folder_name,
                        "notion_output": result["response"],
                        "metadata": {
                            "source_file": result["metadata"]["file_path"],
                            "is_chunk": result["metadata"]["is_chunked"],
                            "chunk_index": result["chunk_idx"] + 1 if result["metadata"]["is_chunked"] else None,
                            "total_chunks": result["total_chunks"],
                            "processing_date": datetime.now().isoformat(),
                            "model_used": model_path
                        }
                    }
                    
                    output_file = os.path.join(chunk_dir, "result.json")
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, ensure_ascii=False, indent=2)
                    saved_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ ({chunk_id}): {e}")
                    continue
        
        logger.info(f"âœ… {saved_count}/{len(results)}ê°œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

def process_with_memory_management(target_files: List[Tuple[str, str]], 
                                  output_directory: str,
                                  batch_size: int = 3,
                                  max_chunks_per_batch: int = 30):
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ê³ ë ¤í•œ ì²˜ë¦¬"""
    
    current_batch = []
    current_chunk_count = 0
    batch_num = 1
    total_processed = 0
    
    for i, (folder_name, file_path) in enumerate(tqdm(target_files, desc="íŒŒì¼ ì²˜ë¦¬")):
        try:
            # íŒŒì¼ ë¡œë“œ
            utterances = load_json_file(file_path)
            if not utterances:
                logger.warning(f"âš ï¸ ë¹ˆ íŒŒì¼ ê±´ë„ˆëœ€: {file_path}")
                continue
            
            # íšŒì˜ë¡ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ê²°í•©
            meeting_lines = []
            for utt in utterances:
                if utt.get("text"):
                    line = f"[{utt['timestamp']}] {utt['speaker']}: {utt['text']}"
                    meeting_lines.append(line)
            
            if not meeting_lines:
                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì—†ìŒ: {file_path}")
                continue
            
            full_text = "\n".join(meeting_lines)
            chunks = chunk_text(full_text, chunk_size=5000, overlap=512)
            
            # ì²­í¬ ìˆ˜ í™•ì¸ ë° ë°°ì¹˜ ì²˜ë¦¬
            if current_chunk_count + len(chunks) > max_chunks_per_batch and current_batch:
                # í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
                logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({current_chunk_count}ê°œ ì²­í¬)")
                results = process_files_batch(current_batch)
                save_results(results, output_directory)
                total_processed += len(current_batch)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del results
                gc.collect()
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                # ìƒˆ ë°°ì¹˜ ì‹œì‘
                batch_num += 1
                current_batch = [(folder_name, file_path, chunks)]
                current_chunk_count = len(chunks)
            else:
                # í˜„ì¬ ë°°ì¹˜ì— ì¶”ê°€
                current_batch.append((folder_name, file_path, chunks))
                current_chunk_count += len(chunks)
                
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path}): {e}")
            continue
    
    # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
    if current_batch:
        try:
            logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({current_chunk_count}ê°œ ì²­í¬)")
            results = process_files_batch(current_batch)
            save_results(results, output_directory)
            total_processed += len(current_batch)
        except Exception as e:
            logger.error(f"âŒ ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    return total_processed

def validate_input_directory(base_directory: str) -> List[Tuple[str, str]]:
    """ì…ë ¥ ë””ë ‰í† ë¦¬ ê²€ì¦ ë° íŒŒì¼ ëª©ë¡ ìƒì„±"""
    if not os.path.exists(base_directory):
        raise FileNotFoundError(f"ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_directory}")
    
    target_files = []
    for folder in os.listdir(base_directory):
        folder_path = os.path.join(base_directory, folder)
        if os.path.isdir(folder_path):
            json_file = os.path.join(folder_path, "05_final_result.json")
            if os.path.exists(json_file):
                target_files.append((folder, json_file))
    
    if not target_files:
        raise ValueError(f"ì²˜ë¦¬í•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_directory}")
    
    return target_files

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì •
    base_directory = "../Raw_Data_val"
    output_directory = "4B_awq_model_results_optimized"
    batch_size = 3
    max_chunks_per_batch = 30
    
    logger.info("="*60)
    logger.info("ğŸš€ ìµœì í™”ëœ íšŒì˜ë¡ ì²˜ë¦¬ ì‹œì‘")
    logger.info(f"ğŸ“‚ ì…ë ¥: {base_directory}")
    logger.info(f"ğŸ“‚ ì¶œë ¥: {output_directory}")
    logger.info(f"âš™ï¸ ë°°ì¹˜ í¬ê¸°: {batch_size}, ìµœëŒ€ ì²­í¬/ë°°ì¹˜: {max_chunks_per_batch}")
    logger.info("="*60)
    
    try:
        # ì…ë ¥ ê²€ì¦
        target_files = validate_input_directory(base_directory)
        logger.info(f"ğŸ“ {len(target_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        initialize_model()
        
        # ì²˜ë¦¬ ì‹œì‘
        start_time = datetime.now()
        processed_count = process_with_memory_management(
            target_files, 
            output_directory,
            batch_size,
            max_chunks_per_batch
        )
        
        # ì™„ë£Œ
        elapsed_time = datetime.now() - start_time
        logger.info("="*60)
        logger.info(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼: {processed_count}/{len(target_files)}")
        logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time}")
        logger.info(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_directory}")
        logger.info("="*60)
        
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()