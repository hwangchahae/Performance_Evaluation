# 학습 후(Post-Training) 모델 성능 평가 종합 요약

## 평가 개요
- **평가 일시**: 2025년 8월 10일
- **평가 대상**: Qwen 모델 (1.7B, 4B, 8B) - LoRA 파인튜닝 완료
- **평가 상태**: 학습 후 파인튜닝된 모델
- **전체 파일 수**: 368개
- **샘플링 크기**: 100개 (27.2%)
- **랜덤 시드**: 42

## 모델별 성능 비교

| 모델 크기 | TF-IDF 코사인 유사도 | Embedding 코사인 유사도 |
|----------|---------------------|------------------------|
| **1.7B** | 0.1912             | 0.9024                |
| **4B**   | 0.2847             | 0.9159                |
| **8B**   | 0.2895             | 0.9219                |

## 주요 발견사항

### 1. TF-IDF 유사도 분석
- **1.7B → 4B**: 48.9% 향상 (0.1912 → 0.2847)
- **4B → 8B**: 1.7% 향상 (0.2847 → 0.2895)
- **전체 향상**: 1.7B 대비 8B는 51.4% 향상

### 2. Embedding 유사도 분석
- **1.7B → 4B**: 1.5% 향상 (0.9024 → 0.9159)
- **4B → 8B**: 0.7% 향상 (0.9159 → 0.9219)
- **전체 향상**: 1.7B 대비 8B는 2.2% 향상

## 학습 전후 성능 비교

### TF-IDF 유사도 개선율
| 모델 | 학습 전 | 학습 후 | 개선율 |
|------|---------|---------|--------|
| 1.7B | 0.1878  | 0.1912  | +1.8%  |
| 4B   | 0.2086  | 0.2847  | +36.5% |
| 8B   | 0.2108  | 0.2895  | +37.3% |

### Embedding 유사도 개선율
| 모델 | 학습 전 | 학습 후 | 개선율 |
|------|---------|---------|--------|
| 1.7B | 0.9002  | 0.9024  | +0.2%  |
| 4B   | 0.9115  | 0.9159  | +0.5%  |
| 8B   | 0.9178  | 0.9219  | +0.4%  |

## 최고 성능 파일 분석

### 4B 모델 최고 성능 (TF-IDF 기준)
1. 제22대국회 회의록: 0.5232
2. 산업통상자원중소벤처기업위원회: 0.4992
3. TS3009b_chunk_2: 0.4974

### 8B 모델 최고 성능 (TF-IDF 기준)
1. 외교통일위원회: 0.5752
2. IS1000b_chunk_2: 0.5192
3. 정무위원회: 0.5014

### Embedding 최고 성능 (8B 모델)
1. ES2014a: 0.9826
2. IS1000b_chunk_2: 0.9728
3. 행정안전위원회: 0.9709

## 성능 패턴 분석

### 파인튜닝 효과
1. **TF-IDF 성능 대폭 개선**
   - 4B, 8B 모델: 36-37% 개선
   - 1.7B 모델: 제한적 개선 (1.8%)

2. **Embedding 성능 미미한 개선**
   - 모든 모델: 0.2-0.5% 개선
   - 이미 높은 기준선에서 소폭 상승

3. **모델 크기별 파인튜닝 효과**
   - 대형 모델(4B, 8B)이 파인튜닝에 더 잘 반응
   - 1.7B 모델은 파인튜닝 효과가 제한적

## 결론

1. **파인튜닝의 효과**
   - TF-IDF 기반 유사도에서 극적인 개선
   - 특히 4B, 8B 모델에서 탁월한 효과
   - 도메인 특화 어휘 학습이 성공적

2. **최적 모델 선택**
   - **성능 최우선**: 8B 모델 (TF-IDF: 0.2895, Embedding: 0.9219)
   - **성능/효율 균형**: 4B 모델 (TF-IDF: 0.2847, Embedding: 0.9159)
   - **리소스 제약**: 1.7B 모델은 권장하지 않음 (파인튜닝 효과 미미)

3. **실무 적용 권장사항**
   - 회의록 요약 작업: 4B 또는 8B 파인튜닝 모델 사용
   - 4B와 8B의 성능 차이가 크지 않음 (TF-IDF 1.7% 차이)
   - 운영 환경에 따라 4B 모델이 최적 선택일 가능성

## 개선 제안

1. **추가 최적화**
   - 1.7B 모델: 다른 파인튜닝 전략 필요
   - 학습 데이터 확대 고려
   - 하이퍼파라미터 조정

2. **실제 업무 평가**
   - 요약 품질 정성 평가 필요
   - 사용자 만족도 조사
   - 처리 속도 벤치마크

3. **장기 전략**
   - 정기적인 재학습 체계 구축
   - 도메인별 특화 모델 개발
   - 지속적인 성능 모니터링